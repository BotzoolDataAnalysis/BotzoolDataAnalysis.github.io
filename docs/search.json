[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Botzool Data Analysis",
    "section": "",
    "text": "This is a website for the study materials related to the courses on data analysis at the Department of Botany and Zoology, Faculty of Science, Masaryk University.\nThe related courses are:\n\nBi8190 Manipulace a vizualizace dat v R (Data manipulation and visualisation in R)\nBi6050 Introduction to Biostatistics in English\nBi7540 Analýza dat v ekologii společenstev\nBi7542 Data analysis in community ecology\n\nMoreover, this website contains guidelines for the use of AI in ecology.\n\nWebsite Navigation\n\nData Analysis in Community Ecology\nData Manipulation and Visualisation\nAI",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html",
    "title": "6 Advanced Visualisation",
    "section": "",
    "text": "You already know how to make basic plots using ggplot functions. We’ll now dive deeper into data visualisation using ggplot2 package and its extensions.\nEvery plot in ggplot consists of 7 parts that together serve as instructions on how to draw a plot:\nTo produce any plot, ggplot needs data, mapping and at least one layer. The other parts have some defaults, but often need adjustments for a better appearance.\nWe will again use the penguin dataset and play with the individual plot parts.\nlibrary(palmerpenguins)\nlibrary(readxl)\nlibrary(tidyverse)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#faceting",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#faceting",
    "title": "6 Advanced Visualisation",
    "section": "6.1 Faceting",
    "text": "6.1 Faceting\nFrom Chapter 3, you already know how to visualise the relationship between the penguin bill length and bill depth.\n\npenguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, fill = species, shape = species)) +\n  geom_point() + \n  geom_smooth(aes(colour = species), method = 'lm', show.legend = F) + \n  scale_shape_manual(values = c(21, 22, 24)) + \n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)', fill = 'Species', shape = 'Species') + \n  theme_bw()\n\n\n\n\n\n\n\n\nBut what if we now want to look at the differences between males and females in addition? We could change the definition of the fill and colour aesthetics, so that they display sex and keep just shape for species identity:\n\npenguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, fill = sex, shape = species)) +\n  geom_point() + \n  geom_smooth(aes(colour = sex), method = 'lm', show.legend = F) + \n  scale_shape_manual(values = c(21, 22, 24)) + \n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)', fill = 'Sex', shape = 'Species') \n\n\n\n\n\n\n\n\nBut as you see, it gets a bit hard to read the plot. There are some NA values in the sex column, which would be better removed so that they do not add a mess to the plot. And what about splitting the plot into three smaller plots, one for each species? This is where facetting comes into play.\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, fill = sex, shape = species)) +\n  geom_point() + \n  geom_smooth(aes(colour = sex), method = 'lm', show.legend = F) + \n  scale_shape_manual(values = c(21, 22, 24)) + \n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)', fill = 'Sex', shape = 'Species') + \n  facet_wrap(~species)+\n  theme_bw()\n\n\n\n\n\n\n\n\nMuch better, but do we really need the different shapes for different species if we have them now in different facets? It would be better to add shape differentiation to the sex variable, as not everyone is able to distinguish colours. It might also help in the legend, where we now have just black dots for both levels.\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, fill = sex, shape = sex)) +\n  geom_point() + \n  geom_smooth(aes(colour = sex), method = 'lm', show.legend = F) + \n  scale_shape_manual(values = c(21, 22, 24)) + \n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)', fill = 'Sex', shape = 'Sex') + \n  facet_wrap(~species)+\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can now clearly see that for Adélie penguins, there is a positive relationship between bill length and bill depth for females, but not for males. On the other hand, for Chinstrap penguins, the relationship is stronger for males than for females.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#data-argument",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#data-argument",
    "title": "6 Advanced Visualisation",
    "section": "6.2 Data argument",
    "text": "6.2 Data argument\nTill now, we have worked just with one data frame for the whole plot. You might remember that it is possible to place aesthetic mappings (aes()) either in the ggplot() call, and then it works for the whole plot, or in a geom function, which then overwrites the global mappings for that layer only. The same is possible for the data argument. We can define a different dataset for a certain layer to add, e.g., points from another dataset or highlight a subset of the data.\nAs an example, we will draw a scatterplot of penguin body mass vs flipper length, where we highlight penguins from the Dream Island with bigger red points:\n\npenguins |&gt; \n  ggplot(aes(body_mass_g, flipper_length_mm))+\n  geom_point()+\n  geom_point(data = penguins |&gt; filter(island == 'Dream'), colour = 'red', size = 3)+\n  theme_bw()+\n  labs(x = 'Body mass (g)', y = 'Flipper length (mm)')\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote that we added specifications of the point appearance of the highlighted point in the geom_point() function outside the aes(). This means the characteristics are fixed for that layer and do not change according to the data. All the points of that layer will always be red and of size 3.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#scales",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#scales",
    "title": "6 Advanced Visualisation",
    "section": "6.3 Scales",
    "text": "6.3 Scales\nScale functions always follow the pattern scale_{aesthetic}_{type}. We already used scale_shape_manual() to manually rewrite the default shapes used in the plot. We can use scales to modify values, limits, breaks or labels of any aesthetics in our plot, typically colour, fill, and shape. To define desired values on your own, you can use manual scale definition, but there are many other types, e.g. continuous (useful, e.g., for defining labels for continuous variables), discrete (e.g. labels for discrete variables), gradient (e.g. for defining colour gradient for continuous variables). scale_x_{type} and scale_y_{type} are useful to define axis limits, ticks and labels.\nWe will use the example of penguin body mass vs. flipper length and modify different scales of this plot. First of all, we will show with different colours on which island the penguins were measured. To change the default colours, we can define our own colours using scale_fill_manual() and the values argument inside, similarly to what we did for the shape before. But we can also use a predefined colour scale, e.g. from colour brewer using scale_fill_brewer(). We can also modify the labels in the legend by using the argument labels. Let’s say we want to add the word ‘Island’ to each label:\n\npenguins |&gt; \n  ggplot(aes(body_mass_g, flipper_length_mm, fill = island))+\n  geom_point(pch = 21)+\n  scale_fill_brewer(type = 'qual', palette = 7, labels = c('Dream' = 'Dream Island', 'Biscoe' = 'Biscoe Island', 'Torgersen' = 'Torgersen Island'))+\n  theme_bw()+\n  labs(x = 'Body mass (g)', y = 'Flipper length (mm)', fill = 'Island')\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe used type = 'qual' inside scale_color_brewer() to use colours appropriate for discrete values. You can experiment with many more such colour scales that are already available.\nInstead of colouring the points according to the island, we can also use colours to show a numerical variable, e.g. year when the measurement was taken:\n\npenguins |&gt; \n  ggplot(aes(body_mass_g, flipper_length_mm, fill = year))+\n  geom_point(pch = 21)+\n  theme_bw()+\n  labs(x = 'Body mass (g)', y = 'Flipper length (mm)', fill = 'Year')\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote that the plot automatically chooses a continuous colour scale instead of the discrete one before, and the legend now shows a range from the lowest to the highest value. But for the year, it doesn’t really make sense to show decimals. We can modify this using scale_fill_continuous():\n\npenguins |&gt; \n  ggplot(aes(body_mass_g, flipper_length_mm, fill = year))+\n  geom_point(pch = 21)+\n  scale_fill_continuous(breaks = c(2007, 2008, 2009))+\n  theme_bw()+\n  labs(x = 'Body mass (g)', y = 'Flipper length (mm)', fill = 'Year')\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nScales are also useful to set limits, breaks or labels on the plot axes. For example, we might want to zoom only to the penguins with body mass between 3000 and 5000 g:\n\npenguins |&gt; \n  ggplot(aes(body_mass_g, flipper_length_mm, fill = year))+\n  geom_point(pch = 21)+\n  scale_fill_continuous(breaks = c(2007, 2008, 2009))+\n  scale_x_continuous(limits = c(3000, 5000))+\n  theme_bw()+\n  labs(x = 'Body mass (g)', y = 'Flipper length (mm)', fill = 'Year')\n\nWarning: Removed 72 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote that when we do this, we get a warning message, indicating how many points were not displayed because they are outside the defined range. That’s just to prevent us from accidentally missing them.\nIt is even possible to transform the axis using the transform argument or shortcuts for commonly used transformations, e.g. scale_x_log10().",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#legend-modifications",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#legend-modifications",
    "title": "6 Advanced Visualisation",
    "section": "6.4 Legend modifications",
    "text": "6.4 Legend modifications\nIn all the plots we made so far, the legend was by default placed on the right side next to the plot. Sometimes it is helpful to move it somewhere else, for example, due to space constraints. Legend position might be modified inside the theme() function. We can move it to the bottom of the plot:\n\npenguins |&gt; \n  ggplot(aes(body_mass_g, flipper_length_mm, fill = island))+\n  geom_point(pch = 21)+\n  scale_fill_brewer(type = 'qual', palette = 7, labels = c('Dream' = 'Dream Island', 'Biscoe' = 'Biscoe Island', 'Torgersen' = 'Torgersen Island'))+\n  theme_bw()+\n  theme(legend.position = 'bottom')+\n  labs(x = 'Body mass (g)', y = 'Flipper length (mm)', fill = 'Island')\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSometimes, there is a space inside the plot, so it is possible to place the legend there and reduce the image size. In this case, we have to specify two arguments, legend.position() and legend.justification(). The values should indicate coordinates on the two axes and have values of 0-1, where 0 means the beginning of the axis and 1 the end. For example, 0, 0 means the bottom left corner of the plot, 1, 1 the top right corner.\n\npenguins |&gt; \n  ggplot(aes(body_mass_g, flipper_length_mm, fill = island))+\n  geom_point(pch = 21)+\n  scale_fill_brewer(type = 'qual', palette = 7, labels = c('Dream' = 'Dream Island', 'Biscoe' = 'Biscoe Island', 'Torgersen' = 'Torgersen Island'))+\n  theme_bw()+\n  theme(legend.position = c(1, 0), legend.justification = c(1, 0))+\n  labs(x = 'Body mass (g)', y = 'Flipper length (mm)', fill = 'Island')\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nTo remove the legend completely, set legend.position = 'none'.\nThe theme() function allows us to modify many more plot elements. We can remove any plot component by setting the argument for it to element_blank(), modify the text size, label angle, ticks length, background grid colour and many more. See the documentation for more details.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#reordering",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#reordering",
    "title": "6 Advanced Visualisation",
    "section": "6.5 Reordering",
    "text": "6.5 Reordering\nFor the final presentation of plots, it is sometimes needed to reorder the levels of the categorical variable. Let’s look at the following plot, where we want to look at which penguin species has the longest bill on average.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(bill_length_mean = mean(bill_length_mm, na.rm = T)) |&gt; \n  ggplot(aes(species, bill_length_mean)) +\n  geom_col()\n\n\n\n\n\n\n\n\nFor the final visualisation, it might be better to reorder the species from that with the longest bill to that with the shortest bill. To reorder levels of a categorical variable, we need to treat it as a factor. Then, we can reorder the levels according to the values of a given variable using a function fct_reorder() from the forcats package, which is also a part of tidyverse. We want to modify an existing column, so we will rewrite it in mutate(). Inside the fct_reorder() function, we specify two arguments: the first specifies which variable should be reordered, and the second specifies which values should be taken to make the order. You can imagine this as if you first arrange the table according to the bill length in descending order, and then fix the order of the categorical variable (species) from this ordered table.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(bill_length_mean = mean(bill_length_mm, na.rm = T)) |&gt; \n  mutate(species = fct_reorder(species, desc(bill_length_mean))) |&gt; \n  ggplot(aes(species, bill_length_mean)) +\n  geom_col()\n\n\n\n\n\n\n\n\nThe forcats package contains many useful functions for working with factors. Check, for example, fct_relevel() for manual reordering of factor levels.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#composing-plots-together",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#composing-plots-together",
    "title": "6 Advanced Visualisation",
    "section": "6.6 Composing plots together",
    "text": "6.6 Composing plots together\nSometimes it is useful to combine multiple plots in one figure with multiple panels. This might be easily done using the patchwork package.\n\nlibrary(patchwork)\n\n(Please place this line at the top of your script to the other library() calls.)\nLet’s say we want to make one figure showing the relationship between the penguin bill length and bill depth for the three penguin species, and a boxplot showing the distribution of body mass for individual species next to each other.\nWe will first create these two plots and save them to an object:\n\np1 &lt;- penguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, fill = species, shape = species)) +\n  geom_point() + \n  geom_smooth(aes(colour = species), method = 'lm', show.legend = F) + \n  scale_shape_manual(values = c(21, 22, 24)) + \n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)', fill = 'Species', shape = 'Species') + \n  theme_bw()\n\np2 &lt;- ggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() + \n  labs(y = 'Body mass (g)', fill = 'Species') + \n  theme_bw() + \n  theme(axis.title.x = element_blank())\n\nAnd then combine the two plots together:\n\np1 + p2\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nSo easy it is. And we can play more with the plot layout. Some useful functionalities of the patchwork package are that you can add annotations to the plots to identify individual panels:\n\np1 + p2 + plot_annotation(tag_levels = 'a')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nOr collect all legends of the subplots and place them in one place:\n\np1 + p2 + plot_layout(guides = 'collect')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#useful-extensions",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#useful-extensions",
    "title": "6 Advanced Visualisation",
    "section": "6.7 Useful extensions",
    "text": "6.7 Useful extensions\nIt is possible to do almost everything you would imagine using ggplot2 and its extensions. We do not have time to cover all of that during this semester, but you are very welcome to experiment and explore this world on your own.\nJust a few tips for packages we personally find very useful for our work:\n\nggpubr to make publication-ready plots - some easy-to-use functions for creating and customising ggplot plots\nggeffects to calculate model predictions and plot them (will be replaced by modelbased, but the functionality shouldn’t be lost)\ngordi a ggplot-based package for making ordination plots, developed in our group (coming soon 😉)\nggnewscale to use multiple colour or fill scales in the same plot, especially useful for maps\nshiny for creating web apps from R",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#exercises",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#exercises",
    "title": "6 Advanced Visualisation",
    "section": "6.8 Exercises",
    "text": "6.8 Exercises\n\nfacet_wrap() facets the plot according to one variable, look in the documentation, what the related function facet_grid() does and use it to make a plot of penguin bill length and bill depth faceted by species and island.\nUse the gapminder dataset, from the gapminder package, which provides values for life expectancy, GDP per capita, and population size for each country of the world. Visualise the relationship between GDP per capita (on a log scale) and life expectancy in 2007. Highlight the Czech Republic and Slovakia with different colours. Place the legend in the top left corner of the plot. * Make the size of the points proportional to the country population size.\nCreate a plot showing the development of life expectancy in the Czech Republic and Slovakia over time.\nCombine the two plots from exercises 2 and 3 using p1 / p2. What is the difference compared to p1 + p2? Add annotations ‘A’ and ‘B’ to the panels. * Modify the legend so that there is only one legend with coloured points at the bottom.\nUse the dataset Axmanova-Forest-understory-diversity-analyses.xlsx and recreate the following plot:\n\n\n\n\n\n\n\n\n\nUse the Pokémon dataset (read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-01/pokemon_df.csv')), calculate the mean defense power for each Pokémon type and draw a barplot showing the mean defense power for each type. Order the bars from the highest to the shortest. *Colour the bars according to the colour defined for each Pokémon type in the column color_1. Add errorbars showing the minimum and maximum for each type.\nSave all plots you created so far to the plots folder.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/6_advanced_visualisation.html#further-reading",
    "href": "DataManipulationVisualisation/6_advanced_visualisation.html#further-reading",
    "title": "6 Advanced Visualisation",
    "section": "6.8 Further reading",
    "text": "6.8 Further reading\nggplot2 vignette: https://ggplot2.tidyverse.org\nR 4 Data Science: https://r4ds.hadley.nz/visualize.html\nCheatsheet: https://rstudio.github.io/cheatsheets/html/data-visualization.html\nAesthetics specifications vignette: https://ggplot2.tidyverse.org/articles/ggplot2-specs.html\nggplot2: Elegant Graphics for Data Analysis (3e): https://ggplot2-book.org\nR Graphics Cookbook: https://r-graphics.org\npatchwork vignette: https://patchwork.data-imaginist.com/index.html\nggplot Extensions: https://exts.ggplot2.tidyverse.org/gallery\nMastering Shiny: https://mastering-shiny.org\nTutorial for effective visual science communication: https://ascpt.onlinelibrary.wiley.com/doi/full/10.1002/psp4.12455\nGraphics principles cheatsheet: https://github.com/GraphicsPrinciples/CheatSheet/blob/master/NVSCheatSheet.pdf",
    "crumbs": [
      "Data Manipulation and Visualization",
      "6 Advanced data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/4_wide_vs_long.html",
    "href": "DataManipulationVisualisation/4_wide_vs_long.html",
    "title": "4 Wide vs.long format",
    "section": "",
    "text": "In this chapter we will look at different data formats and how to change one to another. We will define groups within the data and calculate several statistics for them using summarise functions.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "4 Wide vs. long format"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/4_wide_vs_long.html#data-formats",
    "href": "DataManipulationVisualisation/4_wide_vs_long.html#data-formats",
    "title": "4 Wide vs.long format",
    "section": "4.1 Data formats",
    "text": "4.1 Data formats\nThere are two main ways how the data can be organised across rows and columns. Wide or long format. We will show you an example of spruce forest data, where we recorded plant species in several sites and at each site we also estimated their abundance, here approximated as percentage cover (higher value means that the species covered larger area of the surveyed vegetation plot, but we do not give the area itself, just value relatively to the total area, i.e. percentage of total area). The covers of species might overlap, as they grow in different heights.\n\nWide format is more conservative and used in many older packages for ecological data analysis. In our example we list all species and the colums are used to indicate their abundance at each site. This is the way you need to prepare your species matrix for ordinations in vegan. However, wide format has also many cons.\nOne of them is the size of the file. In the example above, there are abundance of given plants in each of the site. When the species is present in just one site, here Trientalis europaea, it is still keeping space across the whole table, where there can be hundreds or thousands of sites. The table code is then of course memory demanding.\nOne more point. For many ecological analyses you cannot keep the empty cells empty, since they are recognised as NAs. So you have to do another step and fill them with zeros.\nAnother disadvantage is that you cannot add easily new information to the listed species. If you for example want to separate species that are in a tree vegetation layer (recognised in vegetation ecology as 1), herb layer (6) and moss layer (9), you would have to add this information to the name of the species e.g. Picea_abies_1. Can you see a conflict with the basic principles of tidyverse?\n\nLong format in contrast, is great for handling large datasets. Imagine we have more sites than those shown in the example above. In this format, we list all the species for Site1, then for Site2 etc. but we list only the species that are really present! By simple count of the rows belonging to each site you have the information about overall species richness.\nWe can also add any information, describing the data, such as vegetation layers, growth forms, native/alien status etc. After that we can very simply filter, summarise and calculate further statistics.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "4 Wide vs. long format"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/4_wide_vs_long.html#from-long-to-wide",
    "href": "DataManipulationVisualisation/4_wide_vs_long.html#from-long-to-wide",
    "title": "4 Wide vs.long format",
    "section": "4.2 From long to wide",
    "text": "4.2 From long to wide\nWe will first start a new script for this chapter and load the libraries. Remember to keep the script tidy and to put there remarks.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\n\nNow, we will import the data. In the first example we will again use the Forest understory data from our folder: Link to Github folder However, this time we will upload the species data saved in a long format and we will prepare a matrix in a wide format, so that it can be used in specific ecological analyses e.g. in vegan.\n\nspe &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-spe.xlsx\")\ntibble(spe)\n\n# A tibble: 2,277 × 4\n   RELEVE_NR Species              Layer CoverPerc\n       &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n 1         1 Quercus petraea agg.     1        63\n 2         1 Acer campestre           1        18\n 3         1 Crataegus laevigata      4         2\n 4         1 Cornus mas               4         8\n 5         1 Lonicera xylosteum       4         3\n 6         1 Galium sylvaticum        6         3\n 7         1 Carex digitata           6         3\n 8         1 Melica nutans            6         2\n 9         1 Polygonatum odoratum     6         2\n10         1 Geum urbanum             6         2\n# ℹ 2,267 more rows\n\n\nWe can see that there are plant species names sorted by RELEVE_NR, where each number indicates a vegetation record from one specific site (can be also called vegetation plot or sample). We will rename this name to make it easier for us as PlotID. Further we may need to change the species names to be in the compact format, without any spaces, just underscores. For this we will use mutate function with str_replace (for string specification) indicating that each space should be changed to underscore and we will directly apply it to the original column.\n\nspe %&gt;% \n  rename(PlotID= RELEVE_NR)%&gt;%\n  mutate(Species = str_replace_all(Species, \" \", \"_\"))\n\n# A tibble: 2,277 × 4\n   PlotID Species              Layer CoverPerc\n    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n 1      1 Quercus_petraea_agg.     1        63\n 2      1 Acer_campestre           1        18\n 3      1 Crataegus_laevigata      4         2\n 4      1 Cornus_mas               4         8\n 5      1 Lonicera_xylosteum       4         3\n 6      1 Galium_sylvaticum        6         3\n 7      1 Carex_digitata           6         3\n 8      1 Melica_nutans            6         2\n 9      1 Polygonatum_odoratum     6         2\n10      1 Geum_urbanum             6         2\n# ℹ 2,267 more rows\n\n\n*If you want to play a bit, you can create new column (e.g. SpeciesNew) to see both the original name and changed name.\n\nspe %&gt;% \n  rename(PlotID= RELEVE_NR)%&gt;%\n  mutate(SpeciesNew = str_replace_all(Species, \" \", \"_\"))\n\n# A tibble: 2,277 × 5\n   PlotID Species              Layer CoverPerc SpeciesNew          \n    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;               \n 1      1 Quercus petraea agg.     1        63 Quercus_petraea_agg.\n 2      1 Acer campestre           1        18 Acer_campestre      \n 3      1 Crataegus laevigata      4         2 Crataegus_laevigata \n 4      1 Cornus mas               4         8 Cornus_mas          \n 5      1 Lonicera xylosteum       4         3 Lonicera_xylosteum  \n 6      1 Galium sylvaticum        6         3 Galium_sylvaticum   \n 7      1 Carex digitata           6         3 Carex_digitata      \n 8      1 Melica nutans            6         2 Melica_nutans       \n 9      1 Polygonatum odoratum     6         2 Polygonatum_odoratum\n10      1 Geum urbanum             6         2 Geum_urbanum        \n# ℹ 2,267 more rows\n\n\nNote that in the same way as above you can change different patterns, e.g. removing part of the strings. There are even more complex things we can do, but we will keep it for later as it requires some knowledge of regex rules. Here I created a new column where I removed indication that the species belongs to a complex, i.e.the part of the string saying it is an aggregate, I also prepared a column “check” where I compare if the new and old name are equal or not and filtered just those cases where they are not. This should show me exactly the changed rows. If I am happy with the result, I can then remove the lines with this check and I can even rewrite the original column.\n\nspe %&gt;% \n  rename(PlotID= RELEVE_NR)%&gt;%\n  mutate(SpeciesNew = str_replace_all(Species, \" agg.\", \"\"))%&gt;%\n  mutate(check=(SpeciesNew==Species))%&gt;% \n  filter(check == \"FALSE\") %&gt;%\n  select(PlotID, Species, SpeciesNew, check) #select only relevant to see at a first glance\n\n# A tibble: 159 × 4\n   PlotID Species                  SpeciesNew          check\n    &lt;dbl&gt; &lt;chr&gt;                    &lt;chr&gt;               &lt;lgl&gt;\n 1      1 Quercus petraea agg.     Quercus petraea     FALSE\n 2      1 Rubus fruticosus agg.    Rubus fruticosus    FALSE\n 3      1 Quercus petraea agg.     Quercus petraea     FALSE\n 4      2 Quercus petraea agg.     Quercus petraea     FALSE\n 5      2 Quercus petraea agg.     Quercus petraea     FALSE\n 6      3 Quercus petraea agg.     Quercus petraea     FALSE\n 7      3 Quercus petraea agg.     Quercus petraea     FALSE\n 8      3 Galium pumilum agg.      Galium pumilum      FALSE\n 9      3 Senecio nemorensis agg.  Senecio nemorensis  FALSE\n10      3 Veronica chamaedrys agg. Veronica chamaedrys FALSE\n# ℹ 149 more rows\n\n\nWe have the condensed name with underscores, but there are still more variables in the table. We can either remove them or merge them to be included in the final wide format. Here we will go a bit against tidy rules and add the information about the vegetation layer directly to the variable Species using unite function from the package tidyr which merges strings from two or more columns into a new one: A+B =A_B. Default separator is again underscore, unless you specify it differently by sep=XX argument.\nArgument na.rm indicates what to do if in one of the combined columns there is no value but NA. We have set this argument to TRUE to remove the NA. If you keep it FALSE it can happen that in some data the new string will be a_NA or NA_b, or even NA_NA (see line 4 of our example).\nRemove argument set to TRUE will remove the original columns which we used to combine the new one (in the example above you will have only z). In our case we will keep original columns for visual checking and we will use select function in the next step to remove them.\nNote that function that works in an opposite direction is called separate or separate_wider_delim\n\nspe %&gt;% \n  rename(PlotID= RELEVE_NR)%&gt;%\n mutate(Species = str_replace_all(Species, \" \", \"_\"))%&gt;%\n  unite(\"SpeciesLayer\", Species,Layer, na.rm = TRUE, remove = FALSE) \n\n# A tibble: 2,277 × 5\n   PlotID SpeciesLayer           Species              Layer CoverPerc\n    &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n 1      1 Quercus_petraea_agg._1 Quercus_petraea_agg.     1        63\n 2      1 Acer_campestre_1       Acer_campestre           1        18\n 3      1 Crataegus_laevigata_4  Crataegus_laevigata      4         2\n 4      1 Cornus_mas_4           Cornus_mas               4         8\n 5      1 Lonicera_xylosteum_4   Lonicera_xylosteum       4         3\n 6      1 Galium_sylvaticum_6    Galium_sylvaticum        6         3\n 7      1 Carex_digitata_6       Carex_digitata           6         3\n 8      1 Melica_nutans_6        Melica_nutans            6         2\n 9      1 Polygonatum_odoratum_6 Polygonatum_odoratum     6         2\n10      1 Geum_urbanum_6         Geum_urbanum             6         2\n# ℹ 2,267 more rows\n\n\nAt this point we have everything we need to use it as input for the wide format table: PlotID, SpeciesLayer and values of the abundance saved as CoverPerc. One more step is to select only these or to deselect (-) those not needed.\n\nspe %&gt;% \n  rename(PlotID= RELEVE_NR)%&gt;%\n  mutate(Species = str_replace_all(Species, \" \", \"_\"))%&gt;%\n  unite(\"SpeciesLayer\", Species,Layer, na.rm = TRUE, remove = FALSE) %&gt;%\n  select(PlotID, Species, Layer)\n\n# A tibble: 2,277 × 3\n   PlotID Species              Layer\n    &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1      1 Quercus_petraea_agg.     1\n 2      1 Acer_campestre           1\n 3      1 Crataegus_laevigata      4\n 4      1 Cornus_mas               4\n 5      1 Lonicera_xylosteum       4\n 6      1 Galium_sylvaticum        6\n 7      1 Carex_digitata           6\n 8      1 Melica_nutans            6\n 9      1 Polygonatum_odoratum     6\n10      1 Geum_urbanum             6\n# ℹ 2,267 more rows\n\n\nNow we can finaly use the pivot wider function to transform the data. We have to specify from where we are taking the names of new variables (names_from) and from where we should take the values which should appear in the table (values_from).\n\nspe %&gt;% \n  rename(PlotID= RELEVE_NR)%&gt;%\n  mutate(Species = str_replace_all(Species, \" \", \"_\"))%&gt;%\n  unite(\"SpeciesLayer\", Species,Layer, na.rm = TRUE, remove = TRUE) %&gt;%\n  pivot_wider(names_from = SpeciesLayer, values_from = CoverPerc)\n\n# A tibble: 65 × 370\n   PlotID Quercus_petraea_agg._1 Acer_campestre_1 Crataegus_laevigata_4\n    &lt;dbl&gt;                  &lt;dbl&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n 1      1                     63               18                     2\n 2      2                     63               NA                    NA\n 3      3                     63               NA                    NA\n 4      4                     38               NA                    NA\n 5      5                     63               NA                    NA\n 6      6                     38               NA                    NA\n 7      7                     63               NA                    NA\n 8      8                     38               NA                    NA\n 9      9                     38               NA                    NA\n10     10                     63               NA                    NA\n# ℹ 55 more rows\n# ℹ 366 more variables: Cornus_mas_4 &lt;dbl&gt;, Lonicera_xylosteum_4 &lt;dbl&gt;,\n#   Galium_sylvaticum_6 &lt;dbl&gt;, Carex_digitata_6 &lt;dbl&gt;, Melica_nutans_6 &lt;dbl&gt;,\n#   Polygonatum_odoratum_6 &lt;dbl&gt;, Geum_urbanum_6 &lt;dbl&gt;,\n#   Anemone_species_6 &lt;dbl&gt;, Viola_mirabilis_6 &lt;dbl&gt;,\n#   Hieracium_murorum_6 &lt;dbl&gt;, Platanthera_bifolia_6 &lt;dbl&gt;,\n#   Convallaria_majalis_6 &lt;dbl&gt;, Hepatica_nobilis_6 &lt;dbl&gt;, …\n\n\nThere are different combinations of species in each plot, some of them are present and some not. Since we changed the format, all species, even those not occurring in that particular site/plot have to get some values. In long format abundance or some other information is not stored for absent species, so they get NAs. Therefore, one more step is to fill the empty cells by zeros using values_fill. In this case we can do that, because we know that if the species was absent its abundance was exactly 0.\n\nspe %&gt;% \n  rename(PlotID= RELEVE_NR)%&gt;%\n  mutate(Species = str_replace_all(Species, \" \", \"_\"))%&gt;%\n  unite(\"SpeciesLayer\", Species,Layer, na.rm = TRUE, remove = TRUE) %&gt;%\n  pivot_wider(names_from = SpeciesLayer, values_from = CoverPerc,\n              values_fill = 0) -&gt; spe_wide",
    "crumbs": [
      "Data Manipulation and Visualization",
      "4 Wide vs. long format"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/4_wide_vs_long.html#from-wide-to-long",
    "href": "DataManipulationVisualisation/4_wide_vs_long.html#from-wide-to-long",
    "title": "4 Wide vs.long format",
    "section": "4.3 From wide to long",
    "text": "4.3 From wide to long\nSometimes it is needed to transform the data from wide to long. Here we need to say which column should not be changed, which is the PlotID (cols = -PlotID). Alternatively would work also -1.\nThan we specify what to do with the column names, how they should be saved (names_to). And how to call the column with values (values_to).\n\nspe_wide %&gt;% \n  pivot_longer(cols = -PlotID, names_to = 'species', values_to = 'cover')\n\n# A tibble: 23,985 × 3\n   PlotID species                cover\n    &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1      1 Quercus_petraea_agg._1    63\n 2      1 Acer_campestre_1          18\n 3      1 Crataegus_laevigata_4      2\n 4      1 Cornus_mas_4               8\n 5      1 Lonicera_xylosteum_4       3\n 6      1 Galium_sylvaticum_6        3\n 7      1 Carex_digitata_6           3\n 8      1 Melica_nutans_6            2\n 9      1 Polygonatum_odoratum_6     2\n10      1 Geum_urbanum_6             2\n# ℹ 23,975 more rows\n\n\nWe need to remove the empty rows. Because we filled them before with zeros, we will first finish the transformation and then filter out the rows with cover equal to zero.\n\nspe_wide %&gt;% \n  pivot_longer(cols = -PlotID, names_to = 'species', values_to = 'cover')%&gt;%\n  filter(!cover == 0)\n\n# A tibble: 2,277 × 3\n   PlotID species                cover\n    &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1      1 Quercus_petraea_agg._1    63\n 2      1 Acer_campestre_1          18\n 3      1 Crataegus_laevigata_4      2\n 4      1 Cornus_mas_4               8\n 5      1 Lonicera_xylosteum_4       3\n 6      1 Galium_sylvaticum_6        3\n 7      1 Carex_digitata_6           3\n 8      1 Melica_nutans_6            2\n 9      1 Polygonatum_odoratum_6     2\n10      1 Geum_urbanum_6             2\n# ℹ 2,267 more rows\n\n\nSometimes we have data with NAs (not zeros) here it is very useful to use argument values_drop_na =TRUE\n\nspe_wide %&gt;% \n  pivot_longer(cols = -PlotID, names_to = 'species', values_to = 'cover', values_drop_na =TRUE)\n\n# A tibble: 23,985 × 3\n   PlotID species                cover\n    &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n 1      1 Quercus_petraea_agg._1    63\n 2      1 Acer_campestre_1          18\n 3      1 Crataegus_laevigata_4      2\n 4      1 Cornus_mas_4               8\n 5      1 Lonicera_xylosteum_4       3\n 6      1 Galium_sylvaticum_6        3\n 7      1 Carex_digitata_6           3\n 8      1 Melica_nutans_6            2\n 9      1 Polygonatum_odoratum_6     2\n10      1 Geum_urbanum_6             2\n# ℹ 23,975 more rows",
    "crumbs": [
      "Data Manipulation and Visualization",
      "4 Wide vs. long format"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/4_wide_vs_long.html#group-by-count",
    "href": "DataManipulationVisualisation/4_wide_vs_long.html#group-by-count",
    "title": "4 Wide vs.long format",
    "section": "4.4 Group by, count",
    "text": "4.4 Group by, count\nUsing group_by we can define how the data should be arranged into groups. Then we can proceed with asking questions about these groups. Basic function how to summarise the information, is to count the number of rows.\nIn the first example we are working with species list in a long format. We can easily calculate number of species in each plot/sample by counting corresponding number of rows. First you have to specify what is the grouping variable, here it would be PlotID. And then you can directly count.\n\nspe %&gt;% \n  rename(PlotID= RELEVE_NR)%&gt;%\n  group_by(PlotID)%&gt;%\n  count()\n\n# A tibble: 65 × 2\n# Groups:   PlotID [65]\n   PlotID     n\n    &lt;dbl&gt; &lt;int&gt;\n 1      1    44\n 2      2    17\n 3      3    16\n 4      4    22\n 5      5    15\n 6      6    22\n 7      7    23\n 8      8    24\n 9      9    28\n10     10    19\n# ℹ 55 more rows\n\n\nCount returns number of rows in a new variable called n. You can directly put extra line to your code and rename it. e.g. rename(SpeciesRichness =n). Note that in this case we simply counted all the rows. But we know some woody species might be recorded in tree vegetation layer, as shrub or as a small juvenile, so we potentially calculated such species three times. We can play a bit with distinct function and remove the layer information. Is there a difference in the resulting numbers?\n\nspe %&gt;% \n  distinct(PlotID= RELEVE_NR, Species)%&gt;%\n  group_by(PlotID)%&gt;%\n  count()%&gt;%\n  rename(SpeciesRichness=n)\n\n# A tibble: 65 × 2\n# Groups:   PlotID [65]\n   PlotID SpeciesRichness\n    &lt;dbl&gt;           &lt;int&gt;\n 1      1              42\n 2      2              16\n 3      3              15\n 4      4              20\n 5      5              14\n 6      6              19\n 7      7              22\n 8      8              23\n 9      9              22\n10     10              18\n# ℹ 55 more rows\n\n\nCount can be used also for other cases. For example I am interested in how many rows/plots/samples in the table are assigned to vegetation types. I will upload the table with descriptive characteristics for each forest plot, named as env, which is a shortcut often used for environmental data file.\n\nenv &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-env.xlsx\")\ntibble(env)\n\n# A tibble: 65 × 13\n   RELEVE_NR ForestType ForestTypeName  Biomass pH_KCl Canopy_E3 Radiation  Heat\n       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1         1          2 oak hornbeam f…    12.8   5.28        80     0.881 0.857\n 2         2          1 oak forest          9.9   3.24        80     0.933 0.814\n 3         3          1 oak forest         15.2   4.01        80     0.916 0.850\n 4         4          1 oak forest         16     3.76        75     0.930 0.948\n 5         5          1 oak forest         20.7   3.50        70     0.869 0.869\n 6         6          1 oak forest         46.4   3.8         65     0.918 0.883\n 7         7          1 oak forest         49.2   3.48        65     0.829 0.803\n 8         8          2 oak hornbeam f…    48.7   3.68        85     0.869 0.869\n 9         9          2 oak hornbeam f…    13.8   4.24        80     0.614 0.423\n10        10          1 oak forest         79.1   4.00        70     0.905 0.930\n# ℹ 55 more rows\n# ℹ 5 more variables: TWI &lt;dbl&gt;, Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;,\n#   deg_lon &lt;dbl&gt;, deg_lat &lt;dbl&gt;\n\n\nAnd calculate the number of plots in each forest type.\n\nenv %&gt;% \n  group_by(ForestTypeName)%&gt;% \n  count() \n\n# A tibble: 4 × 2\n# Groups:   ForestTypeName [4]\n  ForestTypeName          n\n  &lt;chr&gt;               &lt;int&gt;\n1 alluvial forest        11\n2 oak forest             16\n3 oak hornbeam forest    28\n4 ravine forest          10\n\n\nWe can even skip the group_by step and directly specify what to count, which is very useful.\n\nenv %&gt;% \n  count(ForestTypeName)\n\n# A tibble: 4 × 2\n  ForestTypeName          n\n  &lt;chr&gt;               &lt;int&gt;\n1 alluvial forest        11\n2 oak forest             16\n3 oak hornbeam forest    28\n4 ravine forest          10\n\n\nCount can be very useful to check for duplicate rows. You can ask to count for example IDs in the file where you expect unique ID at each row and filter those results that have more occurrences than 1. In the example below, each ID was used just once, so the filter returns no rows. Such a check is important especially if you need to append another information by join functions.\n\nenv %&gt;% \n  count(PlotID=RELEVE_NR) %&gt;%\n  filter(n&gt;1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: PlotID &lt;dbl&gt;, n &lt;int&gt;",
    "crumbs": [
      "Data Manipulation and Visualization",
      "4 Wide vs. long format"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/4_wide_vs_long.html#summarise",
    "href": "DataManipulationVisualisation/4_wide_vs_long.html#summarise",
    "title": "4 Wide vs.long format",
    "section": "4.5 Summarise",
    "text": "4.5 Summarise\nUsing group_by and summarise we can calculate e.g. mean values, or total sum of the values within the group. See more in the further reading. Here is an example of summarise for environmental data:\n\nenv %&gt;% \n  group_by(ForestTypeName) %&gt;%\n  summarise(meanBiomass= mean(Biomass))\n\n# A tibble: 4 × 2\n  ForestTypeName      meanBiomass\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 alluvial forest           148. \n2 oak forest                 26.7\n3 oak hornbeam forest        44.4\n4 ravine forest              79.1\n\n\nSummarise is also very useful for species data. For example, we can calculate the total abundance, approximated as cover, of all plants in the plot/sample. Later on we can ask what is the relative share of parasites, endangered or alien species. Or we can calculate community mean of some traits, such as the mean height of the plants or their mean leaf area index. For this we would have to append new information, so we will leave it for next chapter.\n\nspe %&gt;% \n  group_by(PlotID=RELEVE_NR) %&gt;%\n  summarise(totalCover= sum(CoverPerc))\n\n# A tibble: 65 × 2\n   PlotID totalCover\n    &lt;dbl&gt;      &lt;dbl&gt;\n 1      1        164\n 2      2        115\n 3      3        101\n 4      4         96\n 5      5        109\n 6      6        121\n 7      7        147\n 8      8        185\n 9      9        128\n10     10        174\n# ℹ 55 more rows\n\n\nWe can also define more variables we want to summarise and more functions we want to apply. Try to find out more here. Below is an example of iris dataset, where I decided to get minimum,mean and maximum values for sepal leaves in different species.\n\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\n\niris%&gt;% \n  group_by(Species)%&gt;%\n  summarise(across(c(Sepal.Length, Sepal.Width), list(min= min, mean = mean, max = max )))\n\n# A tibble: 3 × 7\n  Species    Sepal.Length_min Sepal.Length_mean Sepal.Length_max Sepal.Width_min\n  &lt;fct&gt;                 &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n1 setosa                  4.3              5.01              5.8             2.3\n2 versicolor              4.9              5.94              7               2  \n3 virginica               4.9              6.59              7.9             2.2\n# ℹ 2 more variables: Sepal.Width_mean &lt;dbl&gt;, Sepal.Width_max &lt;dbl&gt;",
    "crumbs": [
      "Data Manipulation and Visualization",
      "4 Wide vs. long format"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/4_wide_vs_long.html#exercises",
    "href": "DataManipulationVisualisation/4_wide_vs_long.html#exercises",
    "title": "4 Wide vs.long format",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n1. Forest Understory Data - download the data from the repository Link to Github folderand save it into your project folder. This dataset is used above in the chapter, please use it to prepare your own script with remarks, copy and train what is described above. Note that you will use species data in long format: Axmanova-Forest-spe.xlsx, and environmental data Axmanova-Forest-env.xlsx\n2. Use Spruce forest data used in the graphical example in the beginning of this chapter. 2a, Import spruce_forestWIDE.xlsx &gt; transform wide to long format &gt; keep only true presences &gt; calculate species richness for each site (count of number of species). 2b, Import also spruce_forestLONG.xlsx &gt; and transform long to wide format.\n3. Train transformation of wide to long with another dataset Acidophilous grasslands namely the species file called SW_moravia_acidgrass_species.csv. Import the data and check structure &gt; transform the format to long, remove absences. *Use separate_wider_delim function to separate information about species name and layer into two variables.\n4. Work with dataset Lepidoptera, namely spe_matrix_MSvejnoha, which is the species file with counts of moths in forest steppe localities. Import the data and check the structure &gt; transform to long format &gt; count number of individuals at each site &gt; create barplot to visualise the differences (ggplot2, geom_col). *for comparison you can calculate the number of species (unique names) and visualise this as well.\n5. Use Forest understory environmental data Axmanova-Forest-env.xlsx&gt; prepare a new variable which will combine Forest type code and name (unite) &gt; calculate mean values of biomass for these forest types (group_by, summarise) &gt; prepare a graph with boxplots of forest type and biomass (ggplot2, geom_boxplot). *you can also play more with the data, calculate min, mean and max values for biomass and soil pH at once.\n6. We will use the iris dataset integrated in R. Use glimpse(iris) to check the structure &gt; if needed change the format to tibble using as_tibble function (iris.data&lt;- iris %&gt;% as_tibble()) &gt; calculate median values for selected parameters within different iris species.\n7. “Tidy datasets are all alike, but every messy dataset is messy in its own way.” Hadley Wickham Import Example4 from the Messy data and try to find duplicate rows.\n8. * Import Example2 from the Messy data and try to make the data tidy with the use of separate function. This works in an opposite direction to unite.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "4 Wide vs. long format"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/4_wide_vs_long.html#further-reading",
    "href": "DataManipulationVisualisation/4_wide_vs_long.html#further-reading",
    "title": "4 Wide vs.long format",
    "section": "4.7 Further reading",
    "text": "4.7 Further reading\nSummarise https://dplyr.tidyverse.org/reference/summarise.html\nSummarise multiple columns https://dplyr.tidyverse.org/reference/summarise_all.html\nData transformation in R for data science https://r4ds.hadley.nz/data-transform.html\nData tidying including pivoting https://r4ds.hadley.nz/data-tidy.html",
    "crumbs": [
      "Data Manipulation and Visualization",
      "4 Wide vs. long format"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html",
    "href": "DataManipulationVisualisation/2_data_manipulation.html",
    "title": "2 Data Manipulation",
    "section": "",
    "text": "In this chapter, we will try to explore the data, prepare subsets with only selected variables and filter them to only defined cases. We will prepare new variables and rearrange the data according to them. We will also further train importing and exporting the data.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#introducing-dplyr",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#introducing-dplyr",
    "title": "2 Data Manipulation",
    "section": "2.1. Introducing dplyr",
    "text": "2.1. Introducing dplyr\nThe term data manipulation might sound a bit tricky. However, it does not mean we plan to show you how to cheat and make better results. It just means we want to show you how to easily handle the data, prepare them in the form you need. The functions for basic data handling, namely select, filter, mutate, arrange, slice come from the tidyverse package called dplyr. Do you remember how to find out more about the package? If nothing else you can try “?dplyr” which actually gives you more hints where to look further.\n\n\n\n\n\nNote that many of these operations can be done also in some table editors (eg. Excel) before importing to R. However, the effort and time demands would be much higher and would be increasing enormously with the size of the dataset. In contrast, in R you can change and rerun the steps in one pipeline and the data will be immediately ready for next analyses.\nWe will need following libraries\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nand the forest dataset. Again I will import the data just once and use the pipe to test the functions/effects without actually changing the data\n\ndata &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-understory-diversity-analyses.xlsx\")\nnames(data)\n\n [1] \"PlotID\"           \"ForestType\"       \"ForestTypeName\"   \"Herbs\"           \n [5] \"Juveniles\"        \"CoverE1\"          \"Biomass\"          \"Soil_depth_categ\"\n [9] \"pH_KCl\"           \"Slope\"            \"Altitude\"         \"Canopy_E3\"       \n[13] \"Radiation\"        \"Heat\"             \"TransDir\"         \"TransDif\"        \n[17] \"TransTot\"         \"EIV_light\"        \"EIV_moisture\"     \"EIV_soilreaction\"\n[21] \"EIV_nutrients\"    \"TWI\"",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#select",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#select",
    "title": "2 Data Manipulation",
    "section": "2.2 Select",
    "text": "2.2 Select\nSelect extracts columns/variables based on their names or position. It is important to realise the difference between select and filter. Select is used to select the names of variables I want to keep in the dataset, while filter applies to rows depending on their values.\nYou can select by naming the variables. Here you would appreciate the tidy style of names! Tidy names means no need to use parentheses :-)\n\ndata %&gt;% \n  select(PlotID, ForestType, ForestTypeName) \n\n# A tibble: 65 × 3\n   PlotID ForestType ForestTypeName     \n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;              \n 1      1          2 oak hornbeam forest\n 2      2          1 oak forest         \n 3      3          1 oak forest         \n 4      4          1 oak forest         \n 5      5          1 oak forest         \n 6      6          1 oak forest         \n 7      7          1 oak forest         \n 8      8          2 oak hornbeam forest\n 9      9          2 oak hornbeam forest\n10     10          1 oak forest         \n# ℹ 55 more rows\n\n\nYou can also select by position. However, be sure it will stay the same after all the changes you might do with the data.\n\ndata %&gt;% \n  select(1:3)\n\n# A tibble: 65 × 3\n   PlotID ForestType ForestTypeName     \n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;              \n 1      1          2 oak hornbeam forest\n 2      2          1 oak forest         \n 3      3          1 oak forest         \n 4      4          1 oak forest         \n 5      5          1 oak forest         \n 6      6          1 oak forest         \n 7      7          1 oak forest         \n 8      8          2 oak hornbeam forest\n 9      9          2 oak hornbeam forest\n10     10          1 oak forest         \n# ℹ 55 more rows\n\n\nSometimes you decide you want to get rid of some variables. Either you can name all the others which you want to keep, or you can remove those unwanted with minus sign. If it is one variable it is easy “select(-xx)”, if two or more, you have to use “select(-c(xx, xy))”\n\ndata %&gt;% \n  select(-c(ForestType,ForestTypeName))\n\n# A tibble: 65 × 20\n   PlotID Herbs Juveniles CoverE1 Biomass Soil_depth_categ pH_KCl Slope Altitude\n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1      1    26        12      20    12.8              5     5.28     4      412\n 2      2    13         3      25     9.9              4.5   3.24    24      458\n 3      3    14         1      25    15.2              3     4.01    13      414\n 4      4    15         5      30    16                3     3.77    21      379\n 5      5    13         1      35    20.7              3     3.5      0      374\n 6      6    16         3      60    46.4              6     3.8     10      380\n 7      7    17         5      70    49.2              7     3.48     6      373\n 8      8    21         1      70    48.7              5     3.68     0      390\n 9      9    15         4      15    13.8              3.5   4.24    38      255\n10     10    14         4      75    79.1              5     4.01    13      340\n# ℹ 55 more rows\n# ℹ 11 more variables: Canopy_E3 &lt;dbl&gt;, Radiation &lt;dbl&gt;, Heat &lt;dbl&gt;,\n#   TransDir &lt;dbl&gt;, TransDif &lt;dbl&gt;, TransTot &lt;dbl&gt;, EIV_light &lt;dbl&gt;,\n#   EIV_moisture &lt;dbl&gt;, EIV_soilreaction &lt;dbl&gt;, EIV_nutrients &lt;dbl&gt;, TWI &lt;dbl&gt;\n\n\nYou can also define range of variables between two of them.\n\ndata %&gt;%\n  select(PlotID:ForestTypeName)\n\n# A tibble: 65 × 3\n   PlotID ForestType ForestTypeName     \n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;              \n 1      1          2 oak hornbeam forest\n 2      2          1 oak forest         \n 3      3          1 oak forest         \n 4      4          1 oak forest         \n 5      5          1 oak forest         \n 6      6          1 oak forest         \n 7      7          1 oak forest         \n 8      8          2 oak hornbeam forest\n 9      9          2 oak hornbeam forest\n10     10          1 oak forest         \n# ℹ 55 more rows\n\n\nOr you can combine the approaches listed above\n\ndata %&gt;%\n  select (PlotID, 3:6)\n\n# A tibble: 65 × 5\n   PlotID ForestTypeName      Herbs Juveniles CoverE1\n    &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1      1 oak hornbeam forest    26        12      20\n 2      2 oak forest             13         3      25\n 3      3 oak forest             14         1      25\n 4      4 oak forest             15         5      30\n 5      5 oak forest             13         1      35\n 6      6 oak forest             16         3      60\n 7      7 oak forest             17         5      70\n 8      8 oak hornbeam forest    21         1      70\n 9      9 oak hornbeam forest    15         4      15\n10     10 oak forest             14         4      75\n# ℹ 55 more rows\n\n\nSelect can be also used in combination with stringr package to identify the pattern in the names: try several options: starts_with, ends_with or more general one contains\n\ndata %&gt;%\n  select (PlotID, starts_with(\"EIV\"))\n\n# A tibble: 65 × 5\n   PlotID EIV_light EIV_moisture EIV_soilreaction EIV_nutrients\n    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1      1      5            4.38             6.68          4.31\n 2      2      4.71         4.64             4.67          3.69\n 3      3      4.36         4.7              4.8           3.55\n 4      4      5.26         4.38             5.53          3.56\n 5      5      6.14         4                5.33          3.46\n 6      6      6.19         4.35             6.75          5.06\n 7      7      6.19         4.25             6.09          4.33\n 8      8      5.29         4.6              5.07          4.12\n 9      9      5.47         4.36             5.46          3.5 \n10     10      6.53         3.86             6             3   \n# ℹ 55 more rows\n\n\nSelect can effectively help you organise the data. Imagine you have a workflow where you need only some variables, but in a certain sequence. And you import the data from different people, or years. With the use of select in your script you can order the variables always in the same way e.g. SampleID, ForestType, SpeciesNr, Productivity, even during import. And you can also rename the variables using select, to get exactly what you need. Here the new name is at the left, as in rename.\n\ndata %&gt;% \n  select(SampleID=PlotID, ForestCode=ForestType, SpeciesNr=Herbs, Productivity=Biomass) \n\n# A tibble: 65 × 4\n   SampleID ForestCode SpeciesNr Productivity\n      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1        1          2        26         12.8\n 2        2          1        13          9.9\n 3        3          1        14         15.2\n 4        4          1        15         16  \n 5        5          1        13         20.7\n 6        6          1        16         46.4\n 7        7          1        17         49.2\n 8        8          2        21         48.7\n 9        9          2        15         13.8\n10       10          1        14         79.1\n# ℹ 55 more rows",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#arrange",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#arrange",
    "title": "2 Data Manipulation",
    "section": "2.3 Arrange",
    "text": "2.3 Arrange\nThis function keeps the same variables just reorders the rows according to the values we select. To see the changes at a first glance I will first select only few variables.\n\ndata &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-understory-diversity-analyses.xlsx\") \ndata &lt;- data %&gt;% select(PlotID, ForestType, ForestTypeName, Biomass)\n\nNow I decided to arrange the data by Forest type\n\ndata %&gt;% \n  arrange(ForestTypeName)\n\n# A tibble: 65 × 4\n   PlotID ForestType ForestTypeName  Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1    101          4 alluvial forest    91.1\n 2    103          4 alluvial forest   114. \n 3    104          4 alluvial forest   188. \n 4    110          4 alluvial forest   126. \n 5    111          4 alluvial forest    84.8\n 6    113          4 alluvial forest    74.5\n 7    125          4 alluvial forest   123. \n 8    127          4 alluvial forest   176  \n 9    129          4 alluvial forest   100. \n10    131          4 alluvial forest   163. \n# ℹ 55 more rows\n\n\nWe can also decide to arrange the data from the highest value to the lowest, i.e. in descending order\n\ndata %&gt;% \n  arrange(desc(Biomass))\n\n# A tibble: 65 × 4\n   PlotID ForestType ForestTypeName      Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1    132          4 alluvial forest        287.\n 2    130          3 ravine forest          238 \n 3    104          4 alluvial forest        188.\n 4    127          4 alluvial forest        176 \n 5    131          4 alluvial forest        163.\n 6    110          4 alluvial forest        126.\n 7    125          4 alluvial forest        123.\n 8    128          3 ravine forest          121.\n 9    103          4 alluvial forest        114.\n10    124          2 oak hornbeam forest    102.\n# ℹ 55 more rows\n\n\nOr we can arrange according to more variables. Here the forest type goes first, as I decided it is the most important grouping variable. Within each type I want to see the rows/cases ordered by biomass values.\n\ndata %&gt;% \n  arrange(ForestType, ForestTypeName, desc(Biomass)) %&gt;% print(n=30)\n\n# A tibble: 65 × 4\n   PlotID ForestType ForestTypeName      Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1     10          1 oak forest             79.1\n 2      7          1 oak forest             49.2\n 3      6          1 oak forest             46.4\n 4     44          1 oak forest             34.1\n 5     82          1 oak forest             33.9\n 6     28          1 oak forest             29.7\n 7     42          1 oak forest             21.5\n 8      5          1 oak forest             20.7\n 9     81          1 oak forest             20.6\n10      4          1 oak forest             16  \n11     47          1 oak forest             15.5\n12      3          1 oak forest             15.2\n13     31          1 oak forest             14.9\n14    100          1 oak forest             13.5\n15      2          1 oak forest              9.9\n16     11          1 oak forest              7.4\n17    124          2 oak hornbeam forest   102. \n18    120          2 oak hornbeam forest    98.5\n19    121          2 oak hornbeam forest    84.7\n20    126          2 oak hornbeam forest    72.9\n21     18          2 oak hornbeam forest    72.2\n22    118          2 oak hornbeam forest    66.2\n23     99          2 oak hornbeam forest    64.8\n24    109          2 oak hornbeam forest    60.7\n25    112          2 oak hornbeam forest    58.4\n26    102          2 oak hornbeam forest    57.4\n27    117          2 oak hornbeam forest    54.7\n28      8          2 oak hornbeam forest    48.7\n29     86          2 oak hornbeam forest    46.9\n30     87          2 oak hornbeam forest    42.9\n# ℹ 35 more rows\n\n\nTo see more rows of the resulting table, I specified their number using print function.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#distinct",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#distinct",
    "title": "2 Data Manipulation",
    "section": "2.4 Distinct",
    "text": "2.4 Distinct\nDistinct is a function that takes your data and remove all the duplicate rows, keeping only the unique ones. There are many cases where you will really appreciate this elegant and easy way. For example, I want a list of unique PlotIDs, unique combinations of two categories etc. Here I want to prepare list of forest types codes and names.\n\ndata %&gt;%\n  arrange(ForestType) %&gt;%\n  select(ForestType, ForestTypeName) %&gt;%\n  distinct()\n\n# A tibble: 4 × 2\n  ForestType ForestTypeName     \n       &lt;dbl&gt; &lt;chr&gt;              \n1          1 oak forest         \n2          2 oak hornbeam forest\n3          3 ravine forest      \n4          4 alluvial forest    \n\n\nThe same can be done also if I skip the select tool, because distinct works more like select+distinct.\n\ndata %&gt;%      \n  arrange(ForestType) %&gt;%\n  distinct(ForestType, ForestTypeName)\n\n# A tibble: 4 × 2\n  ForestType ForestTypeName     \n       &lt;dbl&gt; &lt;chr&gt;              \n1          1 oak forest         \n2          2 oak hornbeam forest\n3          3 ravine forest      \n4          4 alluvial forest    \n\n\nSometimes, distinct can be used also when importing data just to remove forgotten duplicate rows: data&lt;- read_csv… %&gt;% distinct().",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#filter-and-slice",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#filter-and-slice",
    "title": "2 Data Manipulation",
    "section": "2.5 Filter and slice",
    "text": "2.5 Filter and slice\nWhen we have a large dataset, we sometimes need to create a subset of the rows/cases. First we have to define upon which variable we are going to filter the rows (e.g. Forest type, soil pH…) and which values are acceptable and which are not.\nIn this first example I use categorical variable and I want to match exact value, so I have to use ==\n\ndata %&gt;% \n  filter(ForestTypeName ==\"alluvial forest\")\n\n# A tibble: 11 × 4\n   PlotID ForestType ForestTypeName  Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1    101          4 alluvial forest    91.1\n 2    103          4 alluvial forest   114. \n 3    104          4 alluvial forest   188. \n 4    110          4 alluvial forest   126. \n 5    111          4 alluvial forest    84.8\n 6    113          4 alluvial forest    74.5\n 7    125          4 alluvial forest   123. \n 8    127          4 alluvial forest   176  \n 9    129          4 alluvial forest   100. \n10    131          4 alluvial forest   163. \n11    132          4 alluvial forest   287. \n\n\nOr I might define a list of values, especially for categorical variables. And the filter function will try to find rows with values exactly matching those %in% the list.\n\ndata %&gt;% \n  filter(ForestTypeName %in% c(\"alluvial forest\", \"ravine forest\"))\n\n# A tibble: 21 × 4\n   PlotID ForestType ForestTypeName  Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1    101          4 alluvial forest    91.1\n 2    103          4 alluvial forest   114. \n 3    104          4 alluvial forest   188. \n 4    105          3 ravine forest      65.5\n 5    106          3 ravine forest      33.6\n 6    108          3 ravine forest      82.3\n 7    110          4 alluvial forest   126. \n 8    111          4 alluvial forest    84.8\n 9    113          4 alluvial forest    74.5\n10    114          3 ravine forest      71.5\n# ℹ 11 more rows\n\n\nIf I filter continuous variable, I can work with thresholds. For example, here the biomass of herb layer is measured in g/m2 and I want only those rows/cases where the biomass values are higher than 80.\n\ndata &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-understory-diversity-analyses.xlsx\") \ndata %&gt;% \n  filter(Biomass &gt; 80) #[g/m2]\n\n# A tibble: 17 × 22\n   PlotID ForestType ForestTypeName      Herbs Juveniles CoverE1 Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1    101          4 alluvial forest        28         6      90    91.1\n 2    103          4 alluvial forest        35         5      80   114. \n 3    104          4 alluvial forest        25        11      85   188. \n 4    108          3 ravine forest          28         7      70    82.3\n 5    110          4 alluvial forest        37         4      95   126. \n 6    111          4 alluvial forest        26        10      85    84.8\n 7    120          2 oak hornbeam forest    36         6      75    98.5\n 8    121          2 oak hornbeam forest    30        12      80    84.7\n 9    122          3 ravine forest          29         6      60   102. \n10    124          2 oak hornbeam forest    34        15      75   102. \n11    125          4 alluvial forest        25         8      95   123. \n12    127          4 alluvial forest        53         6       0   176  \n13    128          3 ravine forest          20         4      70   121. \n14    129          4 alluvial forest        31         7      85   100. \n15    130          3 ravine forest          18         5      85   238  \n16    131          4 alluvial forest        59         3      95   163. \n17    132          4 alluvial forest        60         7      90   287. \n# ℹ 15 more variables: Soil_depth_categ &lt;dbl&gt;, pH_KCl &lt;dbl&gt;, Slope &lt;dbl&gt;,\n#   Altitude &lt;dbl&gt;, Canopy_E3 &lt;dbl&gt;, Radiation &lt;dbl&gt;, Heat &lt;dbl&gt;,\n#   TransDir &lt;dbl&gt;, TransDif &lt;dbl&gt;, TransTot &lt;dbl&gt;, EIV_light &lt;dbl&gt;,\n#   EIV_moisture &lt;dbl&gt;, EIV_soilreaction &lt;dbl&gt;, EIV_nutrients &lt;dbl&gt;, TWI &lt;dbl&gt;\n\n\nSometimes you might find it useful to filter something out, rather than specifying what should stay. This is done by the exclamation mark as you can see below.\n\ndata %&gt;% \n  filter(!is.na(Juveniles))\n\n# A tibble: 65 × 22\n   PlotID ForestType ForestTypeName      Herbs Juveniles CoverE1 Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      1          2 oak hornbeam forest    26        12      20    12.8\n 2      2          1 oak forest             13         3      25     9.9\n 3      3          1 oak forest             14         1      25    15.2\n 4      4          1 oak forest             15         5      30    16  \n 5      5          1 oak forest             13         1      35    20.7\n 6      6          1 oak forest             16         3      60    46.4\n 7      7          1 oak forest             17         5      70    49.2\n 8      8          2 oak hornbeam forest    21         1      70    48.7\n 9      9          2 oak hornbeam forest    15         4      15    13.8\n10     10          1 oak forest             14         4      75    79.1\n# ℹ 55 more rows\n# ℹ 15 more variables: Soil_depth_categ &lt;dbl&gt;, pH_KCl &lt;dbl&gt;, Slope &lt;dbl&gt;,\n#   Altitude &lt;dbl&gt;, Canopy_E3 &lt;dbl&gt;, Radiation &lt;dbl&gt;, Heat &lt;dbl&gt;,\n#   TransDir &lt;dbl&gt;, TransDif &lt;dbl&gt;, TransTot &lt;dbl&gt;, EIV_light &lt;dbl&gt;,\n#   EIV_moisture &lt;dbl&gt;, EIV_soilreaction &lt;dbl&gt;, EIV_nutrients &lt;dbl&gt;, TWI &lt;dbl&gt;\n\n\nSpecific alternative to filter is slice function. Let’s say I want to get top 3 rows/cases/vegetation samples with the highest numbers of recorded juveniles. So I can arrange the values and find out the threshold and filter for the values above it. Or I can use slice_max to do the job for me.\n\ndata %&gt;% \n  filter(!is.na(Juveniles)) %&gt;%\n  slice_max(Juveniles, n = 3) \n\n# A tibble: 3 × 22\n  PlotID ForestType ForestTypeName      Herbs Juveniles CoverE1 Biomass\n   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1    124          2 oak hornbeam forest    34        15      75   102. \n2    112          2 oak hornbeam forest    32        14      60    58.4\n3    119          2 oak hornbeam forest    63        14      50    36.6\n# ℹ 15 more variables: Soil_depth_categ &lt;dbl&gt;, pH_KCl &lt;dbl&gt;, Slope &lt;dbl&gt;,\n#   Altitude &lt;dbl&gt;, Canopy_E3 &lt;dbl&gt;, Radiation &lt;dbl&gt;, Heat &lt;dbl&gt;,\n#   TransDir &lt;dbl&gt;, TransDif &lt;dbl&gt;, TransTot &lt;dbl&gt;, EIV_light &lt;dbl&gt;,\n#   EIV_moisture &lt;dbl&gt;, EIV_soilreaction &lt;dbl&gt;, EIV_nutrients &lt;dbl&gt;, TWI &lt;dbl&gt;\n\n\nTip: try slice_min if you need the lowest values.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#mutate",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#mutate",
    "title": "2 Data Manipulation",
    "section": "2.6 Mutate",
    "text": "2.6 Mutate\nMutate adds new variables that are functions of existing variables. It can also overwrite the original variable e.g. round the values.\nFor example, in the field I recorded separately juveniles of woody plants and all other herb-layer species. However, I want to sum these two values for each row/case/vegetation plot so that I can speak about overall species richness. I create new variable where I simply sum these two (note how easy it is with tidy names!).\n\ndata %&gt;% \n  mutate(SpeciesRichness = Herbs+Juveniles) %&gt;%\n  select(PlotID, SpeciesRichness, Herbs, Juveniles)\n\n# A tibble: 65 × 4\n   PlotID SpeciesRichness Herbs Juveniles\n    &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1      1              38    26        12\n 2      2              16    13         3\n 3      3              15    14         1\n 4      4              20    15         5\n 5      5              14    13         1\n 6      6              19    16         3\n 7      7              22    17         5\n 8      8              22    21         1\n 9      9              19    15         4\n10     10              18    14         4\n# ℹ 55 more rows\n\n\nSometimes you want to add variable, where all the rows/cases will get the same value. For example because you plan to join the data with other data or because it is useful for another mutate. This is also the very simple way how to change the data with abundances to presence/absence data.\n\ndata %&gt;% \n  mutate(selection = 1)%&gt;%\n  select(PlotID, selection, ForestType, ForestTypeName)\n\n# A tibble: 65 × 4\n   PlotID selection ForestType ForestTypeName     \n    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;              \n 1      1         1          2 oak hornbeam forest\n 2      2         1          1 oak forest         \n 3      3         1          1 oak forest         \n 4      4         1          1 oak forest         \n 5      5         1          1 oak forest         \n 6      6         1          1 oak forest         \n 7      7         1          1 oak forest         \n 8      8         1          2 oak hornbeam forest\n 9      9         1          2 oak hornbeam forest\n10     10         1          1 oak forest         \n# ℹ 55 more rows\n\n\nYou can also create a variable with more categories based on values of other variable using mutate ifelse You give the condition when it should be called low and what to do if the condition is not met - name it high.\n\ndata %&gt;%\n  mutate(Productivity = ifelse(Biomass&lt;60,\"low\",\"high\")) %&gt;% \n  select (PlotID, ForestTypeName, Productivity, Biomass) %&gt;%\n  print(n=20)\n\n# A tibble: 65 × 4\n   PlotID ForestTypeName      Productivity Biomass\n    &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;\n 1      1 oak hornbeam forest low             12.8\n 2      2 oak forest          low              9.9\n 3      3 oak forest          low             15.2\n 4      4 oak forest          low             16  \n 5      5 oak forest          low             20.7\n 6      6 oak forest          low             46.4\n 7      7 oak forest          low             49.2\n 8      8 oak hornbeam forest low             48.7\n 9      9 oak hornbeam forest low             13.8\n10     10 oak forest          high            79.1\n11     11 oak forest          low              7.4\n12     14 oak hornbeam forest low             14.2\n13     16 oak hornbeam forest low             26.2\n14     18 oak hornbeam forest high            72.2\n15     28 oak forest          low             29.7\n16     29 oak hornbeam forest low             37.9\n17     31 oak forest          low             14.9\n18     32 oak hornbeam forest low             19.2\n19     36 oak hornbeam forest low              3.3\n20     41 oak hornbeam forest low             26.9\n# ℹ 45 more rows\n\n\nThe same can be done with similar approach using mutate cases when\n\ndata %&gt;% \n  mutate(Productivity = case_when(Biomass &lt;= 60 ~ \"low\", Biomass &gt; 60 ~ \"high\")) %&gt;% \n  select (PlotID, ForestTypeName, Productivity, Biomass) %&gt;%\n  print(n=20)\n\n# A tibble: 65 × 4\n   PlotID ForestTypeName      Productivity Biomass\n    &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;\n 1      1 oak hornbeam forest low             12.8\n 2      2 oak forest          low              9.9\n 3      3 oak forest          low             15.2\n 4      4 oak forest          low             16  \n 5      5 oak forest          low             20.7\n 6      6 oak forest          low             46.4\n 7      7 oak forest          low             49.2\n 8      8 oak hornbeam forest low             48.7\n 9      9 oak hornbeam forest low             13.8\n10     10 oak forest          high            79.1\n11     11 oak forest          low              7.4\n12     14 oak hornbeam forest low             14.2\n13     16 oak hornbeam forest low             26.2\n14     18 oak hornbeam forest high            72.2\n15     28 oak forest          low             29.7\n16     29 oak hornbeam forest low             37.9\n17     31 oak forest          low             14.9\n18     32 oak hornbeam forest low             19.2\n19     36 oak hornbeam forest low              3.3\n20     41 oak hornbeam forest low             26.9\n# ℹ 45 more rows\n\n\nI have already said that we often use mutate to directly transform original values. For example I want to round the numbers in several variables at once, so I use mutate across\n\ndata %&gt;% \n  mutate(across(c(Biomass, pH_KCl, TransTot), round)) %&gt;%\n  select(PlotID, Biomass, pH_KCl,TransTot)\n\n# A tibble: 65 × 4\n   PlotID Biomass pH_KCl TransTot\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1      1      13      5        7\n 2      2      10      3        7\n 3      3      15      4        7\n 4      4      16      4        6\n 5      5      21      4        7\n 6      6      46      4        7\n 7      7      49      3        7\n 8      8      49      4        7\n 9      9      14      4        7\n10     10      79      4        6\n# ℹ 55 more rows",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#save-the-data",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#save-the-data",
    "title": "2 Data Manipulation",
    "section": "2.7 Save the data",
    "text": "2.7 Save the data\nIf you decide that you already prepared the dataset in the form it should be used later, you probably need to save it and export e.g. as csv. Remember, that we were mostly trying how it would look like with the %&gt;%, so you have to either assign the dataset to a new name, or add the write command at the end of the pipeline. See also chapter1.\nHere we will play a bit with tidyverse package readr.\nWrite a comma delimited file\n\nwrite_csv(data, \"exercisesIA/forest_selected1.csv\") \n\nWrite a semicolon delimited file “;”\n\nwrite_csv2(data, \"exercisesIA/forest_selected2.csv\")\n\nor find out more in the readr cheatsheet. Note that in contrast to write.csv functions the write_csv keeps the philosophy of tidyverse, so do not expect row names in the export.\nYou can also write directly Excel file, for example, with writexl library.\n\nlibrary(writexl)\nwrite_xlsx(data, 'exercisesIA/forestEnv.xlsx')\n\nTo create an xlsx with (multiple) named sheets, you would need to provide a list of data frames.Here I specify that data1 should be saved as an Excel sheet forestEnv (environmental data) and data2 as a sheet forestSpe (species data of the same dataset).\n\nlibrary(writexl)\ndata1 &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-understory-diversity-analyses.xlsx\")\ndata2 &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-spe.xlsx\")\nwrite_xlsx(list(forestEnv = data1, forestSpe = data2), 'exercisesIA/forest.xlsx' )",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#exercises",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#exercises",
    "title": "2 Data Manipulation",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\n1. Forest Data - Axmanova-Forest-understory-diversity-analyses.xlsx - download the data from the repository and save it into your project folder Link to Github folder This dataset is used in the whole chapter, please use it to prepare your own script with remarks, copy and train what is described above.\nTip: If you copy the description and add it directly to your script, it is simply too long, right? Do you know how to read long lines of text in R script? Go to “Code” and tick “soft wrap long lines”.\n2. Directly from R studio create folder “results/chapter2” for saving todays datasets, as we will train a bit of exporting the data.\n3. Use the Forest Data to train more. Create variable “author” which will contain name Axmanova &gt; as we did before, create variable richness summing the herbs and juveniles &gt; select variables plotID, author, richness, productivity (renamed biomass) and pH. Try to save this subset into “results/chapter2”.\n4.Use the Forest dataset again. Find five plots with lowest biomass, from other forest types than oak forests.\n5. Select PlotID and all variables that are connected to transmitted light (Trans..) &gt; round these variables.\n6. We will switch to another dataset, Species Forest Data - Axmanova-Forest-spe.xlsx - download the data from the repository and save it into your project folder Link to Github folder This is a long-format of the plant species recorded in the vegetation plots. Long format means that the records of species from one site are grouped by the same ID (here as RELEVENr). For each species, it is indicated in which vegetation layer it occurred and estimation of the abundance in the form of percentage cover.\nImport the data and check the structure &gt; change it to presence absence data (abundance set to 1) *Alternatively prepare presence absence data without considering different layers and save both lists on two different sheets of Excel file.\n7. Stay with the same dataset, we will try to get the list of all species occuring in the tree layer. Import the data&gt; sort the species data alphabetically &gt; get the list of unique species names of the tree layer i.e. layer 1 &gt; print, view all the species or store them as a new dataset. Save the result into csv file.\n8. iris dataset We will use the famous iris dataset of flower measurements. Find out more details asking R “?iris” as the dataset is integrated and ready to use. Check the structure of the dataset &gt; select variables including species and those of length measurements &gt; arrange according to Sepal.Length &gt; and filter 15 rows with the longest sepals. Which species prevails among these top 15?\n9. squirrel data Load data of squirrel observations from the Central Park Squirrel Census using this line: read_csv(‘https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-23/squirrel_data.csv’)\nLoad squirrels data, check the structure, rename variables to get tidy names &gt; find out what are the possible colors of fur &gt; select at least 5 variables, include ID, fur color, location,.. &gt; filter squirrels of one fur color, up to your preference &gt; arrange data by location &gt; print at least 30 rows\n10.*get back to Forest data and using case when define three levels of productivity low, medium, high",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/2_data_manipulation.html#further-reading",
    "href": "DataManipulationVisualisation/2_data_manipulation.html#further-reading",
    "title": "2 Data Manipulation",
    "section": "2.9 Further reading",
    "text": "2.9 Further reading\ndplyr main web page https://dplyr.tidyverse.org\nFind dplyr Cheatsheet in Posit https://posit.co/resources/cheatsheets/\nChapter devoted to Data transformation in the R for data science book https://r4ds.hadley.nz/data-transform.html\nencoding issues during import and export https://irene.rbind.io/post/encoding-in-r/",
    "crumbs": [
      "Data Manipulation and Visualization",
      "2 Data manipulation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html",
    "href": "DataManipulationVisualisation/12_github.html",
    "title": "12 GitHub",
    "section": "",
    "text": "Let’s start with Git (shortcut for Global Information Tracker). It is a version control system, which means a software tool to track, manage and organise changes to files over time. It’s like the ‘Track Changes’ mode in Microsoft Word, but more powerful and scaled up to track multiple files in a folder called a repository (repo). It tracks and records changes to a file and allows you to recall a specific version at any time later. It was originally developed and used by professional software developers, but it also provides many advantages for data science. The data science community uses it not only for tracking source code for the analysis, but also source data, results and figures.\n\n\n\n\n\nA huge advantage comes when you start to collaborate with other people. This is where hosting services for Git-based projects come in. We already showed you a little bit of GitHub in this course, but we will explain its functionalities more consistently and in more detail now. Git is a software that you have locally on your computer. GitHub is the host server, allowing for collaboration of multiple people on the same project. Imagine it as if you have a Word document where you work on your own, but move it to Google Docs so that more people can collaborate on a single file. But there’s much more than in Google Docs. With GitHub, you can, for example:\n\ncreate a repository to collaborate on a certain analysis with several people\npublish data and code for the analysis you did in your paper, and link the repository to Zenodo to store them permanently\nfind and download data or code from someone else’s repository\npublish an R package, or download an R package published there\nuse Issues to track problems in the code\nhost a webpage like this one",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#git-and-github---what-is-it-and-what-is-it-good-for",
    "href": "DataManipulationVisualisation/12_github.html#git-and-github---what-is-it-and-what-is-it-good-for",
    "title": "12 GitHub",
    "section": "",
    "text": "Let’s start with Git (shortcut for Global Information Tracker). It is a version control system, which means a software tool to track, manage and organise changes to files over time. It’s like the ‘Track Changes’ mode in Microsoft Word, but more powerful and scaled up to track multiple files in a folder called a repository (repo). It tracks and records changes to a file and allows you to recall a specific version at any time later. It was originally developed and used by professional software developers, but it also provides many advantages for data science. The data science community uses it not only for tracking source code for the analysis, but also source data, results and figures.\n\n\n\n\n\nA huge advantage comes when you start to collaborate with other people. This is where hosting services for Git-based projects come in. We already showed you a little bit of GitHub in this course, but we will explain its functionalities more consistently and in more detail now. Git is a software that you have locally on your computer. GitHub is the host server, allowing for collaboration of multiple people on the same project. Imagine it as if you have a Word document where you work on your own, but move it to Google Docs so that more people can collaborate on a single file. But there’s much more than in Google Docs. With GitHub, you can, for example:\n\ncreate a repository to collaborate on a certain analysis with several people\npublish data and code for the analysis you did in your paper, and link the repository to Zenodo to store them permanently\nfind and download data or code from someone else’s repository\npublish an R package, or download an R package published there\nuse Issues to track problems in the code\nhost a webpage like this one",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#find-and-download-data-and-code",
    "href": "DataManipulationVisualisation/12_github.html#find-and-download-data-and-code",
    "title": "12 GitHub",
    "section": "12.2 Find and download data and code",
    "text": "12.2 Find and download data and code\nhow to find, download code from someone without an account (examples - maps course, published codes for articles)",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#set-up-your-own-account",
    "href": "DataManipulationVisualisation/12_github.html#set-up-your-own-account",
    "title": "12 GitHub",
    "section": "12.3 Set up your own account",
    "text": "12.3 Set up your own account\nAt the beginning, this is the most demanding task - to set up your own GitHub account and connect it to your RStudio, but once it works, you’ll be able to make your own GitHub repository to share your work with others and much more. We will now go through the setup together.\n\n12.3.1 Create a GitHub account\nFirst of all, you’ll need to create your own account at GitHub.com. It is worth paying some attention to your username, which should preferably include your name. You can change it later, but it’s better not to do it.\n\n\n12.3.2 Install Git\nInstall Git if you don’t have it installed already. You will have to use the command line to interact with Git, you can find more information here. You can launch it directly from RStudio using the Terminal window, which you will usually find next to the Console.\n\nType which git to see if you have Git installed somewhere in your computer.\nwhich git\nThe following line will tell you which version you are using.\ngit --version\nIf you get a message like git: command not found, you’ll have to download and install Git for Windows or macOS. Look here for more guidance.\n\n\n12.3.3 Introduce yourself to Git\nThis is possible either using the Terminal:\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email \"jane@example.com\"\ngit config --global --list\nor using the usethis package:\n\nlibrary(usethis)\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")\n\nReplace the example name and email with your username and email associated with your GitHub account.\n\n\n12.3.4 Personal access tokens\nWe now have to make sure that RStudio can communicate with Git. Confirm, that Git has your username and email by running\n\nuse_git()\n\nThen, generate a personal access token using\n\ncreate_github_token()\n\nClick ‘Generate token’ and copy the token to a safe place.\nThen, add the personal access token (PAT) to RStudio:\n\ngitcreds::gitcreds_set()\n\nPaste the PAT in response to the dialogue in the console:\n? Enter password or token: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \n-&gt; Adding new credentials... \n-&gt; Removing credentials from cache... \n-&gt; Done.\nYou should now be able to work with GitHub from your RStudio. Look here to read more about personal access tokens and GitHub-RStudio configuration.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#create-a-github-repository",
    "href": "DataManipulationVisualisation/12_github.html#create-a-github-repository",
    "title": "12 GitHub",
    "section": "12.4 Create a GitHub repository",
    "text": "12.4 Create a GitHub repository\nGo to https://github.com and log in. You might see something like this:\n\nClick on the green button to create a new repository. Enter the name of your first repository. You can also add a brief description. Let the repository be public and initialise a README file by clicking on the ‘Add README’ button. Then click the green button ‘Create a repository’ at the bottom of the page.\nTo link the repository to a folder in your computer, click the green button ‘&lt;&gt; Code’ and copy the URL to clipboard\n\n\n\n\n\nIn RStudio, start a new project. Instead of the usual ‘New Directory’ click on ‘Version Control’.\n\n\n\n\n\nChoose ‘Git’.\n\n\n\n\n\nAnd enter the URL of your new repository.\n\n\n\n\n\nChoose a folder where you want your project to be saved. It might be the folder you use for this course. Click ‘Create Project’. This will create an R project in your computer linked with the remote GitHub repository.\nLook in the file browser pane if you find a README file there and open it in RStudio.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#local-changes-commit-push",
    "href": "DataManipulationVisualisation/12_github.html#local-changes-commit-push",
    "title": "12 GitHub",
    "section": "12.5 Local changes, commit, push",
    "text": "12.5 Local changes, commit, push\nModify the README file, e.g. by adding a line ‘This is a line from RStudio’ and create a new R script with a few lines. Save your changes.\nNow, commit your changes to your local repo. Open the ‘Git’ tab.\n\nTick the ‘Staged’ boxes for the files you changed and click on ‘Commit’\n\nYou will see a popup window showing the changes you made. Enter a Commit message and click ‘Commit’.\n\nNow you have committed the changes locally. In the top left of the popup window, you will see a message, that ‘your branch is ahead of ’origin/main’ by one commit. It means you have locally committed some changes that are not synchronised with the remote counterpart yet. To send your changes to the GitHub remote repository (push), click on the green ‘Push’ button.\n\nIf successful, your changes should now be visible in the GitHub repository. Refresh the webpage with the GitHub repo and check it.\nIt is a good practice to provide informative commit messages. In case you need to return to a specific version later, it will be easier to find it.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#remote-changes-pull",
    "href": "DataManipulationVisualisation/12_github.html#remote-changes-pull",
    "title": "12 GitHub",
    "section": "12.6 Remote changes, pull",
    "text": "12.6 Remote changes, pull\nLet’s stay in the remote GitHub repository for a while and try another way to modify files in the repository. At the GitHub webpage, click on the pencil at the top of the readme file to edit it.\n\nAdd a line, e.g., ‘This is a line added remotely.’ and click on the green button ‘Commit changes…’.\n\nAdd a commit message and click on ‘Commit changes’.\n\n\n\n\n\nNow return to your RStudio, which is now one commit behind its remote counterpart. To synchronise the local repository with the remote, click on the blue ‘Pull’ arrow to pull the changes to your local repository. You can do that either directly in the Git window:\n\nOr first open the Commit popup and then click on the blue arrow.\n\n\nBoth ways lead to the same result: you will see a popup window showing what changed and get all the changes made remotely to your local repository.\n\nIf you see something like this, you can just close the window. The problem is, if you get some error message that the pull was not successful, but hopefully you don’t.\nIt is not so useful to modify files in the remote repository. Except for the readme file, we do not recommend adding or modifying files directly on the GitHub webpage. The reason is, you can do whatever you want in your local repository and then commit and push the changes, but not much is possible to modify on the webpage, and you cannot directly test if the scripts work.\nThe ‘Pull’ button becomes much more useful when you start collaborating with others on the same repository. Each of you has a local copy of the repository, and all of you commit and push changes to the same remote repository. Once you commit your changes and push them to the remote repo, your colleague has to pull them to synchronise his/her local repository with yours via the remote.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#gitignore",
    "href": "DataManipulationVisualisation/12_github.html#gitignore",
    "title": "12 GitHub",
    "section": "12.7 .gitignore",
    "text": "12.7 .gitignore\nIn your local repository, you can also have some files that you don’t want to be tracked. You can add to your repository file called .gitignore to tell Git which files should be ignored, i.e, not be tracked. For example:\n# History files\n.Rhistory\n\n# RStudio files\ntestrepo.Rproj\n\n# data - ignores everything in the data folder\ndata/*\n\n# figures - ignores all figures\n*.png\n*.jpg",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#collaboration-with-others",
    "href": "DataManipulationVisualisation/12_github.html#collaboration-with-others",
    "title": "12 GitHub",
    "section": "12.8 Collaboration with others",
    "text": "12.8 Collaboration with others\n\n12.8.1 Add collaborators\nA huge advantage of GitHub or other hosting services is that you can collaborate with others on the same project. You can add collaborators to your repository by clicking on ‘Settings’ &gt; ‘Collaborators’ &gt; ‘Add people’.\n\nType the username of your colleague and click ‘Add to repository’. Your colleague will receive an email invitation and has to accept it. Now he/she can clone the repository to his/her computer and get another local repository.\n\n\n12.8.2 Basic workflow\nBoth of you can now make some changes, commit them and push to the remote repository. To keep updated and not overwrite changes of the other, you will need to pull changes from the remote that someone else made before you push your changes. Usually, the sequence would be: pull &gt; make some changes &gt; commit &gt; push.\nIt is not a very good idea for two or more people to work on the same file at the same time, because you can overwrite the changes of the other person. But the project might work very well if you divide your roles somehow, for example, Alice is responsible for the data preparation, Bob for the subsequent analysis, and Cecilia for the final visualisation. Ideally, each of them has its own script and doesn’t overwrite the code of the other, although he/she uses previous steps made by someone else.\n\n\n12.8.3 Branches\nBranching means that you take a detour from the main stream of development and do some work in parallel, without changing the main stream. It might be useful to experiment with some idea, when you are not sure it will work. Or when working with others, you can do your own work on a branch without overwriting someone else’s work and then merge it back later on.\nAt GitHub, if you look at the ‘Code’ and click on the button with the branch symbol on the left, you can see what branches the repository includes. You will probably see only one branch called ‘main’, which is the default.\n\nYou can create a new branch here. Type the branch name into the search box. You can add, for example, your initials to create your branch in a repo with multiple collaborators. The click on the Create branch button.\n\nIn the remote repository, you are now switched to the branch you just created. Come back to the local in your RStudio. To switch to your branch there, pull the remote changes first. Then click on the branch name in the upper right corner of the Git pane and then select your branch.\n\n\n\n\n\nYou should see something like this:\n&gt;&gt;&gt; C:/Program Files/Git/bin/git.exe checkout -b KK origin/KK\nbranch 'KK' set up to track 'origin/KK'.\nSwitched to a new branch 'KK'\nNow you can make some changes (for example add a new script), commit them and push to the remote. Check the GitHub webpage, if the changes are visible there. You should see something like this:\n\n\n\n12.8.4 Pull request\nTo merge the changes back to the main branch, you can make a pull request. Click on the green button ‘Compare & pull request’.\n\nYou can now change the title and add a brief description. You can also check the changes when you scroll down. Then click on the green button ’Create a pull request.\n\nThe changes you made are still not in the main branch, you just created a request to merge them into the main branch. The second step is to merge the pull request. You can either do it directly by clicking on the green button ‘Merge pull request’.\n\nAnd then ‘Confirm merge’.\n\nOr if you are collaborating with others on this project and you have someone as a project manager, you can (and probably should) leave the pull request open, so that the project manager can check which changes you made and decide to include them into the main branch or not.\nFor now, you can merge the pull request and check if you see the changes made on your branch in the main branch.\n\n\n12.8.5 Merge conflicts\nMost of the time, the merge will go smoothly. However, if both branches that are merging changed the same part of the same file in the meantime, you will get a merge conflict. The same happens, when you pull/push changes between local and remote in case both of them changed the same part of the same file.\nDo not panic when this happens. It’s usually not so difficult to resolve the merge conflict. The first step is to determine, which files are in conflict. Then open the file to see, which lines are in conflict. You will see something like this:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; KK\n\nThis is a line from RStudio. Modified in different way.\n\n======= \n\nThis is a line from RStudio. Modified.\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main \n\n\n\n\n\n\n\nWhere the lines between &lt;&lt;&lt;&lt;&lt;&lt;&lt; KK and ======= are the content from the branch you are currently on. The lines between ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt; main are from the branch we are merging. To resolve the conflict, we have to edit this section, either keep one of the two version or come with a new one. We also have to remove the conflict markers. Now we can commit the changes to finalize the merge.\nHow to avoid merge conflicts:\n\ncommit often, work in small steps\npush and pull regularly\norganise your code in multiple scripts, one for each task (e.g., data preparation, analysis, final visualisation)\norganise the workflow in multiple steps and divide the work on different part of the code between the collaborators, do not work on the same thing simultaneously",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#time-travel",
    "href": "DataManipulationVisualisation/12_github.html#time-travel",
    "title": "12 GitHub",
    "section": "12.9 Time travel",
    "text": "12.9 Time travel\nreview history, go back to specific version",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#glossary",
    "href": "DataManipulationVisualisation/12_github.html#glossary",
    "title": "12 GitHub",
    "section": "12.10 Glossary",
    "text": "12.10 Glossary\n\nrepository (repo)\n\nthe directory or folder for version control\n\nlocal\n\nthe repository on your computer\n\nremote\n\nthe repository on GitHub\n\nclone\n\nget some work from a remote for the first time\n\nstage\n\nchoose the changes you want to record before you commit them\n\ncommit\n\na record of change. If you create or edit a file in your repository and save the changes, you need to record your changes via a commit.\n\nhash\n\ncommit id\n\npush\n\nsend commits from the local repo to the remote\n\npull\n\nretrieve commits from the remote repo to the local\n\ndiff\n\ndifferences between two commits\n\nbranch\n\na stream of work development\n\npull request\n\nrequest to merge one branch to another\n\nmerge\n\ncombine two branches\n\nmerge conflict\n\nwhen the same part of the same file changed in two different sources (two branches, or local and remote) and are not able to merge automatically\n\ncheckout\n\ntime travel to a specific commit in the past",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#other-tips",
    "href": "DataManipulationVisualisation/12_github.html#other-tips",
    "title": "12 GitHub",
    "section": "12.11 Other Tips",
    "text": "12.11 Other Tips\n\nYou can change use private repositories for work in progress which includes data and scripts that are not published yet and you don’t want to make them public, e.g., before publishing a scientific paper based on the analysis.\nIf you are a student or a university researcher, you can get access to GitHub Education, which provides some benefits, such as free private repositories with more than three collaborators, access to GitHub Copilot, etc.\nYou can use the connection between GitHub and Zenodo to publish data and scripts associated with your scientific paper and get DOI for them. Look here for the instructions.\nIt is also possible to create and host a webpage like this one through GitHub. Some researchers create their personal webpage this way. To do that you will have to learn basics of Quarto, but don’t be afraid, it is not so difficult. We recommend looking here for some introduction to Quarto and here to explore more about GitHub pages.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#exercises",
    "href": "DataManipulationVisualisation/12_github.html#exercises",
    "title": "12 GitHub",
    "section": "12.12 Exercises *",
    "text": "12.12 Exercises *\n\nCreate a new GitHub repository, connect it with your RStudio and create a version control project on your computer. Create a usual project structure from your local repository (add folders for data, scripts, plots, etc.). Add a readme and some scripts or data from this course. Commit the changes and push them to the remote repository. Check if everything works.\nAdd collaborators (e.g., those who sit next to you) to your repository.\nMake this repository private.\nCreate a branch for each of you.\nSwitch to your branch, make some changes, commit them and push to the remote.\nCreate a pull request to merge your changes with the main branch.\nDelete the repository.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/12_github.html#further-reading",
    "href": "DataManipulationVisualisation/12_github.html#further-reading",
    "title": "12 GitHub",
    "section": "12.13 Further reading",
    "text": "12.13 Further reading\nHappy Git and GitHub for the useR: https://happygitwithr.com\nBioStats Version Control with Git and GitHub: https://biostats-r.github.io/biostats/github\nFlight rules for Git (what to do when things go wrong): https://github.com/k88hudson/git-flight-rules\nHow to get out of terrible situations: https://ohshitgit.com\nOpen source game to learn Git: https://ohmygit.org\nGitHub Training Kit: https://training.github.com\nGitHub learning sources: https://docs.github.com/en/get-started/start-your-journey/git-and-github-learning-resources\nTen simple rules for taking advantage of Git and GitHub: https://doi.org/10.1371/journal.pcbi.1007142\nRStudio Version Control Guide: https://docs.posit.co/ide/user/ide/guide/tools/version-control.html\nPro Git: https://git-scm.com/book/en/v2",
    "crumbs": [
      "Data Manipulation and Visualization",
      "12 GitHub"
    ]
  },
  {
    "objectID": "CommunityEcology/community_ecology.html",
    "href": "CommunityEcology/community_ecology.html",
    "title": "Data Analysis in Community Ecology",
    "section": "",
    "text": "Cookbook might be moved here",
    "crumbs": [
      "Community Ecology"
    ]
  },
  {
    "objectID": "AIinTeaching/ai_in_teaching.html",
    "href": "AIinTeaching/ai_in_teaching.html",
    "title": "AI in Data Analysis",
    "section": "",
    "text": "AI tools, and especially LLMs (large language models) have been widely available for just a few years, basically for just a little while. However, they have managed to gain a die-hard fan base, as well as a group of people who see them nearly as a harbinger of the end of the world, or at least of the world as we know it (Kokotajlo et al., 2025). So, everything we are trying to point out here may soon become completely irrelevant…\nMany people use it regularly, including the authors of this leaflet. The most common things we use it for are coding tasks (mainly script debugging), literature search, summarising long texts, explaining things or text editing. AI tools can give better search results than traditional search engines, can help with learning new analytical methods, new programming languages, and it can also make interdisciplinary research easier (Mammides & Papadopoulos, 2024; RStudioDataLab, 2023). We want to encourage you to play, experiment with AI tools, and try to understand how they work and where the limits of their usage lie, too. But keep in mind that the content of your conversations with AI tools (i.e. data, information about yourself) may be used for further improvement of the model, especially in free versions. So, even if some tools (e.g. Chat GPT) allows you to turn off chat history or delete your conversations, think twice before you share anything and do not send sensitive data to LLMs.\nOne of the important (and still not really resolved) issues of extensive usage for LLMs relates to copyright. If you let an LLM write a homework, an essay or even a whole article, who is actually the author of the whole thing? Is this a case of plagiarism? Isn’t it fair to acknowledge the authorship of the AI tool, too? How? Should, e.g., ChatGPT be credited as high as a co-author? Who is responsible for the correctness and accuracy of such text (The group for AI in teaching at Masaryk University, 2023; Wu et al., 2024)? It is also useful to realize that the quality of an AI-generated text originates from thousands of well-written texts by professional writers (Wu et al. 2024) whose work is usually not credited in the AI output. When you are a researcher, such things as true authorship of ideas, text, credibility and authenticity of your outcomes will be the questions you will definitely encounter (Johnson et al., 2024; The group for AI in teaching at Masaryk University, 2023; Wu et al., 2024). It might (or might not, who knows) also happen that large (uncredited) usage of LLMs in writing research papers can be reviewed as unacceptable in the future, and as such, it might even discredit your whole research work (Johnson et al., 2024). Many journals and institutions (even MUNI, check this link) have prepared guidelines regarding AI usage and its reporting. Usually, usage of AI must be reported when it is used to write longer parts of text (e.g., abstract), or to analyse or visualise data. However, broader consensus on AI use reporting is still missing and therefore it is necessary to check the specific rules and conditions set by each institution, journal, or lecturer before using AI tools.\nOther ethical issues come up directly from the generated content. LLMs can suffer from issues such as training data poisoning or improper data sanitization, which may lead to significant biases in the outcomes (Johnson et al., 2024; Wu et al., 2024). LLMs are also known to hallucinate non-existent stuff, e.g. literature references or names of software packages. Especially in the case of software packages, this can pose a considerable security risk. Such packages can then be published by hackers with harmful code, which you then unsuspectingly run on your own computer (Françoisn - f@briatte.org, 2025; Mammides & Papadopoulos, 2024; Wu et al., 2024).\nHeavy reliance on AI tools may also prevent you from learning crucial research skills, such as formulating your own ideas, deep understanding of your topic, various analytical approaches, critical reasoning, etc. (Johnson et al., 2024; Millard et al., 2024). The usage of AI tools also raises equity questions, especially in regard to paid versions. Naturally, LLMs will be more important for non-native English speakers, but the access to higher-level—but often paid—tools is not available for all, especially for people from low-income countries (Campbell et al., 2024; Wu et al., 2024).\nThe flip side of AI tools also includes their environmental impact. Training and tuning a single LLM produces more CO2 emissions than the average American does in his entire lifetime (Strubell et al., 2019). Using LLM chatbots for one year generates 25 times more CO2 emissions than training the GPT-3 model. This does not even consider the infrastructure and equipment needed, which requires a lot of water and mining of rare elements, causes contamination, and more (Chien et al., 2023; Cooper et al., 2024).",
    "crumbs": [
      "AI in Data Analysis"
    ]
  },
  {
    "objectID": "AIinTeaching/ai_in_teaching.html#ethics",
    "href": "AIinTeaching/ai_in_teaching.html#ethics",
    "title": "AI in Data Analysis",
    "section": "",
    "text": "AI tools, and especially LLMs (large language models) have been widely available for just a few years, basically for just a little while. However, they have managed to gain a die-hard fan base, as well as a group of people who see them nearly as a harbinger of the end of the world, or at least of the world as we know it (Kokotajlo et al., 2025). So, everything we are trying to point out here may soon become completely irrelevant…\nMany people use it regularly, including the authors of this leaflet. The most common things we use it for are coding tasks (mainly script debugging), literature search, summarising long texts, explaining things or text editing. AI tools can give better search results than traditional search engines, can help with learning new analytical methods, new programming languages, and it can also make interdisciplinary research easier (Mammides & Papadopoulos, 2024; RStudioDataLab, 2023). We want to encourage you to play, experiment with AI tools, and try to understand how they work and where the limits of their usage lie, too. But keep in mind that the content of your conversations with AI tools (i.e. data, information about yourself) may be used for further improvement of the model, especially in free versions. So, even if some tools (e.g. Chat GPT) allows you to turn off chat history or delete your conversations, think twice before you share anything and do not send sensitive data to LLMs.\nOne of the important (and still not really resolved) issues of extensive usage for LLMs relates to copyright. If you let an LLM write a homework, an essay or even a whole article, who is actually the author of the whole thing? Is this a case of plagiarism? Isn’t it fair to acknowledge the authorship of the AI tool, too? How? Should, e.g., ChatGPT be credited as high as a co-author? Who is responsible for the correctness and accuracy of such text (The group for AI in teaching at Masaryk University, 2023; Wu et al., 2024)? It is also useful to realize that the quality of an AI-generated text originates from thousands of well-written texts by professional writers (Wu et al. 2024) whose work is usually not credited in the AI output. When you are a researcher, such things as true authorship of ideas, text, credibility and authenticity of your outcomes will be the questions you will definitely encounter (Johnson et al., 2024; The group for AI in teaching at Masaryk University, 2023; Wu et al., 2024). It might (or might not, who knows) also happen that large (uncredited) usage of LLMs in writing research papers can be reviewed as unacceptable in the future, and as such, it might even discredit your whole research work (Johnson et al., 2024). Many journals and institutions (even MUNI, check this link) have prepared guidelines regarding AI usage and its reporting. Usually, usage of AI must be reported when it is used to write longer parts of text (e.g., abstract), or to analyse or visualise data. However, broader consensus on AI use reporting is still missing and therefore it is necessary to check the specific rules and conditions set by each institution, journal, or lecturer before using AI tools.\nOther ethical issues come up directly from the generated content. LLMs can suffer from issues such as training data poisoning or improper data sanitization, which may lead to significant biases in the outcomes (Johnson et al., 2024; Wu et al., 2024). LLMs are also known to hallucinate non-existent stuff, e.g. literature references or names of software packages. Especially in the case of software packages, this can pose a considerable security risk. Such packages can then be published by hackers with harmful code, which you then unsuspectingly run on your own computer (Françoisn - f@briatte.org, 2025; Mammides & Papadopoulos, 2024; Wu et al., 2024).\nHeavy reliance on AI tools may also prevent you from learning crucial research skills, such as formulating your own ideas, deep understanding of your topic, various analytical approaches, critical reasoning, etc. (Johnson et al., 2024; Millard et al., 2024). The usage of AI tools also raises equity questions, especially in regard to paid versions. Naturally, LLMs will be more important for non-native English speakers, but the access to higher-level—but often paid—tools is not available for all, especially for people from low-income countries (Campbell et al., 2024; Wu et al., 2024).\nThe flip side of AI tools also includes their environmental impact. Training and tuning a single LLM produces more CO2 emissions than the average American does in his entire lifetime (Strubell et al., 2019). Using LLM chatbots for one year generates 25 times more CO2 emissions than training the GPT-3 model. This does not even consider the infrastructure and equipment needed, which requires a lot of water and mining of rare elements, causes contamination, and more (Chien et al., 2023; Cooper et al., 2024).",
    "crumbs": [
      "AI in Data Analysis"
    ]
  },
  {
    "objectID": "AIinTeaching/ai_in_teaching.html#principles-of-llms",
    "href": "AIinTeaching/ai_in_teaching.html#principles-of-llms",
    "title": "AI in Data Analysis",
    "section": "Principles of LLMs",
    "text": "Principles of LLMs\nMany people often talk about LLMs as if they were real people. We all know, or perhaps even use, phrases like “ChatGPT told me…” or “He thinks that…”. Some people are used to having long conversations with them, and some even use them as psychotherapists (e.g., Lau et al. (2025)). However, the principles of how LLMs work are far from what we would consider “thinking”.\nThe basic principle of LLMs is predicting the next word, or more precisely, the next “token”, based on the sequence of the previous ones. Each token—typically a word or part of a word—is first transformed into an array of numbers (an embedding). Then the model assigns probabilities of occurrence to each given token, based on its occurrence in other situations. Based on the probabilities, a token is selected and added to the string, and the process starts again. Interestingly, the best-fitting word is not always selected. Instead, it often samples from a distribution of more, but still highly probable options, to introduce some variability. This way, the model produces what looks like a “reasonable continuation” of a text (Stanford CS324, 2022; Wolfram, 2023).\nAnd this is where it gets complicated. The “reasonable continuation” means “what a human would expect a text to look like”. It is derived from billions of websites, articles, and books, which were at some point downloaded from the internet and broken into tokens. The model learns co-occurrence patterns, context and sequences of tokens, which are then turned into probabilities (Stanford CS324, 2022; Wolfram, 2023). To put things in some perspective, the English language corpus includes around 40,000 words. However, even though those billions of web pages may seem enough for calculating probabilities of all the tokens, for many rare tokens, it is still not enough. This is where large neural networks, machine learning, and optimization algorithms step in (Wolfram, 2023).\nYou might have heard about general linear models, which are often used in ecology. Such models typically include a few parameters, and adding new terms (and parameters) can soon make the model quite demanding for computational capacity and also for the size of the dataset. The most advanced version of the GPT-3 model (not used anymore, but the most advanced model which the number of parameters and other information is publicly available for) has 175 billion parameters, its size is about 350 GB and the required RAM capacity is ca 300-700 GB (Brown et al., 2020) which is far beyond what a standard personal computer is able to run currently.\n\n\n\n\n\n\nThis part was consulted with ChatGPT for a correct description of terms and processes that I have often never heard about before and have just a very little idea how they really work.",
    "crumbs": [
      "AI in Data Analysis"
    ]
  },
  {
    "objectID": "AIinTeaching/ai_in_teaching.html#tips-for-using-llms",
    "href": "AIinTeaching/ai_in_teaching.html#tips-for-using-llms",
    "title": "AI in Data Analysis",
    "section": "Tips for using LLMs",
    "text": "Tips for using LLMs\nHere, we put together a list of (hopefully) useful tips for LLM (especially LLM-based chatbots, such as ChatGPT or Gemini) usage:\n\nDifferent tools are useful for different tasks\nChatGPT: Summaries, explanations, research, presentations, coding.\nGoogle Gemini: Helping with code, writing and research.\nMicrosoft Copilot: Writing, researching, coding, brainstorming, summarizing.\nLe Chat Mistral: Quick research, writing, brainstorming, summarizing, learning.\nPerplexity: My strengths: information retrieval, text generation, problem-solving, language understanding, multitask support, creativity.\nClaude.ai: Writing, coding, analysis, research, problem-solving.\n\n\n\n\n\n\nThese answers were generated by each AI tool. The prompt was: “Can you tell me what are your strengths? Like for what tasks people use you the most? Give me a 5-6 word answer.”\n\n\n\nOther AI tools without chat-like interface:\nDeepL: translator, AI-powered text editing\nQuillBot: translator, grammar-checker, paraphraser\nGrammarly: text editing\nElicit: literature search\nGamma: presentations\n\n\n\n\n\n\nWhen you want to compare more AI tools:\nLMArena: side by side comparison of different LLMs, up-to-date lists of best performing models in various tasks\n\n\n\n\n\nIn general\n\nDescribe your question like in a conversation with your coding/learning buddy. LLM can give you not only the direct answer, but it can also explain how the script/library/workflow/topic works (Campbell et al., 2024; Ellen, 2025). LLM can usually explain things in an easier language than online forums (Campbell et al., 2024)\nDon’t ask everything in one prompt, split complex tasks into smaller steps (Mammides & Papadopoulos, 2024; Vieira & Raymond, 2025; Willison, 2025b)\nFirst answer is usually not the best (Çetinkaya-Rundel, 2025)\nBe specific in what you want it to do (Willison, 2025b)\nContext is king (Willison, 2025b)\nDon’t ask about its opinion, better is to ask about consensus (in the topic you are focused on)\nAsk for references\nAsk for options (Willison, 2025b)\nGive examples (Willison, 2025a)\nTry different models to cross-check the outputs. Also, different models are good for different goals (Vieira & Raymond, 2025)\n“Just because code looks good and runs without errors doesn’t mean it’s doing the right thing.” (Willison, 2025a)\nDon’t trust everything, AI tools are overconfident “yes machines” and sometimes hallucinate (Mammides & Papadopoulos, 2024; Vieira & Raymond, 2025; Willison, 2025a)\nDon’t become too dependent on ChatGPT. Shutdowns, price, you should still know how to do things manually (Lubiana et al., 2023)\n\n\n\nPractical tips for data analysis and coding\n\nPrior knowledge of coding and statistics is necessary to provide sufficiently detailed prompt and use and interpret AI-generated code correctly (Campbell et al., 2024)\nSpecify the coding language, packages you want to use (e.g., tidyverse, vegan) (Cooper et al., 2024; Vieira & Raymond, 2025)\nIf you analyze the data with an LLM, always ask the LLM to generate an R script and test its functionality. Do not just pick the results and export the figures. This is essential for reproducibility and further adjustments. \nUse full sentences, with as much context as possible (Cooper et al., 2024)\nIf you are trying to resolve an error, copy not only the error message, but also the whole script where the error emerges from. This gives the LLM a context of the functions and libraries you used (Ellen, 2025)\n“Effective prompts include context, specify the topic, outline the desired output, concise, focused question” (Lubiana et al., 2023)\nWhen the conversation leads nowhere, start a new chat. Try to give the LLM a different context (Willison, 2025b)\nIt does not always change only the part of the script you ask it to, but also other parts without warning (Çetinkaya-Rundel, 2025)\nAlways check the script you want to apply (Willison, 2025a). Clean the suggested script from unnecessary parts, try to run it line-by-line to understand what is happening (Çetinkaya-Rundel, 2025)\nFrom time to time, consult also old-school browsers or StackOverflow, because LLMs can lack information about the latest updates. Also, you can find better (or different) approaches how to do certain things there (Ellen, 2025; Willison, 2025b)\nBefore you start questions (Davjekar, 2024):\n\n“I’m starting a new [type of project] using [programming language/framework]. Can you suggest a basic file structure and essential dependencies I should consider?”\n“I want to build [brief project description]. Can you help me break this down into smaller tasks and suggest an order of implementation?”\n\nQuestions to ask about the code (RStudioDataLab, 2023):\n\nWhy did you generate this code?\nWhat does this code do?\nHow can I fix this error?\nWhat are the alternatives to this code?\nHow can I improve this code?\n\nHandy prompts (Lubiana et al., 2023):\n\n“Add explanatory comments to this code:”\n“Rename the variables for clarity:”\n“Write me a standard GitHub README file for the above code.”\n“Extract functions for increased clarity:”\n“Re-write and optimize this for-loop:” \n “Write me regex for R/Python/Excel with a pattern that will extract {} from {}”\n“Create a ggplot2 violin plot with a log10 Y axis”",
    "crumbs": [
      "AI in Data Analysis"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/11_database_to_plot.html",
    "href": "DataManipulationVisualisation/11_database_to_plot.html",
    "title": "11 From a database to a plot",
    "section": "",
    "text": "(Real-world example for botanists) Use the data from the folder basiphilous_grasslands and recreate the following plot:\n\n\n\n\n\n\n\n\n\nNote that some data handling will be needed before you start plotting (transform the species abundance data from wide to long format, join species characteristics, calculate the number of species and number of specialists (THE_THF) for each plot (Releve), join header data and transform the variable for the time of sampling (Rs_observ) to a character/factor).",
    "crumbs": [
      "Data Manipulation and Visualization",
      "11 From a database to a plot"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/1_introduction.html",
    "href": "DataManipulationVisualisation/1_introduction.html",
    "title": "1 Introduction",
    "section": "",
    "text": "In this chapter, we will train how to get ready for analyses in R. Preparing the project, starting a script, importing data and their first exploration. We will follow the rules for preparing reproducible, reportable, clean and tidy workflow and scripts.\nYou already know that R is a programming language and environment for statistical analysis and data visualisation. R is free and open-source, available from the CRAN directory. R is extended by a large number of software packages, which contain reusable code, documentation, and sample data. In this course, we will focus in the first place on the tidyverse collection of packages, which are designed for everyday data handling and visualisation (see more here).\nWe will work with R using the interface called RStudio IDE, in short RStudio, which is an integrated development environment for R and can be downloaded here.\nTO DO: install R, RStudio and tidyverse package. Having trouble and need advice? Try further reading section at the end of this chapter or come and ask before the first lesson.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "1 Introduction"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/1_introduction.html#introducing-tidyverse",
    "href": "DataManipulationVisualisation/1_introduction.html#introducing-tidyverse",
    "title": "1 Introduction",
    "section": "1.1. Introducing Tidyverse",
    "text": "1.1. Introducing Tidyverse\n\n\n\n\n\nTidyverse is a collection of R packages for transforming and visualizing data, which share an underlying philosophy (tidy data, tibbles, %&gt;%) and common interface. When you install the tidyverse, you get all the core packages at once, namely\n\nreadr for data importing\n\ndplyr with tools for data manipulation (e.g. select, filter, arrange, mutate…)\ntidyr with functions that help you get to tidy data and transform their format (e.g.pivot)\ntibble introducing simple dataframes called tibbles\nggplot2 package for data visualisation\nstringrfor working with strings, matching defined patterns, clean unwanted parts\nforcats which enables easier work with factors\nlubridatehelping to work with date-times\npurr which offers complete and consistent set of tools for working with functions and vectors, introduces map function instead of complicated loops\n\nIn addition you get automatically installed also several more packages, which share the same approach, although developed later or by someone else, for example - readxl elegant direct import from Excel files - magrittrpackage where the pipe was originally introduced, including double-sided pipe\nFind more about tidyverse here or check cheatsheets and vignettes for individual packages.\nRemember, that all the core packages are activated within the tidyverse library\n\nlibrary(tidyverse)\n\nwhile for the extra ones you have to use an extra call\n\nlibrary(readxl)\n\n\n1.1.1 Tidy data\nData that are easy to handle and analyse. \nFind more in the R for data science book https://r4ds.hadley.nz/\n\n\n1.1.2 Tibbles\nTibbles are new, updated versions of base-R data frames. They are designed to work better with other tidyverse packages. In contrast to data frames, tibbles never convert the type of the inputs (e.g. strings to factors), they never change the names of variables, and they never create row names. Example:\n\ndata &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-understory-diversity-analyses.xlsx\")\ntibble (data)\n\n# A tibble: 65 × 22\n   PlotID ForestType ForestTypeName      Herbs Juveniles CoverE1 Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      1          2 oak hornbeam forest    26        12      20    12.8\n 2      2          1 oak forest             13         3      25     9.9\n 3      3          1 oak forest             14         1      25    15.2\n 4      4          1 oak forest             15         5      30    16  \n 5      5          1 oak forest             13         1      35    20.7\n 6      6          1 oak forest             16         3      60    46.4\n 7      7          1 oak forest             17         5      70    49.2\n 8      8          2 oak hornbeam forest    21         1      70    48.7\n 9      9          2 oak hornbeam forest    15         4      15    13.8\n10     10          1 oak forest             14         4      75    79.1\n# ℹ 55 more rows\n# ℹ 15 more variables: Soil_depth_categ &lt;dbl&gt;, pH_KCl &lt;dbl&gt;, Slope &lt;dbl&gt;,\n#   Altitude &lt;dbl&gt;, Canopy_E3 &lt;dbl&gt;, Radiation &lt;dbl&gt;, Heat &lt;dbl&gt;,\n#   TransDir &lt;dbl&gt;, TransDif &lt;dbl&gt;, TransTot &lt;dbl&gt;, EIV_light &lt;dbl&gt;,\n#   EIV_moisture &lt;dbl&gt;, EIV_soilreaction &lt;dbl&gt;, EIV_nutrients &lt;dbl&gt;, TWI &lt;dbl&gt;\n\n\n\n\n1.1.3 Pipes %&gt;%\nThe Tidyverse tools use a pipe: %&gt;% or |&gt; The pipe allows the output of a previous command to be used as input to another command instead of using nested functions. It means, pipe binds individual steps into a sequence and it reads from left to right. In base R the logic of reading is from inside out and you have to save all the steps separately.\nSee this example of the same steps with different approaches&gt;\nBase R method\n\ndata &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-understory-diversity-analyses.xlsx\")\n# 1. Create new variable in the data\ndata$Productivity &lt;- ifelse(data$Biomass &lt; 60, \"low\", \"high\")\n\n# 2. Select only some columns, save as new dataframe\ndf &lt;- data[, c(\"PlotID\", \"ForestTypeName\", \"Productivity\", \"pH_KCl\")]\n\n# 3. Order by soil pH (descending)\ndf &lt;- df[order(df$pH_KCl, decreasing = TRUE), ]\n\n# 4. Keep only the first 15 rows with highest pH\ndf_top20 &lt;- df[1:15, ]\n\n# 5. Print the resulting subset to see which forest types grow in the high pH soils and if they have rather low or high productivity\ndf_top20\n\n# A tibble: 15 × 4\n   PlotID ForestTypeName      Productivity pH_KCl\n    &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;         &lt;dbl&gt;\n 1    104 alluvial forest     high           7.14\n 2    103 alluvial forest     high           7.13\n 3    128 ravine forest       high           7.03\n 4    111 alluvial forest     high           7.01\n 5    114 ravine forest       high           6.93\n 6    123 ravine forest       low            6.91\n 7    105 ravine forest       high           6.78\n 8    122 ravine forest       high           6.68\n 9    101 alluvial forest     high           6.67\n10    119 oak hornbeam forest low            6.63\n11    113 alluvial forest     high           6.61\n12    120 oak hornbeam forest high           6.42\n13    115 ravine forest       low            6.03\n14    110 alluvial forest     high           5.82\n15    121 oak hornbeam forest high           5.77\n\n\nPiping (the same steps, but just in one line)\n\ndata &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-understory-diversity-analyses.xlsx\") %&gt;%\n  mutate(Productivity = ifelse(Biomass &lt; 100, \"low\", \"high\")) %&gt;%\n  select(PlotID, ForestTypeName, Productivity, pH_KCl) %&gt;%\n  arrange(desc(pH_KCl)) %&gt;%\n  slice_head(n = 15) %&gt;%\n  print()\n\n# A tibble: 15 × 4\n   PlotID ForestTypeName      Productivity pH_KCl\n    &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;         &lt;dbl&gt;\n 1    104 alluvial forest     high           7.14\n 2    103 alluvial forest     high           7.13\n 3    128 ravine forest       high           7.03\n 4    111 alluvial forest     low            7.01\n 5    114 ravine forest       low            6.93\n 6    123 ravine forest       low            6.91\n 7    105 ravine forest       low            6.78\n 8    122 ravine forest       high           6.68\n 9    101 alluvial forest     low            6.67\n10    119 oak hornbeam forest low            6.63\n11    113 alluvial forest     low            6.61\n12    120 oak hornbeam forest low            6.42\n13    115 ravine forest       low            6.03\n14    110 alluvial forest     high           5.82\n15    121 oak hornbeam forest low            5.77\n\n\nNote: It is even possible to overwrite original dataset with an assignment pipe %&lt;&gt;% included in magrittr package. We will try to avoid this in our lessons, as it cannot be undone.\nTip: Insert a pipe by ctrl+shift+M",
    "crumbs": [
      "Data Manipulation and Visualization",
      "1 Introduction"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/1_introduction.html#effective-and-reproducible-workflow",
    "href": "DataManipulationVisualisation/1_introduction.html#effective-and-reproducible-workflow",
    "title": "1 Introduction",
    "section": "1.2. Effective and reproducible workflow",
    "text": "1.2. Effective and reproducible workflow\nThere are several rules to make your workflow effective and reproducible after some time or by other people.\n\n1.2.1 Projects\nIf you start your work by setting the working directory, the reproducibility by someone else is very limited (see more here). Good habit is to organize each data analysis into a project: a folder on your computer that holds all the files relevant to that particular piece of work.\nR Studio easily enables creating projects and switching between them (either through File-Open/New/Recent Project or by clicking on the upper right corner icon for projects). \nTip: get used to creating the same subfolders in each of your project: data, scripts, results, maps, backup etc. to further organise the project structure. If you save the data into them, you can directly import them, see them through the project in R studio.\n\n\n1.2.2 Scripts\n\nlibraries - list all the libraries your code need at the beginning of the script\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\n\nremarks\n\n- add notes to your scripts, you will be grateful later\n- change the text to non-active (marked with hash tags #) ctrl + shift + C\n\nseparate scripts or sections\n\n- insert named section using ctrl + shift + R\n- or divide the code by ####\n- fold all sections with Alt + O\n- unfold all with Shift + Alt + O\n\nnames of variables\n\n- should be short and easy to handle, without spaces, strange symbols Simple variable names will save you really so much time and parentheses !\nSee this example\n\ndata &lt;- read_csv(\"data/messy_data/Example0.csv\")\nnames(data)\n\n[1] \"Relevé number\"                   \"Taxon\"                          \n[3] \"Vegetation layer\"                \"Cover %\"                        \n[5] \"Braun-Blanquet Scale (New) Code\"\n\n\nYou can rename strange names one by one. First use the new name and put the old one on the right like here.\n\ndata %&gt;% rename (Releve = \"Relevé number\")\n\n# A tibble: 51 × 5\n   Releve Taxon              `Vegetation layer` `Cover %` Braun-Blanquet Scale…¹\n    &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                 \n 1 435469 Ribes uva-crispa                    0         2 +                     \n 2 435469 Ulmus minor                         0         3 1                     \n 3 435469 Crataegus monogyn…                  0         2 +                     \n 4 435469 Rubus caesius                       0        13 a                     \n 5 435469 Prunus padus ssp.…                  0         3 1                     \n 6 435469 Populus alba                        0        13 a                     \n 7 435469 Fraxinus excelsior                  0        68 4                     \n 8 435469 Arctium lappa                       6         1 r                     \n 9 435469 Adoxa moschatelli…                  6         2 +                     \n10 435469 Aegopodium podagr…                  6        13 a                     \n# ℹ 41 more rows\n# ℹ abbreviated name: ¹​`Braun-Blanquet Scale (New) Code`\n\n\nOr you can change all difficult patterns at once, using RegEx Regular expressions. Import the example once again.\nYou identify the pattern on left, starting with two backslashes and define the outcome on right side. Be sure to keep the logic in your sequence - what is first and what last, as it really changes the patterns one by one, as they are listed.\n\ndata &lt;- read_csv(\"data/messy_data/Example0.csv\")\nnames(data)\n\n[1] \"Relevé number\"                   \"Taxon\"                          \n[3] \"Vegetation layer\"                \"Cover %\"                        \n[5] \"Braun-Blanquet Scale (New) Code\"\n\ndata %&gt;%\n  rename_all(~ str_replace_all(., c(\n    \"\\\\.\" = \"\",     # remove dots in the variable names\n    \"\\\\é\" = \"e\",    # replace é by e\n    \"\\\\%\" = \"perc\", # remove symbol % and change it to perc\n    \"\\\\(\" = \"\",     # remove brackets\n    \"\\\\)\" = \"\",     # remove brackets\n    \"\\\\/\" = \"\",     # remove slash\n    \"\\\\?\" = \"\",     # remove questionmark\n    \"\\\\s\" = \".\")))   # remove spaces\n\n# A tibble: 51 × 5\n   Releve.number Taxon        Vegetation.layer Cover.perc Braun-Blanquet.Scale…¹\n           &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                 \n 1        435469 Ribes uva-c…                0          2 +                     \n 2        435469 Ulmus minor                 0          3 1                     \n 3        435469 Crataegus m…                0          2 +                     \n 4        435469 Rubus caesi…                0         13 a                     \n 5        435469 Prunus padu…                0          3 1                     \n 6        435469 Populus alba                0         13 a                     \n 7        435469 Fraxinus ex…                0         68 4                     \n 8        435469 Arctium lap…                6          1 r                     \n 9        435469 Adoxa mosch…                6          2 +                     \n10        435469 Aegopodium …                6         13 a                     \n# ℹ 41 more rows\n# ℹ abbreviated name: ¹​`Braun-Blanquet.Scale.New.Code`\n\n\nNow check the names again. Did it work?\n\nTip: use the function clean_names from package janitor\n\nNote that in the examples above you are not really rewriting the data you have imported. Here the “data %&gt;%” means, that you are just trying how it would look like if you apply the following steps to data. To really change it you would have to assign your result into a new dataset, by using following options:\n\ndata2&lt;- data1 %&gt;% … #defining first the new dataset\ndata&lt;-data %&gt;% … #rewriting the existing dataset\ndata %&lt;&gt;% data … #rewriting the existing dataset by assignment pipe (magrittr)\ndata1 %&gt;% …-&gt;data2 #making/testing all the steps and assigning it to new dataset as the last step\n\nThis might sound troublesome, but it is actually very helpful. You can try all the steps and when you are happy with the code, you can put everything into one pipe line from import of the data to export of the result.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "1 Introduction"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/1_introduction.html#data-import",
    "href": "DataManipulationVisualisation/1_introduction.html#data-import",
    "title": "1 Introduction",
    "section": "1.3 Data import",
    "text": "1.3 Data import\nWe will train how to import data with Tidyverse, which means readr or readxl in case of Excel files (see cheatsheet)\nWhat is useful is to check which files are stored in the folders we have. Here we list all files in the working=project directory\n\nlist.files() \n\n [1] \"1_introduction.qmd\"                  \"1_introduction.rmarkdown\"           \n [3] \"11_database_to_plot.html\"            \"11_database_to_plot.qmd\"            \n [5] \"11_database_to_plot_files\"           \"12_github.html\"                     \n [7] \"12_github.qmd\"                       \"2_data_manipulation.qmd\"            \n [9] \"3_data_visualisation.qmd\"            \"4_wide_vs_long.qmd\"                 \n[11] \"5_join_functions.qmd\"                \"6_advanced_visualisation.qmd\"       \n[13] \"7_8_automatisation.qmd\"              \"9_10_maps.qmd\"                      \n[15] \"data\"                                \"data_manipulation_visualisation.qmd\"\n[17] \"exercises\"                           \"images\"                             \n[19] \"plots\"                               \"results\"                            \n[21] \"scripts\"                            \n\n\nOr we can dive deeper in the hierarchy and check content of data folder, or specific subfolder\n\nlist.files(\"data\") \n\n [1] \"acidophilous_grasslands\" \"basiphilous_grasslands\" \n [3] \"forest_understory\"       \"frozen_fauna\"           \n [5] \"gapminder\"               \"gapminder_clean\"        \n [7] \"gapminder_continent\"     \"lepidoptera\"            \n [9] \"messy_data\"              \"sands\"                  \n[11] \"spruce_forest_wide_long\" \"turboveg_to_R\"          \n\nlist.files(\"data/forest_understory\")\n\n[1] \"Axmanova-Forest-env.xlsx\"                             \n[2] \"Axmanova-Forest-spe.xlsx\"                             \n[3] \"Axmanova-Forest-understory-diversity-analyses.xlsx\"   \n[4] \"Axmanova-Forest-understory-diversity-merged-long.xlsx\"\n[5] \"readme.txt\"                                           \n[6] \"traits.xlsx\"                                          \n\n\nLet’s select one of the files and import it into our working environment. Depending on the type of file I have to select the right approach. Here it is an Excel file, so I can use function read_excel. Check the cheatsheet for more tips.\n\ndata &lt;- read_excel(\"data/forest_understory/Axmanova-Forest-understory-diversity-analyses.xlsx\")\n\nWe imported the data and here are a few tips how to check the structure\nFirst is the tibble, which actually appears automatically after any import using tidyverse approach - note that only ten rows and several variables are shown, if there are too many, rest is listed below.\n\ntibble(data)\n\n# A tibble: 65 × 22\n   PlotID ForestType ForestTypeName      Herbs Juveniles CoverE1 Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      1          2 oak hornbeam forest    26        12      20    12.8\n 2      2          1 oak forest             13         3      25     9.9\n 3      3          1 oak forest             14         1      25    15.2\n 4      4          1 oak forest             15         5      30    16  \n 5      5          1 oak forest             13         1      35    20.7\n 6      6          1 oak forest             16         3      60    46.4\n 7      7          1 oak forest             17         5      70    49.2\n 8      8          2 oak hornbeam forest    21         1      70    48.7\n 9      9          2 oak hornbeam forest    15         4      15    13.8\n10     10          1 oak forest             14         4      75    79.1\n# ℹ 55 more rows\n# ℹ 15 more variables: Soil_depth_categ &lt;dbl&gt;, pH_KCl &lt;dbl&gt;, Slope &lt;dbl&gt;,\n#   Altitude &lt;dbl&gt;, Canopy_E3 &lt;dbl&gt;, Radiation &lt;dbl&gt;, Heat &lt;dbl&gt;,\n#   TransDir &lt;dbl&gt;, TransDif &lt;dbl&gt;, TransTot &lt;dbl&gt;, EIV_light &lt;dbl&gt;,\n#   EIV_moisture &lt;dbl&gt;, EIV_soilreaction &lt;dbl&gt;, EIV_nutrients &lt;dbl&gt;, TWI &lt;dbl&gt;\n\n\nalternative way is to use glimpse, where the variables are listed below each other, showing the first 15 values\n\nglimpse(data)\n\nRows: 65\nColumns: 22\n$ PlotID           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 16, 18, 28, 29…\n$ ForestType       &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2,…\n$ ForestTypeName   &lt;chr&gt; \"oak hornbeam forest\", \"oak forest\", \"oak forest\", \"o…\n$ Herbs            &lt;dbl&gt; 26, 13, 14, 15, 13, 16, 17, 21, 15, 14, 12, 30, 24, 1…\n$ Juveniles        &lt;dbl&gt; 12, 3, 1, 5, 1, 3, 5, 1, 4, 4, 4, 3, 7, 3, 1, 2, 2, 7…\n$ CoverE1          &lt;dbl&gt; 20, 25, 25, 30, 35, 60, 70, 70, 15, 75, 8, 30, 60, 85…\n$ Biomass          &lt;dbl&gt; 12.8, 9.9, 15.2, 16.0, 20.7, 46.4, 49.2, 48.7, 13.8, …\n$ Soil_depth_categ &lt;dbl&gt; 5.0, 4.5, 3.0, 3.0, 3.0, 6.0, 7.0, 5.0, 3.5, 5.0, 2.0…\n$ pH_KCl           &lt;dbl&gt; 5.28, 3.24, 4.01, 3.77, 3.50, 3.80, 3.48, 3.68, 4.24,…\n$ Slope            &lt;dbl&gt; 4, 24, 13, 21, 0, 10, 6, 0, 38, 13, 29, 47, 33, 24, 0…\n$ Altitude         &lt;dbl&gt; 412, 458, 414, 379, 374, 380, 373, 390, 255, 340, 368…\n$ Canopy_E3        &lt;dbl&gt; 80, 80, 80, 75, 70, 65, 65, 85, 80, 70, 85, 60, 75, 7…\n$ Radiation        &lt;dbl&gt; 0.8813, 0.9329, 0.9161, 0.9305, 0.8691, 0.9178, 0.829…\n$ Heat             &lt;dbl&gt; 0.8575, 0.8138, 0.8503, 0.9477, 0.8691, 0.8834, 0.803…\n$ TransDir         &lt;dbl&gt; 3.72, 4.05, 4.38, 3.48, 3.73, 3.59, 4.49, 3.97, 3.61,…\n$ TransDif         &lt;dbl&gt; 2.83, 2.83, 2.94, 2.96, 3.15, 3.40, 2.87, 2.99, 2.92,…\n$ TransTot         &lt;dbl&gt; 6.55, 6.88, 7.31, 6.44, 6.88, 6.99, 7.36, 6.96, 6.53,…\n$ EIV_light        &lt;dbl&gt; 5.00, 4.71, 4.36, 5.26, 6.14, 6.19, 6.19, 5.29, 5.47,…\n$ EIV_moisture     &lt;dbl&gt; 4.38, 4.64, 4.70, 4.38, 4.00, 4.35, 4.25, 4.60, 4.36,…\n$ EIV_soilreaction &lt;dbl&gt; 6.68, 4.67, 4.80, 5.53, 5.33, 6.75, 6.09, 5.07, 5.46,…\n$ EIV_nutrients    &lt;dbl&gt; 4.31, 3.69, 3.55, 3.56, 3.46, 5.06, 4.33, 4.12, 3.50,…\n$ TWI              &lt;dbl&gt; 3.353962, 2.419177, 2.159580, 1.651170, 4.741780, 2.4…\n\n\nif you forget the names of variables or you want to copy them and store in your script, use simple names\n\nnames(data)\n\n [1] \"PlotID\"           \"ForestType\"       \"ForestTypeName\"   \"Herbs\"           \n [5] \"Juveniles\"        \"CoverE1\"          \"Biomass\"          \"Soil_depth_categ\"\n [9] \"pH_KCl\"           \"Slope\"            \"Altitude\"         \"Canopy_E3\"       \n[13] \"Radiation\"        \"Heat\"             \"TransDir\"         \"TransDif\"        \n[17] \"TransTot\"         \"EIV_light\"        \"EIV_moisture\"     \"EIV_soilreaction\"\n[21] \"EIV_nutrients\"    \"TWI\"             \n\n\nQuite useful baseR approach function is table, which shows the counts per categories of selected variable. In tidyverse it will need more steps to be done, but we will get there next time…\n\ntable(data$ForestTypeName)\n\n\n    alluvial forest          oak forest oak hornbeam forest       ravine forest \n                 11                  16                  28                  10 \n\n\nOf course, you can also view() the data or click on it in the list in the Files to open it in a table-like scrolable format, which can be useful many times. However, this is not recommended with huge tables, as the previews are rather memory demanding.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "1 Introduction"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/1_introduction.html#where-to-find-help",
    "href": "DataManipulationVisualisation/1_introduction.html#where-to-find-help",
    "title": "1 Introduction",
    "section": "1.4 Where to find help",
    "text": "1.4 Where to find help\nMotto: Majority of the problems in R can be solved if you know how to ask and where.\nKnowing that I am lost in “Regular expressions” already helps to ask more specifically.\n\nR studio help Try intergrated help in R studio, where you can find links to selected manuals, cheat sheets. It is the place where you can also find Keyboard shortcuts help to find out which combinations do what. e.g. shift+ctrl+m\nCheatsheets - the most important features of a given package summarised at two A4 pages, ready to print\nVignettes are supporting documents that are available for many packages. They give examples and explain functions available in the package. You can discover vignettes by accessing the help page for a package, or via the browseVignettes() function, which will get you to the overall list. Or you can just try by typing specific names e.g. vignette(“dplyr”)\n[R studio community] (https://forum.posit.co/) Questions sorted by packages, nice to check when looking for frequently asked questions.\nStack Overflow. You probably already came across Stack Overflow if you were trying to Google something, as it suggests answers to coding related questions. It is a good environment to ask questions or try to find if somebody already asked the same things before. Be specific about the coding style. E.g. How to separate variable using tidyverse?\nGitHub is a great source - you can find there data, projects, packages, instructions. If you stay till the end of this course we will show you more.\nAI tools are worth to ask. For example (i), you have a code and you do not understand it, so you can ask for explanation, or (ii) you want to get the code translated to another syntax (e.g. from base R to tidyverse), (iii) or you want to know how to code something but you do not have a clue which packages to use… We will train this a bit.\n\nOr just come and ask!",
    "crumbs": [
      "Data Manipulation and Visualization",
      "1 Introduction"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/1_introduction.html#exercises",
    "href": "DataManipulationVisualisation/1_introduction.html#exercises",
    "title": "1 Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nFor exercises we will use either data that are published somewhere and give you the link, or we will ask you to download the data from the repository and save it into your project folder Link to Github folder\nThere will be some obligatory tasks, while voluntarily tasks will be marked by *.\nIn this chapter:\n1. Create project for this lesson, add subfolders (data, scripts..), download the data from the folder Forest_understory, and store them in the folder “data” within your project. Import file called Axmanova-Forest-understory-diversity-analyses.xlsx into the working environment and check the structure. Prepare script with your notes, separated into sections.\n2. Create another project and start a new script, try switching between the projects. Delete this project.\n3. Find a cheatsheet for readxl. Is there anything more then data import from Excel?\n4. Use cheatsheet to find out how to import second sheet of Excel file. Try this on import of data/frozen_fauna/metadata. Which sheets are there and can you easily check the structure?\n5. Download Example0 from data/messy_data, save them into your data folder and import them to the working environment. Make the dataset tidy by renaming the variable names. Try one by one, or rename_all, or clean_names function from the janitor package. *Save the tidy dataset as Example0_tidy.csv using function write_csv.\n6. Try importing the same, not cleaned file via Rstudio and describe pros/cons.\n7. *Check the folder messy data, what are the problems in examples1-5? We will learn how to fix them in next chapters directly in R, but can you at least imagine how you would do it in Excel?\n8. *Do you know what is reprex and how to prepare it?",
    "crumbs": [
      "Data Manipulation and Visualization",
      "1 Introduction"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/1_introduction.html#further-reading",
    "href": "DataManipulationVisualisation/1_introduction.html#further-reading",
    "title": "1 Introduction",
    "section": "1.6 Further reading",
    "text": "1.6 Further reading\nRStudio: Download and basic information https://posit.co/download/rstudio-desktop/\nDavid Zelený: Tutorial how to install R and Rstudio https://www.davidzeleny.net/anadat-r/doku.php/en:r\nDatacamp: Tutorial for R studio here\nWhy to organise your work in projects https://www.tidyverse.org/blog/2017/12/workflow-vs-script/\nDavid Zelený clean and tidy script https://davidzeleny.net/wiki/doku.php/recol:clean_and_tidy_script\nTidyverse suggestions to good coding style https://style.tidyverse.org/\nR for data science book https://r4ds.hadley.nz/",
    "crumbs": [
      "Data Manipulation and Visualization",
      "1 Introduction"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/3_data_visualisation.html",
    "href": "DataManipulationVisualisation/3_data_visualisation.html",
    "title": "3 Data Visualisation",
    "section": "",
    "text": "In this chapter, we will cover the basics of data visualisation using the ggplot2 package in R. ggplot2 uses a conceptual framework called Grammar of Graphics , which allows you to compose graphs by combining independent components. Every graph produced with ggplot2 can be built from the same components: data, a coordinate system and geoms (visual marks representing data points). That is what makes ggplot2 so powerful. We will now explore how to create various types of plots, customize them, and save them for publications or presentations.\nSo let’s start. ggplot2 is a part of tidyverse, so you should have it installed already, and you can load it by running:\nlibrary(tidyverse)\nThis is a line that should appear at the beginning of each of your new scripts.\nTo make a plot, we first need some data. We will use a dataset of size measurements for three penguin species observed on three islands in the Palmer Achipelago (Antarctica).\nTo get the data, we need to load another package (if you get an error message that there is no such package, install it first):\nlibrary(palmerpenguins)\nBy loading the package, we got access to a penguin dataset, that we might now call as an object called penguins. Before plotting, we can briefly look at the data structure:\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\nThe dataset contains information about 344 penguins, and there are 8 different variables. You can read more about them in the documentation by running ?penguins.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "3 Data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/3_data_visualisation.html#getting-started-scatterplot",
    "href": "DataManipulationVisualisation/3_data_visualisation.html#getting-started-scatterplot",
    "title": "3 Data Visualisation",
    "section": "3.1 Getting started: scatterplot",
    "text": "3.1 Getting started: scatterplot\nWe will now visualise the relationship between the different dimensions of the penguin bill:\n\n\n\n\n\nVisualisation with ggplot2 starts with the function ggplot(). This will define a plot object, where you can then add layers. The first argument to this function is data, which specifies which data to use for plotting. Running the following will create an empty plot, an empty canvas prepared for plotting desired layers onto.\n\nggplot(data = penguins)\n\n\n\n\n\n\n\n\nNext, we have to tell ggplot() how the data should be visualised. For this, we specify the mapping argument, which is always defined by the aes() function (aesthetics). The x and y arguments of the aes() function specify, which variables to map to the x and y axes of the plot.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm))\n\n\n\n\n\n\n\n\nThe ggplot() now knows which variables will be displayed on each axes and added the axis names and value ranges to our empty plot. However, we still didn’t provide any information about how the data point should be displayed. To do so, we define a geom, geometrical object to represent the data. ggplot2 provides a wide variety of possible geometries, all defined by functions starting with geom_. Boxplots are drawn using boxplot geoms (geom_boxplot()), bar charts use bar geoms (geom_bar()), line charts use line geoms (geom_line()), scatterplots use point geoms (geom_point()), etc. We now aim to create a scatterplot, which means adding points to our plot using the geom_point() function:\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNow, the plot with a point for each row in the data appears. However, there is a warning message in the console, which tells us that 2 rows contained missing values or values outside of the scale range. We didn’t set any limit for x or y axis, so it most likely means there are some missing values in the data. We will not focus on this point now, but the ideal next step would be to check the data, where the missing values appeared and why. If possible, correct missing data in the original data, load new data and continue.\nNote that we use + for adding layers to ggplot() instead of %&gt;% or |&gt;. This is because + in ggplot() existed earlier than the pipe was discovered. It might be confusing and a cause errors, but don’t worry, R will tell you what’s wrong if it happens:\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm)) |&gt; \n  geom_point()\n\nError in `geom_point()`:\n! `mapping` must be created by `aes()`.\n✖ You've supplied a &lt;ggplot2::ggplot&gt; object.\nℹ Did you use `%&gt;%` or `|&gt;` instead of `+`?\n\n\nWhen we look at the scatterplot, there doesn’t appear to be a clear relationship between the penguin bill length and bill depth. Let’s add one more layer to our plot to check the relationship. To add a line based on linear regression, we use geom_smooth function with method = 'lm':\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() + \n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\nNow it seems there is a slightly negative relationship, which is a bit contrary to the expectations. Moreover, if we focus on the positions of the points in the scatterplot are a bit clustered. Let’s recall the structure of our data. We have measurements of three different species in the dataset. What if this is the cause? Let’s incorporate the species identity into our visualisation. We can do this by adding colour to the points. For this we need to modify the aesthetics:\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm, colour = species)) +\n  geom_point() + \n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\nNow it is clear that species identity is really important and the relationship between the bill dimensions is in fact positive. This is a classical example of the Simpson’s paradox (a relationship appears in several groups of data, but disappears or reverses when the groups are combined). Look here for an explanation or here for a song :-).\nIn addition to the colour, we can also distinguish species in the plot by adding shape aesthetics.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm, colour = species, shape = species)) +\n  geom_point() + \n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\nNote that the legend is automatically updated to reflect both the different colours and shapes of the points.\nLet’s play a bit more with the different shapes and colours, the points could be better distinguished from each other if the points were black-delimited. This might be changed by the specification of scale for the shape aesthetics. Bounded shapes with coloured inside are 21-25 and we want to use the circle, square and triangle:\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm, colour = species, shape = species)) +\n  geom_point() + \n  geom_smooth(method = 'lm') + \n  scale_shape_manual(values = c(21, 22, 24))\n\n\n\n\n\n\n\n\nOh, but this is not what we wanted, the shapes have only coloured boundaries, but no fill. Let’s make it correctly. The fill colour is specified by the fill aesthetics, so we have to add this one. Moreover, the colour aesthetics is now specified directly in the ggplot() function, which means it applies to all layers in the plot. But we want all points to have black borders, so the colour should apply only to the lines. We can do this by moving the colour aesthetics to the geom_smooth() layer.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm, fill = species, shape = species)) +\n  geom_point() + \n  geom_smooth(aes(colour = species), method = 'lm') + \n  scale_shape_manual(values = c(21, 22, 24))\n\n\n\n\n\n\n\n\nMuch nicer! Note that adding the fill aesthetics to the ggplot() also changed the colour of the confidence intervals for the regression lines.\nThe legend is now showing all aesthetics differentiating the groups in the plot. Maybe we don’t need to display all of them. It would be enough to display a legend for the points, because the lines have the same colours. We can switch off legend for a given layer using show.legend = F:\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm, fill = species, shape = species)) +\n  geom_point() + \n  geom_smooth(aes(colour = species), method = 'lm', show.legend = F) + \n  scale_shape_manual(values = c(21, 22, 24))\n\n\n\n\n\n\n\n\nTo make the plot more publication-ready, we should pay more attention to the axes labels. We can change them using the labs() function. To rename the legend, we have to specify labels for all aesthetics used to distinguish the different categories.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm, fill = species, shape = species)) +\n  geom_point() + \n  geom_smooth(aes(colour = species), method = 'lm', show.legend = F) + \n  scale_shape_manual(values = c(21, 22, 24)) + \n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)', fill = 'Species', shape = 'Species')\n\n\n\n\n\n\n\n\nAlmost publication-ready, except the grey background is not very nice. It might be easily remove by adding a different theme. One of our favourites is theme_bw(), meaning black and white, but you can experiment with different ones or even define your own.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = bill_depth_mm, fill = species, shape = species)) +\n  geom_point() + \n  geom_smooth(aes(colour = species), method = 'lm', show.legend = F) + \n  scale_shape_manual(values = c(21, 22, 24)) + \n  labs(x = 'Bill length (mm)', y = 'Bill depth (mm)', fill = 'Species', shape = 'Species') + \n  theme_bw()\n\n\n\n\n\n\n\n\nThis is our final plot showing the relationship between penguin bill dimensions of three different species.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "3 Data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/3_data_visualisation.html#histogram",
    "href": "DataManipulationVisualisation/3_data_visualisation.html#histogram",
    "title": "3 Data Visualisation",
    "section": "3.2 Histogram",
    "text": "3.2 Histogram\nLet’s explore the dataset more and take a look at different plot types. For example, we can look at the distribution of the penguin body mass using a histogram. We only specify x aesthetics here, because the histogram divides x-axis into equally spaced bins and then uses the height of the bar to display the number of observations that fall into each bin. The binwidth argument sets the width of the bins.\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\n\n\n\n\n\n\nFor now, we will not go into detail on how to improve the appearance of this plot, you can take it as an exercise and experiment with it on your own.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "3 Data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/3_data_visualisation.html#boxplot",
    "href": "DataManipulationVisualisation/3_data_visualisation.html#boxplot",
    "title": "3 Data Visualisation",
    "section": "3.3 Boxplot",
    "text": "3.3 Boxplot\nThere is surely also a difference between different penguin species. We can visualise the differences using a boxplot:\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe can also colour the boxes using the fill aesthetics. To make this plot nicer, we will again change the theme of the plot. We can also change the label of the y axis, while the label of the x axis is a bit redundant and it would be better to remove it. This might be done within the theme() function. That’s where we can customise the properties of different plot components, such as axis labels, legend or background grid lines. To remove the given component, we set the argument to element_blank().\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() + \n  labs(y = 'Body mass (g)', fill = 'Species') + \n  theme_bw() + \n  theme(axis.title.x = element_blank())",
    "crumbs": [
      "Data Manipulation and Visualization",
      "3 Data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/3_data_visualisation.html#bar-chart",
    "href": "DataManipulationVisualisation/3_data_visualisation.html#bar-chart",
    "title": "3 Data Visualisation",
    "section": "3.4 Bar chart",
    "text": "3.4 Bar chart\nTo examine the distribution of a categorical variable, we can use a bar chart. The penguin measurements come from three different islands, let’s see how the distribution of measurement across the islands looks like:\n\nggplot(penguins, aes(x = island)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nAnd were all three penguin species measured at each of the three islands? We can see that when we map species using the fill aesthetics. The colour would control the border colour of the bars. We can also set that all bars should have black borders by setting colour = 'black' in geom_bar. We can also directly modify the theme and plot labels.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(colour = 'black') + \n  labs(x = 'Island', y = 'Number of individuals', fill = 'Species') + \n  theme_bw()\n\n\n\n\n\n\n\n\nInstead of the number of individuals, we can also display the relative frequency of different species at the islands by setting position = 'fill'.\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(colour = 'black', position = 'fill') + \n  labs(x = 'Island', y = 'Relative frequency', fill = 'Species') + \n  theme_bw()",
    "crumbs": [
      "Data Manipulation and Visualization",
      "3 Data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/3_data_visualisation.html#saving-your-plots",
    "href": "DataManipulationVisualisation/3_data_visualisation.html#saving-your-plots",
    "title": "3 Data Visualisation",
    "section": "3.5 Saving your plots",
    "text": "3.5 Saving your plots\nOnce you are satisfied with your plot, it is a good idea to save it as an image. The best way to save a plot from ggplot() is to use the ggsave() function. You have to specify the name of your file. Good practice is to save all plots to a dedicated folder, for example plots. By default, the last plot displayed in the viewer pane will be saved, and the dimensions will be the same as the current extent of the viewer pane. It is not the best idea to rely on this because the size of the plot will change every time you change the extent of your viewer pane. Therefore, it is a good practice to set the size of your figure using the width and height arguments. Sometimes it requires a bit of experimentation, but since you figure out the optimal settings, you can redraw the plot as many times as you want and always save it with the same dimensions.\n\nggsave('plots/island_relative_frequency.png', width = 6, height = 4)\n\nBy default, this function saves the last plot you draw in the Plots pane. Another option to save a specific plot is to save the plot in an object and then save this object within ggsave().\n\nplot1 &lt;- ggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(colour = 'black', position = 'fill') + \n  labs(x = 'Island', y = 'Relative frequency', fill = 'Species') + \n  theme_bw()\n\nggsave('plots/island_relative_frequency.png', plot1, width = 6, height = 4)",
    "crumbs": [
      "Data Manipulation and Visualization",
      "3 Data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/3_data_visualisation.html#exercises",
    "href": "DataManipulationVisualisation/3_data_visualisation.html#exercises",
    "title": "3 Data Visualisation",
    "section": "3.6 Exercises",
    "text": "3.6 Exercises\n\nUse the dataset Axmanova-Forest-understory-diversity-analyses.xslx and visualise the relationship between species richness of forest herb layer and the tree canopy cover. Is the relationship the same in different forest types?\nDraw a boxplot showing the differences in forest herb layer species richness in different forest types. Which forest type appears to be the most species-rich? * Would there be another way to visualise the relationship between the herb-layer species richness and forest type? Explore the ggplot extensions gallery and find a different plot type that might be useful here.\nLoad data of squirrel observations from the Central Park Squirrel Census using this line: read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-23/squirrel_data.csv') Squirrels of which colour were the most common? Draw a barplot to visualise the differences. * Try to find a way to colour the bars with colours resembling the squirrel colours.\nLoad the Pokemon dataset using this line: read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-01/pokemon_df.csv'). Visualise the distribution of Pokémon weights. * Try to use at least two different geom functions.\n* How would you transform the variable before an analysis? Find a way to visualise the distribution on a transformed scale using a ggplot function.\nWhat is the relationship between the attack and special attack power of water-type Pokémons?\nWhich Pokémon type has the highest attack power? Visualise the relationship.\nExplore the relationship between the Pokémon attack and defense power. Distinguish different Pokémon types. * When using different colors for many categories, the plot gets quite messy, try to come up with a solution to distinguish the Pokémon types more clearly in the plot.\nModify axis labels, legend, colours, etc. of all plots you created so far, so that you like their appearance and save them to the folder plots.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "3 Data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/3_data_visualisation.html#further-reading",
    "href": "DataManipulationVisualisation/3_data_visualisation.html#further-reading",
    "title": "3 Data Visualisation",
    "section": "3.7 Further reading",
    "text": "3.7 Further reading\nggplot2 vignette: https://ggplot2.tidyverse.org\nR 4 Data Science: https://r4ds.hadley.nz/data-visualize.html\nCheatsheet: https://rstudio.github.io/cheatsheets/html/data-visualization.html\nggplot2: Elegant Graphics for Data Analysis (3e): https://ggplot2-book.org\nR Graphics Cookbook: https://r-graphics.org\nggplot Extensions: https://exts.ggplot2.tidyverse.org",
    "crumbs": [
      "Data Manipulation and Visualization",
      "3 Data visualisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/5_join_functions.html",
    "href": "DataManipulationVisualisation/5_join_functions.html",
    "title": "5 Join Functions",
    "section": "",
    "text": "In this chapter we will train how to add the information from another dataset (left_join, full_join), how to filter our data according to another dataset (semi_join, anti_join). We will also use our skills from previous chapter to calculate different statistics about our data, like mean indicator values, community weighted means of some traits, proportion of specific plant groups (left_join, group_by, summarise). We will also look at mutate with more conditions (ifelse, case_when).\nCredits: https://r4ds.hadley.nz/joins.html",
    "crumbs": [
      "Data Manipulation and Visualization",
      "5 Join functions"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/5_join_functions.html#matching-selected-information-with-left_join",
    "href": "DataManipulationVisualisation/5_join_functions.html#matching-selected-information-with-left_join",
    "title": "5 Join Functions",
    "section": "5.1 Matching selected information with left_join",
    "text": "5.1 Matching selected information with left_join\nIn most cases, the data you need for analyses are not organised in one file, but rather in more of them. For example we have measured characteristics for each species in the region, so called traits, and we have a species lists for all the sites. To be able to calculate mean values of selected traits for each site, we have to first append the information to individual records. For this we will use the function called left_join which belongs to the group of mutating joins available in tidyverse package dplyr.\nThis function will automatically look for columns with exactly matching names, or we can specify the corresponding variables directly using the by argument. It will append all the columns available in the second “lifeforms” dataset, but only for rows, where there is a exact match. Although we have information also for other species such as Abies alba or Carex sylvatica, these do not have matching rows in the first dataset “data” and the information therefore will not be used. This means left_join is just picking those rows, that are in the first dataset. If the information is missing in the matching file, e.g. there is no information for Trientalis europaea in the lifeforms dataset, in the resulting file there will be NA.\nLet’s train a bit. Start again a new script for this chapter.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\n\nWe will import the data from Forest understory again.\n\nspe&lt;- read_xlsx(\"data/forest_understory/Axmanova-Forest-spe.xlsx\")\n\nand we will import also data with some traits such as plant height, Ellenberg indicator values i.e. values indicating demands on environmental factors as soil reaction, light, nutrients… Lower values indicate lower demands, i.e. species can tolerate low=acidic pH, or low nutrient levels, high values indicate that the species requires or grows in habitats with high=basic soil reaction, high nutrient availability etc.\n\ntraits&lt;- read_xlsx(\"data/forest_understory/traits.xlsx\")\n\nIf we try directly merging these two files it is not working. We will get a warning explaining that there are no common variables.\n\nspe%&gt;% left_join(traits)\n\nError in `left_join()`:\n! `by` must be supplied when `x` and `y` have no common variables.\nℹ Use `cross_join()` to perform a cross-join.\n\n\nSo there are more options what we can do. We check the data and see that the information we need to match is once saved as Species (in the data) and once as a Taxon (in traits).\n\ntibble(spe)\n\n# A tibble: 2,277 × 4\n   RELEVE_NR Species              Layer CoverPerc\n       &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n 1         1 Quercus petraea agg.     1        63\n 2         1 Acer campestre           1        18\n 3         1 Crataegus laevigata      4         2\n 4         1 Cornus mas               4         8\n 5         1 Lonicera xylosteum       4         3\n 6         1 Galium sylvaticum        6         3\n 7         1 Carex digitata           6         3\n 8         1 Melica nutans            6         2\n 9         1 Polygonatum odoratum     6         2\n10         1 Geum urbanum             6         2\n# ℹ 2,267 more rows\n\n\n\ntibble(traits)\n\n# A tibble: 316 × 7\n   Taxon       Height SeedMass EIV_Light EIV_Moisture EIV_Reaction EIV_Nutrients\n   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 Quercus pe…  25      975.           6            5            5             4\n 2 Acer campe…   7.5     54.3          5            5            7             6\n 3 Crataegus …   5.5    276.           5            5            7             5\n 4 Cornus mas    4      488.           6            4            8             5\n 5 Lonicera x…   2      164.           4            5            7             6\n 6 Galium syl…   0.85     1.10         5            5            6             5\n 7 Carex digi…   0.2      1.44         4            5            6             6\n 8 Melica nut…   0.45     2.49         4            5            6             4\n 9 Polygonatu…   0.45   264.           6            3            7             3\n10 Geum urban…   0.55     2.05         5            5            6             7\n# ℹ 306 more rows\n\n\nWe can decide to rename the Species to Taxon and use this column for matching\n\nspe%&gt;% rename(Taxon=Species) %&gt;% \n  left_join(traits, by=\"Taxon\")\n\n# A tibble: 2,277 × 10\n   RELEVE_NR Taxon        Layer CoverPerc Height SeedMass EIV_Light EIV_Moisture\n       &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1         1 Quercus pet…     1        63  25      975.           6            5\n 2         1 Acer campes…     1        18   7.5     54.3          5            5\n 3         1 Crataegus l…     4         2   5.5    276.           5            5\n 4         1 Cornus mas       4         8   4      488.           6            4\n 5         1 Lonicera xy…     4         3   2      164.           4            5\n 6         1 Galium sylv…     6         3   0.85     1.10         5            5\n 7         1 Carex digit…     6         3   0.2      1.44         4            5\n 8         1 Melica nuta…     6         2   0.45     2.49         4            5\n 9         1 Polygonatum…     6         2   0.45   264.           6            3\n10         1 Geum urbanum     6         2   0.55     2.05         5            5\n# ℹ 2,267 more rows\n# ℹ 2 more variables: EIV_Reaction &lt;dbl&gt;, EIV_Nutrients &lt;dbl&gt;\n\n\nor we can even skip the by argument, as there is only one commong matching column.\n\nspe%&gt;% rename(Taxon=Species) %&gt;% \n  left_join(traits)\n\nJoining with `by = join_by(Taxon)`\n\n\n# A tibble: 2,277 × 10\n   RELEVE_NR Taxon        Layer CoverPerc Height SeedMass EIV_Light EIV_Moisture\n       &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1         1 Quercus pet…     1        63  25      975.           6            5\n 2         1 Acer campes…     1        18   7.5     54.3          5            5\n 3         1 Crataegus l…     4         2   5.5    276.           5            5\n 4         1 Cornus mas       4         8   4      488.           6            4\n 5         1 Lonicera xy…     4         3   2      164.           4            5\n 6         1 Galium sylv…     6         3   0.85     1.10         5            5\n 7         1 Carex digit…     6         3   0.2      1.44         4            5\n 8         1 Melica nuta…     6         2   0.45     2.49         4            5\n 9         1 Polygonatum…     6         2   0.45   264.           6            3\n10         1 Geum urbanum     6         2   0.55     2.05         5            5\n# ℹ 2,267 more rows\n# ℹ 2 more variables: EIV_Reaction &lt;dbl&gt;, EIV_Nutrients &lt;dbl&gt;\n\n\nWe can also skip rename step and directly say in the left_join function what is matching with what. Note that it will keep Species as outcome, because this was how we named it in the primary data file\n\nspe%&gt;% left_join(traits, by=c(\"Species\" =\"Taxon\"))\n\n# A tibble: 2,277 × 10\n   RELEVE_NR Species      Layer CoverPerc Height SeedMass EIV_Light EIV_Moisture\n       &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1         1 Quercus pet…     1        63  25      975.           6            5\n 2         1 Acer campes…     1        18   7.5     54.3          5            5\n 3         1 Crataegus l…     4         2   5.5    276.           5            5\n 4         1 Cornus mas       4         8   4      488.           6            4\n 5         1 Lonicera xy…     4         3   2      164.           4            5\n 6         1 Galium sylv…     6         3   0.85     1.10         5            5\n 7         1 Carex digit…     6         3   0.2      1.44         4            5\n 8         1 Melica nuta…     6         2   0.45     2.49         4            5\n 9         1 Polygonatum…     6         2   0.45   264.           6            3\n10         1 Geum urbanum     6         2   0.55     2.05         5            5\n# ℹ 2,267 more rows\n# ℹ 2 more variables: EIV_Reaction &lt;dbl&gt;, EIV_Nutrients &lt;dbl&gt;\n\n\nNote that left_join is a great help. But you should always think about the data. If you have more, potentially matching rows you can do a lot of unwanted mess with left_join. For example if you do not use unique ID, but descriptor such as forest type. So please do read the warning messages. :-)",
    "crumbs": [
      "Data Manipulation and Visualization",
      "5 Join functions"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/5_join_functions.html#full_join-vs.-bind_rows",
    "href": "DataManipulationVisualisation/5_join_functions.html#full_join-vs.-bind_rows",
    "title": "5 Join Functions",
    "section": "5.2 Full_join vs. bind_rows",
    "text": "5.2 Full_join vs. bind_rows\nSometimes it is needed to match both datasets. In contrast to left_join, full_join will add also additional rows that appear in the other dataset.\n\nCredits: https://r4ds.hadley.nz/joins.html\nWe will upload the forest env data and make two subsets to train a bit.\n\nenv&lt;- read_xlsx(\"data/forest_understory/Axmanova-Forest-env.xlsx\")\n\nIn the first subset we will use only first two forest types, namely oak and oak-hoarnbeam forests and selection of variables.\n\nforest1&lt;-env %&gt;% filter(ForestType %in% c(1,2))%&gt;% \n  select(PlotID=RELEVE_NR, ForestType, ForestTypeName, pH_KCl, Biomass)\n\nIn the second subset we will use one forest type shared with the previous subset and two others, namely oak-hornbeam, ravine and alluvial forests, and a bit different selection of variables.\n\nforest2&lt;-env %&gt;% \n  filter(ForestType %in% c(2,3,4))%&gt;% \n  select(PlotID=RELEVE_NR, ForestType, ForestTypeName, pH_KCl,Canopy_E3)\n\nNow left_join the data and check the preview #use view () to see more of the dataset. Here we have just added the extra information available in the second dataset to our selection of plots and variables in the first dataset.\n\nforest1 %&gt;% \n  left_join(forest2)\n\nJoining with `by = join_by(PlotID, ForestType, ForestTypeName, pH_KCl)`\n\n\n# A tibble: 44 × 6\n   PlotID ForestType ForestTypeName      pH_KCl Biomass Canopy_E3\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1      1          2 oak hornbeam forest   5.28    12.8        80\n 2      2          1 oak forest            3.24     9.9        NA\n 3      3          1 oak forest            4.01    15.2        NA\n 4      4          1 oak forest            3.76    16          NA\n 5      5          1 oak forest            3.50    20.7        NA\n 6      6          1 oak forest            3.8     46.4        NA\n 7      7          1 oak forest            3.48    49.2        NA\n 8      8          2 oak hornbeam forest   3.68    48.7        85\n 9      9          2 oak hornbeam forest   4.24    13.8        80\n10     10          1 oak forest            4.00    79.1        NA\n# ℹ 34 more rows\n\n\nIn the next step we will take both subsets and merge them together using full_join. What is the output? We will get a dataset will whole information available, this includes all four forests types. However, since the Forest type 1 is only present in the first subset, the corresponding rows will have only the information available there (forest type, pH). The type 2 is in both datasets so it will have all information from both datasets (forest type, pH, biomass, canopy). Type 3 and 4 will have information only from the second dataset (forest type, pH, canopy). Use view () to see the complete merged dataset.\n\nforest1 %&gt;% \n  full_join(forest2)\n\nJoining with `by = join_by(PlotID, ForestType, ForestTypeName, pH_KCl)`\n\n\n# A tibble: 65 × 6\n   PlotID ForestType ForestTypeName      pH_KCl Biomass Canopy_E3\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1      1          2 oak hornbeam forest   5.28    12.8        80\n 2      2          1 oak forest            3.24     9.9        NA\n 3      3          1 oak forest            4.01    15.2        NA\n 4      4          1 oak forest            3.76    16          NA\n 5      5          1 oak forest            3.50    20.7        NA\n 6      6          1 oak forest            3.8     46.4        NA\n 7      7          1 oak forest            3.48    49.2        NA\n 8      8          2 oak hornbeam forest   3.68    48.7        85\n 9      9          2 oak hornbeam forest   4.24    13.8        80\n10     10          1 oak forest            4.00    79.1        NA\n# ℹ 55 more rows\n\n\nNote that the joining worked good, since we had the same names in both datasets. For example if biomass would be called soil_pH in one and pH_KCl in the other dataset, it will treat them as different variables as you can see below\n\nforest1%&gt;% \n  rename(soil_pH=pH_KCl) %&gt;% \n  full_join(forest2)\n\nJoining with `by = join_by(PlotID, ForestType, ForestTypeName)`\n\n\n# A tibble: 65 × 7\n   PlotID ForestType ForestTypeName      soil_pH Biomass pH_KCl Canopy_E3\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1      1          2 oak hornbeam forest    5.28    12.8   5.28        80\n 2      2          1 oak forest             3.24     9.9  NA           NA\n 3      3          1 oak forest             4.01    15.2  NA           NA\n 4      4          1 oak forest             3.76    16    NA           NA\n 5      5          1 oak forest             3.50    20.7  NA           NA\n 6      6          1 oak forest             3.8     46.4  NA           NA\n 7      7          1 oak forest             3.48    49.2  NA           NA\n 8      8          2 oak hornbeam forest    3.68    48.7   3.68        85\n 9      9          2 oak hornbeam forest    4.24    13.8   4.24        80\n10     10          1 oak forest             4.00    79.1  NA           NA\n# ℹ 55 more rows\n\n\nImagine situation when data were sampled in different years/months or by different colleagues, or in different regions separately. If these subsets have exactly the same structure, we may use bind_rows function to merge them together, one below each other.\n\nforest1&lt;-env %&gt;% \n  filter(ForestType ==1)%&gt;% \n  select(PlotID=RELEVE_NR, ForestType, ForestTypeName, pH_KCl, Biomass)\n\nforest2&lt;-env %&gt;% \n  filter(ForestType ==2)%&gt;% \n  select(PlotID=RELEVE_NR, ForestType, ForestTypeName, pH_KCl, Biomass)\n\nBind and check the whole dataset with view()\n\nforest1%&gt;% \n  bind_rows(forest2)\n\n# A tibble: 44 × 5\n   PlotID ForestType ForestTypeName pH_KCl Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1      2          1 oak forest       3.24     9.9\n 2      3          1 oak forest       4.01    15.2\n 3      4          1 oak forest       3.76    16  \n 4      5          1 oak forest       3.50    20.7\n 5      6          1 oak forest       3.8     46.4\n 6      7          1 oak forest       3.48    49.2\n 7     10          1 oak forest       4.00    79.1\n 8     11          1 oak forest       3.83     7.4\n 9     28          1 oak forest       3.56    29.7\n10     31          1 oak forest       3.20    14.9\n# ℹ 34 more rows\n\n\nNote that the rbind function in baseR works similarly, but less effectively and it is designed for dataframes.\n\nrm(forest1,forest2) #remove unwanted data from the environment",
    "crumbs": [
      "Data Manipulation and Visualization",
      "5 Join functions"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/5_join_functions.html#filtering-joins",
    "href": "DataManipulationVisualisation/5_join_functions.html#filtering-joins",
    "title": "5 Join Functions",
    "section": "5.3 Filtering joins",
    "text": "5.3 Filtering joins\nSometimes we need to filter the data according to another dataset. For example I have looked at the environmental file and decided that for a follow-up analysis I need only alluvial forest. I therefore filtered rows in the env file.\n\nenv_selected &lt;-env %&gt;% \n  filter(ForestType==4)%&gt;%\n  select(PlotID=RELEVE_NR,ForestType, ForestTypeName, pH_KCl, Biomass) %&gt;% print()\n\n# A tibble: 11 × 5\n   PlotID ForestType ForestTypeName  pH_KCl Biomass\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1    101          4 alluvial forest   6.67    91.1\n 2    103          4 alluvial forest   7.12   114. \n 3    104          4 alluvial forest   7.14   188. \n 4    110          4 alluvial forest   5.82   126. \n 5    111          4 alluvial forest   7.01    84.8\n 6    113          4 alluvial forest   6.61    74.5\n 7    125          4 alluvial forest   4.99   123. \n 8    128          4 alluvial forest   7.03   121. \n 9    130          4 alluvial forest   5.74   238  \n10    132          4 alluvial forest   5.56   287. \n11    127          4 alluvial forest   5.56   176  \n\n\nHowever, I also need to filter the rows in the species data, where there is no information about forest types. The only common identificator is the PlotID (i.e. sample/site ID).\n\nspe %&gt;% rename(PlotID=RELEVE_NR) %&gt;% \n  semi_join(env_selected)\n\nJoining with `by = join_by(PlotID)`\n\n\n# A tibble: 530 × 4\n   PlotID Species             Layer CoverPerc\n    &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;\n 1    101 Fraxinus excelsior      1        38\n 2    103 Alnus glutinosa         1        63\n 3    104 Alnus glutinosa         1        63\n 4    110 Alnus glutinosa         1        38\n 5    111 Alnus glutinosa         1        38\n 6    113 Alnus glutinosa         1        63\n 7    125 Alnus glutinosa         1        38\n 8    128 Corylus avellana        1        38\n 9    130 Acer pseudoplatanus     1        38\n10    132 Alnus glutinosa         1        38\n# ℹ 520 more rows\n\n\nWith distinct function you can check if the IDs are the same.\n\nspe %&gt;% rename(PlotID=RELEVE_NR) %&gt;% \n  semi_join(env_selected) %&gt;% \n  distinct(PlotID)\n\nJoining with `by = join_by(PlotID)`\n\n\n# A tibble: 11 × 1\n   PlotID\n    &lt;dbl&gt;\n 1    101\n 2    103\n 3    104\n 4    110\n 5    111\n 6    113\n 7    125\n 8    128\n 9    130\n10    132\n11    127\n\n\nSometimes you may need the opposite - to keep only rows/IDs that are not in the other file. For this you should use anti_join function. An example can be that you want to remove from your list certain group of species such as annuals or nonvasculars (mosses and lichens).",
    "crumbs": [
      "Data Manipulation and Visualization",
      "5 Join functions"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/5_join_functions.html#advanced-mutate-and-.by",
    "href": "DataManipulationVisualisation/5_join_functions.html#advanced-mutate-and-.by",
    "title": "5 Join Functions",
    "section": "5.4 Advanced mutate and .by",
    "text": "5.4 Advanced mutate and .by\nBelow are few examples, how we used mutate so far\n\nenv %&gt;% \n  mutate(author= \"Axmanova\", #add one column with specified value\n        forestType=as.character(ForestType), #change the type of variable\n        biomass = Biomass*1000, # multiply to change to mg\n        forestTypeName= str_replace_all(ForestTypeName,\" \",\"_\"))%&gt;% #change string\n  select(PlotID=RELEVE_NR, author, forestType, biomass,forestTypeName)\n\n# A tibble: 65 × 5\n   PlotID author   forestType biomass forestTypeName     \n    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;              \n 1      1 Axmanova 2            12800 oak_hornbeam_forest\n 2      2 Axmanova 1             9900 oak_forest         \n 3      3 Axmanova 1            15200 oak_forest         \n 4      4 Axmanova 1            16000 oak_forest         \n 5      5 Axmanova 1            20700 oak_forest         \n 6      6 Axmanova 1            46400 oak_forest         \n 7      7 Axmanova 1            49200 oak_forest         \n 8      8 Axmanova 2            48700 oak_hornbeam_forest\n 9      9 Axmanova 2            13800 oak_hornbeam_forest\n10     10 Axmanova 1            79100 oak_forest         \n# ℹ 55 more rows\n\n\nWe also used mutate to adjust more variables at once\n\nenv %&gt;% mutate(across(c(Radiation, Heat), ~ round(.x, digits = 2))) %&gt;%\n  select (PlotID=RELEVE_NR, Radiation, Heat)\n\n# A tibble: 65 × 3\n   PlotID Radiation  Heat\n    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1      1      0.88  0.86\n 2      2      0.93  0.81\n 3      3      0.92  0.85\n 4      4      0.93  0.95\n 5      5      0.87  0.87\n 6      6      0.92  0.88\n 7      7      0.83  0.8 \n 8      8      0.87  0.87\n 9      9      0.61  0.42\n10     10      0.91  0.93\n# ℹ 55 more rows\n\n\n.x refers to the entire column (vector) being transformed inside a function like mutate(across(…)) or map().\nAlternatively if you have a large table with only measured variables you can specify what should not be used, here Species, and mutate will be applied to everything left. This is especially useful for changing formats e.g from character to numeric (function as.numeric)\n\niris %&gt;% \n  as_tibble() %&gt;%  #first change from dataframe to tibble \n  mutate(across(-Species, round))\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1            5           4            1           0 setosa \n 2            5           3            1           0 setosa \n 3            5           3            1           0 setosa \n 4            5           3            2           0 setosa \n 5            5           4            1           0 setosa \n 6            5           4            2           0 setosa \n 7            5           3            1           0 setosa \n 8            5           3            2           0 setosa \n 9            4           3            1           0 setosa \n10            5           3            2           0 setosa \n# ℹ 140 more rows\n\n\nYou can even create a list of functions which should be applied. relocate here was used to shift all the Sepal variables before the first variable / just for easier check.\n\niris%&gt;% \n  as_tibble() %&gt;% \n  mutate(across(starts_with(\"Sepal\"),\n                list(rounded = round, log = log1p)))%&gt;%\n  relocate(starts_with(\"Sepal\"),.before = 1)\n\n# A tibble: 150 × 9\n   Sepal.Length Sepal.Width Sepal.Length_rounded Sepal.Length_log\n          &lt;dbl&gt;       &lt;dbl&gt;                &lt;dbl&gt;            &lt;dbl&gt;\n 1          5.1         3.5                    5             1.81\n 2          4.9         3                      5             1.77\n 3          4.7         3.2                    5             1.74\n 4          4.6         3.1                    5             1.72\n 5          5           3.6                    5             1.79\n 6          5.4         3.9                    5             1.86\n 7          4.6         3.4                    5             1.72\n 8          5           3.4                    5             1.79\n 9          4.4         2.9                    4             1.69\n10          4.9         3.1                    5             1.77\n# ℹ 140 more rows\n# ℹ 5 more variables: Sepal.Width_rounded &lt;dbl&gt;, Sepal.Width_log &lt;dbl&gt;,\n#   Petal.Length &lt;dbl&gt;, Petal.Width &lt;dbl&gt;, Species &lt;fct&gt;\n\n\nWhat we can further do is to mutate a variable using group_by. This can be like below, step by step…\n\nenv %&gt;% \n  group_by(ForestType) %&gt;%\n  select(PlotID=RELEVE_NR, ForestType, Biomass)%&gt;%\n  mutate(meanBiomass=mean(Biomass))%&gt;%\n  mutate(comparison= (case_when(Biomass&gt;meanBiomass ~\"higher\", \n                               Biomass&lt;meanBiomass ~\"lower\",\n                               TRUE ~ \"equal\")))%&gt;%\n  arrange(ForestType,desc(Biomass))\n\n# A tibble: 65 × 5\n# Groups:   ForestType [4]\n   PlotID ForestType Biomass meanBiomass comparison\n    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n 1     10          1    79.1        26.7 higher    \n 2      7          1    49.2        26.7 higher    \n 3      6          1    46.4        26.7 higher    \n 4     44          1    34.1        26.7 higher    \n 5     82          1    33.9        26.7 higher    \n 6     28          1    29.7        26.7 higher    \n 7     42          1    21.5        26.7 lower     \n 8      5          1    20.7        26.7 lower     \n 9     81          1    20.6        26.7 lower     \n10      4          1    16          26.7 lower     \n# ℹ 55 more rows\n\n\nor like this, where we do not use stand alone group_by but much more effective .by directly included in the mutate function.\n\nenv %&gt;%\n  select(PlotID=RELEVE_NR, ForestType, Biomass)%&gt;%\n  mutate(\n    meanBiomass = mean(Biomass, na.rm = TRUE),\n    comparison = case_when(\n      Biomass &gt; meanBiomass ~ \"higher\",\n      Biomass &lt; meanBiomass ~ \"lower\",\n      TRUE ~ \"equal\"),\n    .by = ForestType)\n\n# A tibble: 65 × 5\n   PlotID ForestType Biomass meanBiomass comparison\n    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n 1      1          2    12.8        44.4 lower     \n 2      2          1     9.9        26.7 lower     \n 3      3          1    15.2        26.7 lower     \n 4      4          1    16          26.7 lower     \n 5      5          1    20.7        26.7 lower     \n 6      6          1    46.4        26.7 higher    \n 7      7          1    49.2        26.7 higher    \n 8      8          2    48.7        44.4 higher    \n 9      9          2    13.8        44.4 lower     \n10     10          1    79.1        26.7 higher    \n# ℹ 55 more rows\n\n\nWe need to realise, that after you use group_by, the data stay grouped until you explicitly call ungroup(), or use a verb that drops grouping automatically by default settings (usually summariseor count). While summariseremoves the grouping, mutate does not, therefore grouping still applies until the end of that call.\nAdvantage of .by is that it works only within the mutate function.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "5 Join functions"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/5_join_functions.html#advanced-summarise",
    "href": "DataManipulationVisualisation/5_join_functions.html#advanced-summarise",
    "title": "5 Join Functions",
    "section": "5.5 Advanced summarise",
    "text": "5.5 Advanced summarise\nSummarise reduces the information to single values per group, defined by the group_by function. We used summarise or summarise across and calculated min, mean, median,max values or sum of the values.\n\nenv %&gt;% \n  group_by(ForestTypeName) %&gt;%\n  summarise(meanBiomass= mean(Biomass))\n\n# A tibble: 4 × 2\n  ForestTypeName      meanBiomass\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 alluvial forest           148. \n2 oak forest                 26.7\n3 oak hornbeam forest        44.4\n4 ravine forest              79.1\n\n\nWe did not yet explain anything about n_distinct function. It can be used for getting unique combinations of selected variables directly in summarise. Like here unique number of species in each plot\n\nspe %&gt;% \n  summarise(unique_species = n_distinct(Species), .by = RELEVE_NR)\n\n# A tibble: 65 × 2\n   RELEVE_NR unique_species\n       &lt;dbl&gt;          &lt;int&gt;\n 1         1             42\n 2         2             16\n 3         3             15\n 4         4             20\n 5         5             14\n 6         6             19\n 7         7             22\n 8         8             23\n 9         9             22\n10        10             18\n# ℹ 55 more rows\n\n\nor unique pairs or combinations of selected variables, such as species +their combinations in vegetation layers -here in the whole dataset\n\nspe %&gt;% \n  summarise(unique_species_layer = n_distinct(interaction(Species, Layer)))\n\n# A tibble: 1 × 1\n  unique_species_layer\n                 &lt;int&gt;\n1                  369\n\n\nWhen you become confident with joins and summarise you can even do something like this. I am working with env data, but at the same time I want to use the summarised information from species data. And I can calculate it and match directly in the pipeline without stepping outside, as there is no need to calculating and saving intermediate steps separately.\n\nenv %&gt;% \n  left_join(spe %&gt;% \n              group_by(RELEVE_NR) %&gt;% \n              count(name = \"Richness\")) %&gt;%\n  select(PlotID=RELEVE_NR,ForestType, ForestTypeName, Richness, Biomass, pH_KCl)\n\nJoining with `by = join_by(RELEVE_NR)`\n\n\n# A tibble: 65 × 6\n   PlotID ForestType ForestTypeName      Richness Biomass pH_KCl\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1      1          2 oak hornbeam forest       44    12.8   5.28\n 2      2          1 oak forest                17     9.9   3.24\n 3      3          1 oak forest                16    15.2   4.01\n 4      4          1 oak forest                22    16     3.76\n 5      5          1 oak forest                15    20.7   3.50\n 6      6          1 oak forest                22    46.4   3.8 \n 7      7          1 oak forest                23    49.2   3.48\n 8      8          2 oak hornbeam forest       24    48.7   3.68\n 9      9          2 oak hornbeam forest       28    13.8   4.24\n10     10          1 oak forest                19    79.1   4.00\n# ℹ 55 more rows\n\n\nAnother useful thing is to know how to sum values across more columns and save it in another column, i.e. row-wise sum. The relevant function is called rowSums. We will look at the example of frozen fauna data availability and first we will turn the table into tidy by filling 0 and changing x to 1.\nAlternative is #mutate(across( c(pollen_spores, macrofossils, dna), ~ if_else(!is.na(.x) & .x == “x”, 1, 0)))\n\ndata&lt;-  read_xlsx(\"data/frozen_fauna/FrozenFauna_metadata.xlsx\", sheet=1)%&gt;% \n  clean_names()%&gt;% \n  mutate(across(c(pollen_spores, macrofossils, dna), ~ replace_na(.x, \"0\"))) %&gt;%\n  mutate(across(c(pollen_spores, macrofossils, dna), ~ if_else(.x == \"x\", 1, 0)))%&gt;%\n  mutate(dataAvailableSum = rowSums(across(pollen_spores:dna)))%&gt;% \n  print()\n\n# A tibble: 27 × 7\n      id code   species        pollen_spores macrofossils   dna dataAvailableSum\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;\n 1     1 D-M5   Mammuthus pri…             0            0     1                1\n 2     2 D-BP4  Bison priscus              0            0     1                1\n 3     3 D-MP11 Mammuthus pri…             1            1     0                2\n 4     4 SK2    Stephanorhinu…             0            1     0                1\n 5     5 MP5    Mammuthus pri…             1            1     0                2\n 6     6 CA2    Coelodonta an…             0            1     0                1\n 7     7 MP9    Mammuthus pri…             1            1     1                3\n 8     8 D-MP10 Mammuthus pri…             1            1     0                2\n 9     9 MP3    Mammuthus pri…             1            1     0                2\n10    10 MP4    Mammuthus pri…             1            1     0                2\n# ℹ 17 more rows\n\n\nSummarise is often used for getting so-called community means or community weighted means. For example we have traits for individual species and we want to calculate a mean for each site and compare it.\n\nspe%&gt;% \n  left_join(traits, by=c(\"Species\" =\"Taxon\"))%&gt;%\n  summarise(meanHeight= mean(Height), .by=RELEVE_NR)\n\n# A tibble: 65 × 2\n   RELEVE_NR meanHeight\n       &lt;dbl&gt;      &lt;dbl&gt;\n 1         1      NA   \n 2         2       6.00\n 3         3       3.50\n 4         4       8.14\n 5         5       3.82\n 6         6      NA   \n 7         7      NA   \n 8         8       3.52\n 9         9       8.25\n10        10      NA   \n# ℹ 55 more rows\n\n\nSo far, we had data with no missing values, but most probably you will come across situations where there will be some NAs. For this you will need to use argument na.rm=TRUE which indicates that you want to skip the NAs from counting. Otherwise a single NA would mean also the result will be NA. The same happened here, so we need to fix it.\n\nspe%&gt;% \n  left_join(traits, by=c(\"Species\" =\"Taxon\"))%&gt;%\n  summarise(meanHeight= mean(Height, na.rm=T), .by=RELEVE_NR)\n\n# A tibble: 65 × 2\n   RELEVE_NR meanHeight\n       &lt;dbl&gt;      &lt;dbl&gt;\n 1         1       4.60\n 2         2       6.00\n 3         3       3.50\n 4         4       8.14\n 5         5       3.82\n 6         6       3.3 \n 7         7       4.25\n 8         8       3.52\n 9         9       8.25\n10        10       3.53\n# ℹ 55 more rows\n\n\nWe can also consider to use abundance to weight the result. This is called community weighted mean and it simply gives higher importance to those species that are more abundant and lower importance to the rare once. Here the abundance is approximated as percentage cover in the site.\n\nspe %&gt;%\n  left_join(traits, by = c(\"Species\" = \"Taxon\")) %&gt;%\n  summarise(meanHeight= mean(Height, na.rm=T),\n            meanHeight_weighted = weighted.mean(Height, CoverPerc, na.rm = TRUE),\n            .by=RELEVE_NR)\n\n# A tibble: 65 × 3\n   RELEVE_NR meanHeight meanHeight_weighted\n       &lt;dbl&gt;      &lt;dbl&gt;               &lt;dbl&gt;\n 1         1       4.60               12.9 \n 2         2       6.00               16.3 \n 3         3       3.50               16.7 \n 4         4       8.14               17.2 \n 5         5       3.82               15.3 \n 6         6       3.3                 9.36\n 7         7       4.25               12.9 \n 8         8       3.52               11.2 \n 9         9       8.25               14.6 \n10        10       3.53               14.9 \n# ℹ 55 more rows\n\n\nYou can see that the results are quite different! Note that we are in forest so we now compare together heights of trees, which are dominants, and herbs which are small and are less abundant. Especially for the CMW it plays really big role. Therefore we will now compare only for the herb-layer (6).\n\nspe %&gt;%\n  left_join(traits, by = c(\"Species\" = \"Taxon\")) %&gt;%\n  filter(Layer==6)%&gt;%\n  summarise(meanHeight= mean(Height, na.rm=T),\n            meanHeight_weighted = weighted.mean(Height, CoverPerc, na.rm = TRUE),\n            .by=RELEVE_NR)\n\n# A tibble: 65 × 3\n   RELEVE_NR meanHeight meanHeight_weighted\n       &lt;dbl&gt;      &lt;dbl&gt;               &lt;dbl&gt;\n 1         1      0.379               0.367\n 2         2      0.496               0.483\n 3         3      0.430               0.404\n 4         4      0.598               0.593\n 5         5      0.563               0.527\n 6         6      0.562               0.561\n 7         7      0.491               0.522\n 8         8      0.452               0.657\n 9         9      0.492               0.481\n10        10      0.539               0.555\n# ℹ 55 more rows\n\n\nIn the same way we can calculate community means across more variables. Here we will try the Ellenberg indicator values (abbreviated as EIV, measures of species demands for the particular factors, the higher value mean higher demands or affinity to habitats with higher values of these environmental factors). We will apply them just for presences of species, so no weights. Therefore we do not need to care that much for the differences among layers. However, we will keep each species only once (list of unique species for each plot), so we will remove information about layers first and use distinct. Alternative is to add #filter(Layer==6)%&gt;% if we want to focus on herb-layer only\n\nspe %&gt;%\n  left_join(traits, by = c(\"Species\" = \"Taxon\")) %&gt;%\n  select(-c(Layer,CoverPerc))%&gt;%\n  distinct()%&gt;%\n  group_by(RELEVE_NR)%&gt;%\n  summarise(across(starts_with(\"EIV\"), ~ mean(.x, na.rm = TRUE)))\n\n# A tibble: 65 × 5\n   RELEVE_NR EIV_Light EIV_Moisture EIV_Reaction EIV_Nutrients\n       &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1         1      5.08         4.64         6.54          4.95\n 2         2      4.88         4.81         4.94          4.31\n 3         3      4.87         4.8          4.93          4.4 \n 4         4      5.3          4.5          5.7           4.25\n 5         5      6.07         4.14         5.57          3.86\n 6         6      5.72         4.5          6.39          5.22\n 7         7      5.8          4.4          6.25          4.9 \n 8         8      5.17         4.61         5.43          4.65\n 9         9      5.32         4.5          5.77          4.41\n10        10      6.38         3.69         6.19          3.88\n# ℹ 55 more rows",
    "crumbs": [
      "Data Manipulation and Visualization",
      "5 Join functions"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/5_join_functions.html#exercises",
    "href": "DataManipulationVisualisation/5_join_functions.html#exercises",
    "title": "5 Join Functions",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\n1. Do a bit more with Forest understory data. Upload the env file called Axmanova-Forest-env.xlsx create two subsets: forest1, where you select PlotID=RELEVE_NR, ForestType, ForestTypeName and forest2, with columns ForestType, pH_KCl, Biomass &gt; Now use left_join to match forest2 to forest1. What happened? Check how many times there are rows with the same PlotID using some simple functions.\n2. Upload the data from Lepidoptera folder . First the env data &gt; check the structure. The structure is inverted and we will first transpose it to be able to work with it normally.\n\nenv&lt;- read_xlsx(\"data/lepidoptera/env_char_MSVejnoha.xlsx\") %&gt;%\n  column_to_rownames(var = \"Site\") %&gt;%  # move 'Site' column to row names\n  t() %&gt;%                               # transpose like Excel\n  as.data.frame() %&gt;%                   # back to data frame\n  rownames_to_column(var = \"Site\") %&gt;%  # make the row names a column again\n  as_tibble()%&gt;% \n  view()\n\nNow look at the tibble again. Since we went through the step with dataframe, everything is changed to a text. Add one more row to change everything to numeric (as.numeric) except for the first column called Site (ID of the site) &gt;&gt; check the histogram of the cover of trees across sites &gt;&gt; prepare a new variable called tree_cover_groups corresponding to low/medium/high cover of tress (select the tresholds based on the histogram) &gt; filter out the group with only low cover &gt;&gt; Import the spe data and filter to have the same sites in both files\n3. Upload Penguins data &gt; check names (penguins) &gt; calculate mean weight and SD of weight for different penguin species &gt; visualise the results by ggplot (geom_col).\n4. Stay with Penguins, count how many individuals are at each island and to which species they belong. &gt; use a ggplot to visualise this. *Prepare a wide format matrix with islands as rows x species as names, where values will be counts of individuals.\n5. Get back to Forest understory species file. Axmanova-Forest-spe.xlsx In the data you can see which species occur in the tree layer (=1) &gt; prepare a table where for each plotID you will have in another column a single name of the tree dominant, i.e. tree with highest cover (for simplicity, specify in the slice_max to take just n=1, with_ties=F) &gt; append it to env data &gt; summarise counts of plots with each of the dominant *alternatively you can prepare a script, where all spe data handling will be a part of pipeline of env data handling\n6.Use forest datasets - species and traits &gt; calculate community mean and community weighted mean for SeedMass. *create boxplots for these means within forest types (info about forest type is available in env file).",
    "crumbs": [
      "Data Manipulation and Visualization",
      "5 Join functions"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/5_join_functions.html#further-reading",
    "href": "DataManipulationVisualisation/5_join_functions.html#further-reading",
    "title": "5 Join Functions",
    "section": "5.7 Further reading",
    "text": "5.7 Further reading\nJoins -chapter in R for data science https://r4ds.hadley.nz/joins.html\nSummarise https://dplyr.tidyverse.org/reference/summarise.html\nSummarise multiple columns https://dplyr.tidyverse.org/reference/summarise_all.html\nMutate across https://dplyr.tidyverse.org/reference/across.html\nIf else, case when chapter in R for data science https://r4ds.hadley.nz/logicals.html#conditional-transformations\nCase when https://dplyr.tidyverse.org/reference/case_when.html\nCase when example https://rpubs.com/jenrichmond/case_when",
    "crumbs": [
      "Data Manipulation and Visualization",
      "5 Join functions"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/7_8_automatisation.html",
    "href": "DataManipulationVisualisation/7_8_automatisation.html",
    "title": "7 + 8 Automatisation",
    "section": "",
    "text": "“Copy-and-paste is a powerful tool, but you should avoid doing it more than twice.” – Hadley Wickham, R for Data Science\nIt’s not only a matter of the script length. Repeating the same code multiple times might easily lead to errors and inconsistencies, and it is therefore better to avoid it. There are multiple ways to reduce copy-pasting when we want to repeat a similar operation multiple times. In this chapter, you will learn how to write your own function and some tools for iteration, including for loops and functions from the purrr package.\nThroughout this chapter, we will use the following packages:\nlibrary(palmerpenguins)\nlibrary(broom)\nlibrary(tidyverse)",
    "crumbs": [
      "Data Manipulation and Visualization",
      "7 + 8 Automatisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/7_8_automatisation.html#functions",
    "href": "DataManipulationVisualisation/7_8_automatisation.html#functions",
    "title": "7 + 8 Automatisation",
    "section": "7.1 Functions",
    "text": "7.1 Functions\nIn more complex data analysis tasks, when you need to repeat a similar operation multiple times, e.g. calculate the same model for multiple subsets of data, calculate models with the same explanatory variables for different response variables, or draw similarly looking plots for multiple variables, it becomes really useful to be able to write your own function and hide the repeated code into it. A huge advantage of only writing the code once and saving it as a function is that when you have to change something, you only do it once and thus prevent mistakes like replacing the variable name in one place but not in the other, etc. In the long term, it also saves time and improves the understandability of the code, as the script does not end up being hundreds or thousands of lines long and all important commands are in one place.\nWe will use a penguin dataset that you know already:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n7.1.1 Vector functions\nLet’s say we want to standardise each measured variable in the dataset, and just imagine for now that there is no scale() function, so we have to do it manually. We would end up with something like this:\n\npenguins |&gt; \n  mutate(bill_length_mm = (bill_length_mm - mean(bill_length_mm, na.rm = T))/sd(bill_length_mm, na.rm = T), \n         bill_depth_mm = (bill_depth_mm - mean(bill_depth_mm, na.rm = T))/sd(bill_depth_mm, na.rm = T),\n         flipper_length_mm = (flipper_length_mm - mean(flipper_length_mm, na.rm = T))/sd(flipper_length_mm, na.rm = T),\n         body_mass_g = (body_mass_g - mean(body_mass_g, na.rm = T))/sd(body_mass_g, na.rm = T))\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Torgersen         -0.883         0.784            -1.42      -0.563 \n 2 Adelie  Torgersen         -0.810         0.126            -1.06      -0.501 \n 3 Adelie  Torgersen         -0.663         0.430            -0.421     -1.19  \n 4 Adelie  Torgersen         NA            NA                NA         NA     \n 5 Adelie  Torgersen         -1.32          1.09             -0.563     -0.937 \n 6 Adelie  Torgersen         -0.847         1.75             -0.776     -0.688 \n 7 Adelie  Torgersen         -0.920         0.329            -1.42      -0.719 \n 8 Adelie  Torgersen         -0.865         1.24             -0.421      0.590 \n 9 Adelie  Torgersen         -1.80          0.480            -0.563     -0.906 \n10 Adelie  Torgersen         -0.352         1.54             -0.776      0.0602\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nWhen creating such a code, it is quite easy to forget to replace the variable name in one place when copying it. To avoid that and make the code easier to read, we can transform the code into a function. We need three things to do that:\n\na function name, this should be concise and informative (please avoid myfunction1(), etc.) and not mess up already existing functions, we will use custom_scale to distinguish our function from the scale().\narguments, which are the things that we want to vary across calls, in our case, we have just one numerical variable we are working with, so we will call it x.\nbody, that is the code that stays the same across calls.\n\nEvery function defined in R has the following structure:\n\nname &lt;- function(arguments){\n  body\n}\n\nIn our case, the function would look like this:\n\ncustom_scale &lt;- function(x){\n  (x - mean(x, na.rm = T))/sd(x, na.rm = T)\n}\n\nWhen we run this code, our new function is saved into the environment, and we can use it as any other function. We created a function that takes a vector and returns a vector of the same length that might be use within the mutate() function.\n\npenguins |&gt; \n  mutate(bill_length_mm = custom_scale(bill_length_mm), \n         bill_depth_mm = custom_scale(bill_depth_mm),\n         flipper_length_mm = custom_scale(flipper_length_mm),\n         body_mass_g = custom_scale(body_mass_g))\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Torgersen         -0.883         0.784            -1.42      -0.563 \n 2 Adelie  Torgersen         -0.810         0.126            -1.06      -0.501 \n 3 Adelie  Torgersen         -0.663         0.430            -0.421     -1.19  \n 4 Adelie  Torgersen         NA            NA                NA         NA     \n 5 Adelie  Torgersen         -1.32          1.09             -0.563     -0.937 \n 6 Adelie  Torgersen         -0.847         1.75             -0.776     -0.688 \n 7 Adelie  Torgersen         -0.920         0.329            -1.42      -0.719 \n 8 Adelie  Torgersen         -0.865         1.24             -0.421      0.590 \n 9 Adelie  Torgersen         -1.80          0.480            -0.563     -0.906 \n10 Adelie  Torgersen         -0.352         1.54             -0.776      0.0602\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nWe can, of course, reduce the duplication even further by using the across() function:\n\npenguins |&gt; \n  mutate(across(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g), ~custom_scale(.x)))\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Torgersen         -0.883         0.784            -1.42      -0.563 \n 2 Adelie  Torgersen         -0.810         0.126            -1.06      -0.501 \n 3 Adelie  Torgersen         -0.663         0.430            -0.421     -1.19  \n 4 Adelie  Torgersen         NA            NA                NA         NA     \n 5 Adelie  Torgersen         -1.32          1.09             -0.563     -0.937 \n 6 Adelie  Torgersen         -0.847         1.75             -0.776     -0.688 \n 7 Adelie  Torgersen         -0.920         0.329            -1.42      -0.719 \n 8 Adelie  Torgersen         -0.865         1.24             -0.421      0.590 \n 9 Adelie  Torgersen         -1.80          0.480            -0.563     -0.906 \n10 Adelie  Torgersen         -0.352         1.54             -0.776      0.0602\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThere are some useful RStudio keyboard shortcuts you can use to work with functions:\n\nTo find a definition of any function, place a cursor on the name of the function in the script and press F2.\nTo extract a function from a code you have written, use Alt + Ctrl + X. Just be sure you always check the results, because sometimes it will not do exactly what you expect, so you will have to make some adjustments.\n\nWe can also write summary functions to be used in a summarise() call. For example, we could calculate the difference between the minimum and maximum values of the given variable:\n\nrange_diff &lt;- function(x){\n  max(x, na.rm = T) - min(x, na.rm = T)\n}\n\npenguins |&gt; \n  summarize(bill_length_mm = range_diff(bill_length_mm), \n            bill_depth_mm = range_diff(bill_depth_mm))\n\n# A tibble: 1 × 2\n  bill_length_mm bill_depth_mm\n           &lt;dbl&gt;         &lt;dbl&gt;\n1           27.5           8.4\n\n\n\n\n7.1.2 Data frame functions\nTill now, we have written several vector functions that might be used within dplyr functions. But functions may operate even at the data frame level. We will show here a real-world example of data sampled in three types of sand vegetation - pioneer sand vegetation (Corynephorion), acidophilous sand grasslands (Armerion) and basiphilous sand grasslands (Festucion valesiacae). The species data of all three vegetation types are saved in a long format in one file, and the header data with cluster assignment in a second file.\n\nspe_long &lt;- read_csv('data/sands/sands_spe_long.csv')\n\nRows: 3154 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): valid_name\ndbl (2): releve_nr, cover_perc\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(spe_long)\n\nRows: 3,154\nColumns: 3\n$ releve_nr  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,…\n$ valid_name &lt;chr&gt; \"Achillea millefolium agg.\", \"Artemisia campestris\", \"Carex…\n$ cover_perc &lt;dbl&gt; 0.5, 0.5, 0.5, 3.0, 0.5, 15.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…\n\nhead &lt;- read_csv('data/sands/sands_head.csv') |&gt; \n  select(releve_nr, cluster)\n\nRows: 172 Columns: 89\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (40): country, reference, nr_in_tab, coverscale, author, syntaxon, moss_...\ndbl (46): releve_nr, table_nr, date, surf_area, altitude, exposition, inclin...\nlgl  (3): rs_project, maniptyp, name_ass\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(head)\n\nRows: 172\nColumns: 2\n$ releve_nr &lt;dbl&gt; 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, …\n$ cluster   &lt;chr&gt; \"Corynephorion\", \"Armerion\", \"Corynephorion\", \"Corynephorion…\n\n\nImagine we now want to run an ordination analysis that needs species data in a wide format for each of the three vegetation types separately. We need to subset the species data to only the selected vegetation type, select only relevant columns, square-root the species abundances, and transform the data into a wide format. We can incorporate all these steps into a single function, that we will then run three times for different subsets:\n\nsubset_to_wide &lt;- function(data_long, veg_type){\n  data_long |&gt; \n    semi_join(head |&gt; filter(cluster == veg_type)) |&gt;\n    select(releve_nr, valid_name, cover_perc) |&gt; \n    mutate(cover_perc = sqrt(cover_perc)) |&gt; \n    pivot_wider(names_from = valid_name, values_from = cover_perc, values_fill = 0) |&gt; \n    select(-releve_nr)\n}\n\nspe_wide_cory &lt;- subset_to_wide(spe_long, 'Corynephorion')\n\nJoining with `by = join_by(releve_nr)`\n\nspe_wide_arm &lt;- subset_to_wide(spe_long, 'Armerion')\n\nJoining with `by = join_by(releve_nr)`\n\nspe_wide_fes &lt;- subset_to_wide(spe_long, 'Festucion valesiacae')\n\nJoining with `by = join_by(releve_nr)`\n\n\nWriting your own functions with the dplyr and tidyr calls inside sometimes also brings some challenges. We unfortunately do not have enough space here to deal with them, but there are great sources with detailed explanation, where you can learn more or find help if needed, e.g. https://r4ds.hadley.nz/functions.html#data-frame-functions, programming with dplyr, programming with tidyr, What is data-masking and why do I need {{?.\n\n\n7.1.3 Plot functions\nSometimes it is also useful to reduce the amount of replicated code when plotting many different things in a similar way. The plot functions work similarly to the data frame functions, just return a plot instead of a data frame. For example, we might want to visualise the relationship between bill length and bill depth of three different penguin species separately. We have to first filter our penguin dataset to contain data on only one species, and then create a scatterplot using a ggplot sequence. This is how we would write the code for plotting the data for one species:\n\npenguins |&gt; \n    filter(species == 'Adelie') |&gt; \n    ggplot(aes(bill_length_mm, bill_depth_mm)) +\n    geom_point() +\n    theme_bw() +\n    labs(title = 'Adelie', x = 'Bill length [mm]', y = 'Bill depth [mm]')\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nLet’s now turn this code into a function instead of copy-pasting it two more times to plot the same relationship for the two other species:\n\nplot_species &lt;- function(data, species_name){\n  data |&gt; \n    filter(species == species_name) |&gt; \n    ggplot(aes(bill_length_mm, bill_depth_mm)) +\n    geom_point() +\n    theme_bw() +\n    labs(title = species_name, x = 'Bill length [mm]', y = 'Bill depth [mm]')\n}\nplot_species(penguins, 'Adelie')\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nTo learn more about reducing duplication in your ggplot2 code look here: https://r4ds.hadley.nz/functions.html#plot-functions, Programming with ggplot2, https://ggplot2.tidyverse.org/articles/ggplot2-in-packages.html.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "7 + 8 Automatisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/7_8_automatisation.html#for-loops",
    "href": "DataManipulationVisualisation/7_8_automatisation.html#for-loops",
    "title": "7 + 8 Automatisation",
    "section": "7.2 For loops",
    "text": "7.2 For loops\nWe will now move to the iteration part of this chapter, which means repeating the same action multiple times on different objects. In any programming language, it is possible to automate such repetitions using a for loop. The basic structure of a for loop looks like this:\n\nfor (variable in sequence) {\n  # do something with the variable\n}\n\nThe for loop takes one variable from a given sequence, runs the code inside {} for this variable and then moves to the next variable in the sequence. A basic example might be printing numbers from a sequence:\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nThe for loop takes one number, prints it, then takes the next one, prints it, and so on till the end of the sequence.\nWe can, for example, take our plot function to plot the scatterplot of bill length and bill depth of individual species and loop over the species names to sequentially make a plot for all species.\n\nfor (species_name in unique(penguins$species)) {\n  plot_species(penguins, species_name) |&gt; \n    print()\n}\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are always multiple ways to get to the same result, and this is also true for iteration in R. The purrr package has powerful tools to iterate over multiple elements. Once the problems become more complex, it becomes more effective to use purrr functions in a pipeline instead of complicated nested for loops. But even when you choose to prefer purrr solutions, it is worth being familiar with for loops and their functionality, because you might see them often in the code of other people, and they are universal across programming languages.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "7 + 8 Automatisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/7_8_automatisation.html#purrr-and-working-with-nested-dataframes",
    "href": "DataManipulationVisualisation/7_8_automatisation.html#purrr-and-working-with-nested-dataframes",
    "title": "7 + 8 Automatisation",
    "section": "7.3 purrr and working with nested dataframes",
    "text": "7.3 purrr and working with nested dataframes\nWe already went through most of the tidyverse packages, but we didn’t talk about the purrr package yet. purrr provides powerful tools for automatisation of tasks that need to be repeated multiple times, e.g. for each file in a directory, each element of a list, each dataframe… They allow you to replace for loops with map() functions, which might be more powerful, readable and consistent with the rest of tidyverse.\nLet’s start with the task of reading multiple files at once. Imagine you have multiple files that are of the same structure, but for some reason, they are stored in multiple files. As example data, we will use the gapminder dataset, which provides values for life expectancy, GDP per capita, and population size. The data we have in the gapminder folder is divided by year, and we now want to load them all and combine them into a single tibble. We could do that by copy-pasting the read_csv() function twelve times, but there is a more elegant way. Let’s list the files first:\n\npaths &lt;- list.files('data/gapminder/', pattern = '.csv', full.names = T)\npaths\n\n [1] \"data/gapminder/gapminder_1952.csv\" \"data/gapminder/gapminder_1957.csv\"\n [3] \"data/gapminder/gapminder_1962.csv\" \"data/gapminder/gapminder_1967.csv\"\n [5] \"data/gapminder/gapminder_1972.csv\" \"data/gapminder/gapminder_1977.csv\"\n [7] \"data/gapminder/gapminder_1982.csv\" \"data/gapminder/gapminder_1987.csv\"\n [9] \"data/gapminder/gapminder_1992.csv\" \"data/gapminder/gapminder_1997.csv\"\n[11] \"data/gapminder/gapminder_2002.csv\" \"data/gapminder/gapminder_2007.csv\"\n\n\nThe map() function works similarly to across(), but instead of doing something to each column in a data frame, it does something to each element of a vector. It is an analogy of a for loop, the function takes each element of a sequence and applies a function to it. We can use it now to read all 12 csv files in one line:\n\nfiles &lt;- map(paths, ~read_csv(.x))\n\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 142 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, continent\ndbl (3): lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(files)\n\nList of 12\n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 28.8 55.2 43.1 30 62.5 ...\n  ..$ pop      : num [1:142] 8425333 1282697 9279525 4232095 17876956 ...\n  ..$ gdpPercap: num [1:142] 779 1601 2449 3521 5911 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 30.3 59.3 45.7 32 64.4 ...\n  ..$ pop      : num [1:142] 9240934 1476505 10270856 4561361 19610538 ...\n  ..$ gdpPercap: num [1:142] 821 1942 3014 3828 6857 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 32 64.8 48.3 34 65.1 ...\n  ..$ pop      : num [1:142] 10267083 1728137 11000948 4826015 21283783 ...\n  ..$ gdpPercap: num [1:142] 853 2313 2551 4269 7133 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 34 66.2 51.4 36 65.6 ...\n  ..$ pop      : num [1:142] 11537966 1984060 12760499 5247469 22934225 ...\n  ..$ gdpPercap: num [1:142] 836 2760 3247 5523 8053 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 36.1 67.7 54.5 37.9 67.1 ...\n  ..$ pop      : num [1:142] 13079460 2263554 14760787 5894858 24779799 ...\n  ..$ gdpPercap: num [1:142] 740 3313 4183 5473 9443 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 38.4 68.9 58 39.5 68.5 ...\n  ..$ pop      : num [1:142] 14880372 2509048 17152804 6162675 26983828 ...\n  ..$ gdpPercap: num [1:142] 786 3533 4910 3009 10079 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 39.9 70.4 61.4 39.9 69.9 ...\n  ..$ pop      : num [1:142] 12881816 2780097 20033753 7016384 29341374 ...\n  ..$ gdpPercap: num [1:142] 978 3631 5745 2757 8998 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 40.8 72 65.8 39.9 70.8 ...\n  ..$ pop      : num [1:142] 13867957 3075321 23254956 7874230 31620918 ...\n  ..$ gdpPercap: num [1:142] 852 3739 5681 2430 9140 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 41.7 71.6 67.7 40.6 71.9 ...\n  ..$ pop      : num [1:142] 16317921 3326498 26298373 8735988 33958947 ...\n  ..$ gdpPercap: num [1:142] 649 2497 5023 2628 9308 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 41.8 73 69.2 41 73.3 ...\n  ..$ pop      : num [1:142] 22227415 3428038 29072015 9875024 36203463 ...\n  ..$ gdpPercap: num [1:142] 635 3193 4797 2277 10967 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 42.1 75.7 71 41 74.3 ...\n  ..$ pop      : num [1:142] 25268405 3508512 31287142 10866106 38331121 ...\n  ..$ gdpPercap: num [1:142] 727 4604 5288 2773 8798 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n $ : spc_tbl_ [142 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 43.8 76.4 72.3 42.7 75.3 ...\n  ..$ pop      : num [1:142] 31889923 3600523 33333216 12420476 40301927 ...\n  ..$ gdpPercap: num [1:142] 975 5937 6223 4797 12779 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   continent = col_character(),\n  .. ..   lifeExp = col_double(),\n  .. ..   pop = col_double(),\n  .. ..   gdpPercap = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nWe got a list with 12 data frames. To access the first element of the list, we would call files[[1]]. For further work with lists and map() functions, it is worth remembering that the elements of a list are called with the double square brackets.\nTo combine all data frames in a list, we can use list_rbind():\n\ngapminder_df &lt;- map(paths, ~read_csv(.x)) |&gt; \n  list_rbind()\nglimpse(gapminder_df)\n\nRows: 1,704\nColumns: 5\n$ country   &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Argentina\", …\n$ continent &lt;chr&gt; \"Asia\", \"Europe\", \"Africa\", \"Africa\", \"Americas\", \"Oceania\",…\n$ lifeExp   &lt;dbl&gt; 28.801, 55.230, 43.077, 30.015, 62.485, 69.120, 66.800, 50.9…\n$ pop       &lt;dbl&gt; 8425333, 1282697, 9279525, 4232095, 17876956, 8691212, 69277…\n$ gdpPercap &lt;dbl&gt; 779.4453, 1601.0561, 2449.0082, 3520.6103, 5911.3151, 10039.…\n\n\nBut we somehow lost the information about the year. To fix it, we store the file names in the data frame. First step is to set names of the list elements of paths, the basename() function extracts just the file name from the path. Second, we save the names in a resulting data frame to a column called year.\n\ngapminder_df &lt;- paths |&gt; \n  set_names(basename) |&gt; \n  map(~read_csv(.x)) |&gt; \n  list_rbind(names_to = 'year')\nglimpse(gapminder_df)\n\nRows: 1,704\nColumns: 6\n$ year      &lt;chr&gt; \"gapminder_1952.csv\", \"gapminder_1952.csv\", \"gapminder_1952.…\n$ country   &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Argentina\", …\n$ continent &lt;chr&gt; \"Asia\", \"Europe\", \"Africa\", \"Africa\", \"Americas\", \"Oceania\",…\n$ lifeExp   &lt;dbl&gt; 28.801, 55.230, 43.077, 30.015, 62.485, 69.120, 66.800, 50.9…\n$ pop       &lt;dbl&gt; 8425333, 1282697, 9279525, 4232095, 17876956, 8691212, 69277…\n$ gdpPercap &lt;dbl&gt; 779.4453, 1601.0561, 2449.0082, 3520.6103, 5911.3151, 10039.…\n\n\nStill not perfect, it would be nice to extract just the year from the file name. The easiest way to do it in this case is to use the parse_number() function that extracts just a number from a string:\n\ngapminder_df &lt;- paths |&gt; \n  set_names(basename) |&gt; \n  map(~read_csv(.x)) |&gt; \n  list_rbind(names_to = 'year') |&gt; \n  mutate(year = parse_number(year))\nglimpse(gapminder_df)\n\nRows: 1,704\nColumns: 6\n$ year      &lt;dbl&gt; 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, 1952, …\n$ country   &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Argentina\", …\n$ continent &lt;chr&gt; \"Asia\", \"Europe\", \"Africa\", \"Africa\", \"Americas\", \"Oceania\",…\n$ lifeExp   &lt;dbl&gt; 28.801, 55.230, 43.077, 30.015, 62.485, 69.120, 66.800, 50.9…\n$ pop       &lt;dbl&gt; 8425333, 1282697, 9279525, 4232095, 17876956, 8691212, 69277…\n$ gdpPercap &lt;dbl&gt; 779.4453, 1601.0561, 2449.0082, 3520.6103, 5911.3151, 10039.…\n\n\nIt might be a good idea to save the resulting data frame as a single csv file now to make future data loading easier.\n\nwrite_csv(gapminder_df, 'data/gapminder_clean/gapminder.csv')\n\npurrr contains not only the map() function, but also it relatives, so we will look at the differences between them now:\n\nmap() makes a list\nmap_lgl() makes a logical vector\nmap_int() makes an integer vector\nmap_dbl() makes a double vector\nmap_chr() makes a character vector\n\nEach of the above-mentioned functions takes a vector input, applies a function to each element and returns a vector of the same length.\n\nmap2() takes two vectors, usually of the same length and iterates over two arguments at a time\n\nAnd there is also a group of walk() functions, which return their side-effects and return the input .x. They might be used, for example, for saving multiple files or plots.\nLet’s say, we want to save our gapminder data divided by continent. We first make a nested data frame by continent. This is something similar to group_by(), we take a variable that makes groups in our dataset and divide the rest of the data according to this variable. The grouping variable stays in one column, and a smaller data frame is created for each level of this variable. All these smaller data frames are stored in the data column. We now want to save each of these smaller data frames as a separate csv file. We now create a column, where we define a path for each file to be saved.\n\ngapminder_nest &lt;- gapminder_df |&gt; \n  nest(data = -continent) |&gt; \n  mutate(path = paste0('data/gapminder_continent/gapminder_', continent, '.csv')) \n\nglimpse(gapminder_nest)\n\nRows: 5\nColumns: 3\n$ continent &lt;chr&gt; \"Asia\", \"Europe\", \"Africa\", \"Americas\", \"Oceania\"\n$ data      &lt;list&gt; [&lt;tbl_df[396 x 5]&gt;], [&lt;tbl_df[360 x 5]&gt;], [&lt;tbl_df[624 x 5]&gt;…\n$ path      &lt;chr&gt; \"data/gapminder_continent/gapminder_Asia.csv\", \"data/gapmin…\n\n\nWe can see that the data column is a list containing tibbles with different numbers of rows, but the same number of columns. To look at one of them we need to use the [[]] again.\n\ngapminder_nest$data[[1]]\n\n# A tibble: 396 × 5\n    year country          lifeExp       pop gdpPercap\n   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  1952 Afghanistan         28.8   8425333      779.\n 2  1952 Bahrain             50.9    120447     9867.\n 3  1952 Bangladesh          37.5  46886859      684.\n 4  1952 Cambodia            39.4   4693836      368.\n 5  1952 China               44   556263527      400.\n 6  1952 Hong Kong, China    61.0   2125900     3054.\n 7  1952 India               37.4 372000000      547.\n 8  1952 Indonesia           37.5  82052000      750.\n 9  1952 Iran                44.9  17272000     3035.\n10  1952 Iraq                45.3   5441766     4130.\n# ℹ 386 more rows\n\n\nAnd we can save all the tibbles in separate csv files at once using the walk2() call, which at each step takes one tibble from the data column and one path definition from the path column and runs the write_csv() function with these two arguments.\n\nwalk2(gapminder_nest$data, gapminder_nest$path, ~write_csv(.x, .y))\n\nThe work with nested data frames becomes really helpful when we want to perform the same calculation many times. We will work with the gapminder data again and try to answer the following questions: How does life expectancy change over time in individual countries? In which countries has the life expectancy risen the most over time?\nTo explore the data a little bit first, we can plot them:\n\ngapminder_df |&gt; \n  ggplot(aes(year, lifeExp, group = country)) +\n  geom_line()\n\n\n\n\n\n\n\n\nOverall, life expectancy has been steadily increasing over time, but we need some estimation of the trend in individual countries. A possible way to do this would be to fit a linear model to the data from each country and look at the estimate. Given the 142 countries in the dataset, we really do not want to run the code for each one manually with copy-pasted code. Because we want to work at the country level, we will now nest our data by country. To calculate a linear model for data from each country, we will iterate over the data column and save the resulting model to a new column. This might be done with map() within the mutate() call.\n\nby_country &lt;- gapminder_df |&gt; \n  nest(data = -country) |&gt; \n  mutate(m = map(data, ~lm(lifeExp~year, data = .x)))\n\nThe column m we just created is again a list and contains the results of linear models for the relationship between life expectancy and time for all countries. With each one of them, we can do whatever is possible with a model result:\n\nsummary(by_country$m[[1]])\n\n\nCall:\nlm(formula = lifeExp ~ year, data = .x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5447 -0.9905 -0.2757  0.8847  1.6868 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -507.53427   40.48416  -12.54 1.93e-07 ***\nyear           0.27533    0.02045   13.46 9.84e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.223 on 10 degrees of freedom\nMultiple R-squared:  0.9477,    Adjusted R-squared:  0.9425 \nF-statistic: 181.2 on 1 and 10 DF,  p-value: 9.835e-08\n\n\nTo easily extract the estimates from the model, we will use tidy() function from the broom package. broom is not a part of the tidyverse, but it is a related package, which becomes very helpful when working with models within the tidyverse workflow, because it provides tools to convert statistical objects into tidy tibbles. The tidy() function summarizes information about the model components:\n\ntidy(by_country$m[[1]])\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept) -508.      40.5        -12.5 0.000000193 \n2 year           0.275    0.0205      13.5 0.0000000984\n\n\nAnd we can again use it to iterate over all models to get these summaries for all countries.\n\nby_country &lt;- gapminder_df |&gt; \n  nest(data = -country) |&gt; \n  mutate(m = map(data, ~lm(lifeExp~year, data = .x)), \n         m_tidy = map(m, ~tidy(.x)))\n\nglimpse(by_country)\n\nRows: 142\nColumns: 4\n$ country &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Argentina\", \"A…\n$ data    &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], […\n$ m       &lt;list&gt; [-507.5342716, 0.2753287, -1.10629487, -0.95193823, -0.663581…\n$ m_tidy  &lt;list&gt; [&lt;tbl_df[2 x 5]&gt;], [&lt;tbl_df[2 x 5]&gt;], [&lt;tbl_df[2 x 5]&gt;], [&lt;tb…\n\n\nThe summary is now in a tibble, but there is still a list of tibbles in the m_tidy column. To get the results to a data frame where we can sort the values and filter across all values, we need to unnest().\n\nby_country &lt;- gapminder_df |&gt; \n  nest(data = -country) |&gt; \n  mutate(m = map(data, ~lm(lifeExp~year, data = .x)), \n         m_tidy = map(m, ~tidy(.x))) |&gt; \n  unnest(m_tidy)\n\nglimpse(by_country)\n\nRows: 284\nColumns: 8\n$ country   &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Albania\", \"Albania\", \"Algeria…\n$ data      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;],…\n$ m         &lt;list&gt; [-507.5342716, 0.2753287, -1.10629487, -0.95193823, -0.6635…\n$ term      &lt;chr&gt; \"(Intercept)\", \"year\", \"(Intercept)\", \"year\", \"(Intercept)\",…\n$ estimate  &lt;dbl&gt; -507.5342716, 0.2753287, -594.0725110, 0.3346832, -1067.8590…\n$ std.error &lt;dbl&gt; 40.484161954, 0.020450934, 65.655359062, 0.033166387, 43.802…\n$ statistic &lt;dbl&gt; -12.536613, 13.462890, -9.048348, 10.091036, -24.379118, 25.…\n$ p.value   &lt;dbl&gt; 1.934055e-07, 9.835213e-08, 3.943337e-06, 1.462763e-06, 3.07…\n\n\nWe now have a column for each column originally included in the tibbles inside m_tidy. The list-columns we did not unnest still remain list-columns. There are now two rows for each country, one for each model coefficient - the intercept and the year. We are not interested in model intercepts now, so we can filter them out. To see in which countries has the life expectancy risen the most over time, we can arrange the dataset according to the model estimate.\n\nby_country &lt;- gapminder_df |&gt; \n  nest(data = -country) |&gt; \n  mutate(m = map(data, ~lm(lifeExp~year, data = .x)), \n         m_tidy = map(m, ~tidy(.x))) |&gt; \n  unnest(m_tidy) |&gt; \n  filter(term == 'year') |&gt; \n  arrange(desc(estimate))\n\nglimpse(by_country)\n\nRows: 142\nColumns: 8\n$ country   &lt;chr&gt; \"Oman\", \"Vietnam\", \"Saudi Arabia\", \"Indonesia\", \"Libya\", \"Ye…\n$ data      &lt;list&gt; [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;], [&lt;tbl_df[12 x 5]&gt;],…\n$ m         &lt;list&gt; [-1470.085705, 0.772179, 0.3702564, -0.9886387, -1.7645338,…\n$ term      &lt;chr&gt; \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yea…\n$ estimate  &lt;dbl&gt; 0.7721790, 0.6716154, 0.6496231, 0.6346413, 0.6255357, 0.605…\n$ std.error &lt;dbl&gt; 0.039265234, 0.021970572, 0.034812325, 0.010796685, 0.025754…\n$ statistic &lt;dbl&gt; 19.665718, 30.568863, 18.660721, 58.781125, 24.288551, 22.82…\n$ p.value   &lt;dbl&gt; 2.530346e-09, 3.289412e-11, 4.221338e-09, 4.931386e-14, 3.18…\n\n\nOr we can just print out the top 10 countries:\n\nby_country |&gt; slice_max(estimate, n = 10)\n\n# A tibble: 10 × 8\n   country            data     m     term  estimate std.error statistic  p.value\n   &lt;chr&gt;              &lt;list&gt;   &lt;lis&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Oman               &lt;tibble&gt; &lt;lm&gt;  year     0.772    0.0393      19.7 2.53e- 9\n 2 Vietnam            &lt;tibble&gt; &lt;lm&gt;  year     0.672    0.0220      30.6 3.29e-11\n 3 Saudi Arabia       &lt;tibble&gt; &lt;lm&gt;  year     0.650    0.0348      18.7 4.22e- 9\n 4 Indonesia          &lt;tibble&gt; &lt;lm&gt;  year     0.635    0.0108      58.8 4.93e-14\n 5 Libya              &lt;tibble&gt; &lt;lm&gt;  year     0.626    0.0258      24.3 3.19e-10\n 6 Yemen, Rep.        &lt;tibble&gt; &lt;lm&gt;  year     0.605    0.0265      22.8 5.87e-10\n 7 West Bank and Gaza &lt;tibble&gt; &lt;lm&gt;  year     0.601    0.0332      18.1 5.59e- 9\n 8 Tunisia            &lt;tibble&gt; &lt;lm&gt;  year     0.588    0.0261      22.5 6.64e-10\n 9 Gambia             &lt;tibble&gt; &lt;lm&gt;  year     0.582    0.0192      30.3 3.57e-11\n10 Jordan             &lt;tibble&gt; &lt;lm&gt;  year     0.572    0.0319      17.9 6.31e- 9\n\n\nThe use of nested data frames and broom has great potential. Depending on the question, it is possible to filter only significant results, select models with the highest explanatory power, etc. To learn more about the automatisation using purrr and running many models, look here: https://r4ds.hadley.nz/iteration.html, https://adv-r.hadley.nz/functionals.html, https://r4ds.had.co.nz/many-models.html, https://www.tmwr.org.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "7 + 8 Automatisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/7_8_automatisation.html#exercises",
    "href": "DataManipulationVisualisation/7_8_automatisation.html#exercises",
    "title": "7 + 8 Automatisation",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\nRewrite the following code as a function:\n\nx / sum(x, na.rm = T)*100\ny / sum(y, na.rm = T)*100\nz / sum(z, na.rm = T)*100\n\nUse the function to calculate the percentage of each species in the penguins dataset.\nWrite your own function that takes a numeric vector of temperatures in Fahrenheit and returns them converted to Celsius using the formula \\(C = (F-32) *5/9\\). Test it with the following values: 0°F, 20°F, 68°F, 86°F, 100°F.\nWrite a function to calculate the logistic population growth using the formula \\(N_t = \\frac{K}{1 + \\left( \\frac{K - N_0}{N_0} \\right) e^{-r t}}\\), where \\(N_0\\) is the initial population, K the carrying capacity, r an intrinsic growth rate and t time. Run the function for the values \\(N_0 = 10\\), K = 100, r = 0.3 and time varies from 1 to 20.\n* Write a function that calculates the Shannon diversity index using the formula \\(H' = -∑p_i ln(p_i)\\). Use the function to calculate Shannon diversity of each plot in the sand vegetation dataset.\nTake the Pokémon dataset and visualise the distribution of defense power of water-type Pokémon. Turn this code into a function that helps you draw a histogram of defense power for different Pokémon types. * Try to generalize the code so that you can plot a distribution of any numerical variable.\nUse the function in a for loop and create plots of the distribution of defense power for all Pokémon types. * Save all plots in a plots folder.\n* Do the same using purrr functions.\n* Take the code for visualisation of Ellenberg-type indicator values in different forest types (Exercise 5 in Chapter 6) and turn it into a function. Draw the boxplots for four different Ellenberg-type indicator values using this function. Use a for loop or purrr functions to make these four plots. Combine them and save them in the plots folder.",
    "crumbs": [
      "Data Manipulation and Visualization",
      "7 + 8 Automatisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/7_8_automatisation.html#further-reading",
    "href": "DataManipulationVisualisation/7_8_automatisation.html#further-reading",
    "title": "7 + 8 Automatisation",
    "section": "Further reading",
    "text": "Further reading\nR for Data Science: https://r4ds.hadley.nz/program.html\nHands on Programming with R: https://rstudio-education.github.io/hopr\nAdvanced R: https://adv-r.hadley.nz\nProgramming with dplyr: https://dplyr.tidyverse.org/articles/programming.html\nProgramming with tidyr: https://tidyr.tidyverse.org/articles/programming.html\nProgramming with ggplot2: https://ggplot2-book.org/programming.html\nTidy modeling with R: https://www.tmwr.org",
    "crumbs": [
      "Data Manipulation and Visualization",
      "7 + 8 Automatisation"
    ]
  },
  {
    "objectID": "DataManipulationVisualisation/data_manipulation_visualisation.html",
    "href": "DataManipulationVisualisation/data_manipulation_visualisation.html",
    "title": "Data Manipulation and Visualisation",
    "section": "",
    "text": "Osnova předmětu Manipulace a vizualizace dat\nV průběhu kurzu představíme pokročilé metody manipulace a vizualizace dat v programu R, zejména s využitím knihoven z kolekce tidyverse (tidyr, dplyr, tibble, purr, stringr, ggplot2, readr). Cílem předmětu je naučit studenty rutinní manipulaci s daty, tak aby si je uměli importovat, upravit, filtrovat, připojit nové informace z externích dat, vytvořit nové proměnné (např. na základě výpočtu), seskupit vzorky na základě nějaké charakteristiky/informace a pro tyto skupiny vypočítat další parametry. Dále se studenti naučí základní i pokročilé metody vizualizace dat pomocí ggplot2 a tvorbu základních map v R. Cílem předmětu je i osvojení přístupu open data science, kdy se naučí připravit skript tak, aby bylo možné ho na závěr publikovat na platformě GitHub.\n\n\n1 Úvod - 15. 9. 2025\n\nR jako programovací jazyk\nTidyverse package, %&gt;%, |&gt;\nprojekty v RStudiu, cheatsheets, keyboard shortcuts\n\nzásady tidy skriptu (úprava, nadpisy, záložky, poznámky)\nzdroje informací a kde hledat pomoc, AI\nimport pomocí readr, readxl, na co si dávat pozor (encoding)\nstruktura dat (names, table, glimpse)\ntidy data (zásady, příprava, kontrola), přejmenování proměnných (rename)\n\n\n\n2 Základní manipulace s daty - 22. 9. 2025\n\nzákladní manipulace s daty (select, filter, mutate, arrange, slice)\nexport dat (write_csv)\n\n\n\n3 Vizualizace dat pomocí ggplot - 29. 9. 2025\n\nlogika ggplot\nzákladní geom funkce (point, boxplot, histogram, barplot)\nprokládání trendů\nsymboly, barvy\nlegenda, popisky os\ntheme\nuložení grafu (ggsave)\n\n\n\n4 Wide vs. long format - 13. 10. 2025\n\npřevody formátů (pivot)\nnové proměnné (mutate, group_by, summarise)\nspecies richness, součty/podíly různých hodnot v rámci vzorku (count)\n\n\n\n5 Join funkce - 20. 10. 2025\n\nspojovací funkce (left_join, full_join), přidání informací z jiných datových souborů\nfiltrovací funkce: semi_join, anti_join\npodíly určitých skupin podle vlastností, indikační hodnoty, CWM\núprava nomenklatury (pokročilé mutate, summarise), slučování duplicit\nmutate s vícenásobnou podmínkou (ifelse, case_when)\n\n\n\n6 Pokročilá vizualizace dat - 27. 10. 2025\n\nggplot advanced - faceting, using multiple data sources, scales, position adjustments, legend modifications\nuseful extensions - patchwork, ggpubr, ggeffects\nshiny trailer (ukázka)\n\n\n\n7 + 8 Automatizace skriptu - 3. a 10. 11. 2025\n\nnapsání vlastní funkce\npoužití smyček (for loops)\npurrr a ukázka práce s nested dataframes\n\n\n\n9 + 10 Mapy v R - 24. 11. a 1. 12. 2025\n\nmapy pomocí terra\nzobrazení vzorků v prostoru (přehledová mapa, měřítko, legenda…) na podkladě open street maps\nkartogramy, mapování v gridu\nextrakce dat z rastru, digitální model\nvýběr dat pomocí masky\nškálování mapovaných bodů podle hodnot (barva, symbol)\n\n\n\n11 Od databáze ke grafu (opakovací hodina) - 8. 12. 2025\n\nimport dat z databáze, propojení různých datových souborů, úprava struktury dat\nfiltrování podsouboru\nsloučení duplicit např. vzniklých převodem nomenklatury\nnapojení externích vlastností, výpočty vážených průměrů\npříprava grafu pro publikaci\nsloučení celého procesu do jedné pipeliny\n\n\n\n12 GitHub - 15. 12. 2025\n\njak funguje, stažení dat z veřejných projektů\nversion control\nvlastní účet, propojení s RStudiem\nvytvoření vlastního úložiště (repository), propojení s R projektem v počítači\nspolupráce na projektu (branch, commit, push, pull, merge conflicts)\npublikace skriptu, zveřejnění (doi, zásady readme)\nGitHub pages",
    "crumbs": [
      "Data Manipulation and Visualization"
    ]
  }
]